[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Labs for Intro Biostatistics",
    "section": "",
    "text": "Setup"
  },
  {
    "objectID": "index.html#software-and-cloud-services",
    "href": "index.html#software-and-cloud-services",
    "title": "Labs for Intro Biostatistics",
    "section": "Software and Cloud Services",
    "text": "Software and Cloud Services\nAll labs will use R, RStudio, and Posit Cloud.\nR is a computer program that allows an extraordinary range of statistical calculations. It is a free program, mainly written by volunteer contributors from around the world.\nRStudio is a separate program, also free, that allows you to easily organize separate tabs for R code files, graphic, help docs, and more.\nFor this course, you won’t need R and RStudio installed on your own device because we will work in Posit Cloud. However, if you wish to install them on your device, go to https://rstudio.com/products/rstudio/download/ for instructions about getting set up."
  },
  {
    "objectID": "index.html#using-posit-cloud",
    "href": "index.html#using-posit-cloud",
    "title": "Labs for Intro Biostatistics",
    "section": "Using Posit Cloud",
    "text": "Using Posit Cloud\nWe have setup Workspaces for each lab assignment and each lab section. For example, the workspace for lab01 materials for section 001 is lab01-001. Join the Workspace for each lab by clicking on the appropriate link in Google Classroom (e.g. “Lab 01-005 Posit Cloud Link” for the first week’s materials for section 005)\nImportant: Login with your userid@hawaii.edu account, not a personal gmail account. If you accidentally use your personal account, let me know. I will delete it while you register with the university account.\nNote: Wherever images say “lab02”, replace that with the actual lab number you will be completing (e.g. “lab01” or “lab11”).\n\n\nOnce you’re logged in, the lab01 Workspace will looking something like this (note: where it says “-monday” in the picture should be your section number):\n\n\nEach item listed under “All Projects” is a “Project” within the lab01-section Workspace. Each week I will create a Project template that you will copy and work from (you cannot edit the original).\n\nTo create your own, click “START” to the left of “lab01”.\n\n\n\nOnce the Project is copied and deployed, rename the Project from “lab01” to include your name like this: lab01-muir-chris\n\n\nDisclaimer: These labs rely heavily on those developed by Mike Whitlock for his BIOL 300 course at UBC https://www.zoology.ubc.ca/~whitlock/bio300/. In some cases the materials have been used verbatim, in other cases Chris Muir, and/or Andy Rominger have added new material, or heavily modified the original material by Prof. Whitlock."
  },
  {
    "objectID": "lab01.html#goals",
    "href": "lab01.html#goals",
    "title": "1  Getting started",
    "section": "1.1 Goals",
    "text": "1.1 Goals\n\nLearning how to start with Posit Cloud\nUse the command line\nUse functions in R\nUse vectors\nUse data frames"
  },
  {
    "objectID": "lab01.html#r-rstudio-and-posit-cloud",
    "href": "lab01.html#r-rstudio-and-posit-cloud",
    "title": "1  Getting started",
    "section": "1.2 R, RStudio, and Posit Cloud",
    "text": "1.2 R, RStudio, and Posit Cloud\n\n1.2.1 What is R?\nR is a computer program that allows an extraordinary range of statistical calculations. It is a free program, mainly written by voluntary contributions from statisticians around the world. R is available on most operating systems, including Windows, Mac OS, and Linux.\nR can make graphics and do statistical calculations. It is also a full-fledged computing language. In this manual, we will only scratch the surface of what R can do.\n\n\n1.2.2 What is RStudio?\nRStudio is a separate program, also free, that provides a more elegant front end for R. RStudio allows you to easily organize separate windows for R commands, graphic, help, etc. in one place.\nFor this course, you won’t need R and RStudio installed on your own device because we will work in Posit Cloud. However, if you wish to install them on your device, go to https://rstudio.com/products/rstudio/download/ for instructions about getting set up.\n\n\n1.2.3 What is Posit Cloud?\nPosit Cloud allows you to run a hosted version of RStudio in the cloud that allows you to run R without having to download anything on your personal computer or confirgure your personal computer. You access Posit Cloud from your browser, and conveniently you can access from any computer, not just your personal device.\n\n\n1.2.4 Resources\n\nPosit Cloud provides many learning materials: interactive tutorials covering the basics of data science, cheatsheets, and a guide to using Posit Cloud.\n\n\n\n1.2.5 Getting started with Posit Cloud\nFollow the instructions on the “Setup” page for setting yourself up with Posit Cloud.\nOnce done, proceed with the rest of the lab below."
  },
  {
    "objectID": "lab01.html#learning-the-tools",
    "href": "lab01.html#learning-the-tools",
    "title": "1  Getting started",
    "section": "1.3 Learning the Tools",
    "text": "1.3 Learning the Tools\nWhen you start Posit Cloud, it will automatically start RStudio as well. You run R inside RStudio, itself inside Posit Cloud.\nAfter you have started Posit Cloud, you should see a new window with a menu bar at the top and three main sections. One of the sections is called the “Console” – this is where you type commands to give instructions to R and typically where you see R’s answers to you.\nAnother important corner of this window can show a variety of information. Most importantly to us, this is where graphics will appear, under the tab marked “Plots”.\n\n1.3.1 The command line\nWhen you start Posit Cloud, you’ll see a corner of the window called the “Console.” By the default the console window is in the bottom left of the RStudio screen.\nYou can type commands in this window where there is a prompt (which will look like a &gt; sign at the bottom of the window). The Console has to be the selected window. (Clicking anywhere in the Console selects it.)\nThe &gt; prompt is R’s way of inviting you to give it instructions. You communicate with R by typing commands after the &gt; prompt.\nType “2+2” at the &gt; prompt, and hit return. You’ll see that R can work like a calculator (among its many other powers). It will give you the answer, 4, and it will label that answer with [1] to indicate that it is the first element in the answer. (This is sort of annoying when the answers are simple like this, but can be very valuable when the answers become more complex.)\nRemember, you don’t type the &gt; sign. The &gt; is the prompt that R gives saying it is ready for input. We reproduce it here so you can see which is input (in blue) and which is output (in black or red).\n\n2 + 2\n\n[1] 4\n\n\nYou can use a wide variety of math functions to make calculations here, e.g., log() calculates the log of a number:\n\nlog(42)\n\n[1] 3.73767\n\n\n(By default, this gives the natural log with base \\(e\\).)\nParentheses are used both as a way to group elements of the calculation and also as a way to denote the arguments of functions. (The “arguments” of a function are the set of values given to it as input.) For example, log(3) is applying the function log() to the argument 3.\nAnother mathematical function that often comes in handy is the square root function, sqrt(). For example, the square root of 4 is:\n\nsqrt(42)\n\n[1] 6.480741\n\n\nTo calculate a value with an exponent, used the ^ symbol. For example \\(4^3\\) is written as:\n\n4 ^ 3\n\n[1] 64\n\n\nNote how R ignores white space when it’s not in quotes (we’ll come back to quotes later):\n\n4^3\n\n[1] 64\n\n4  ^  3\n\n[1] 64\n\n\nOf course, many math functions can be combined to give an almost infinite possibility of mathematical expressions. For example,\n\\[\\frac{1}{\\sqrt{2 \\pi (3.1)^2}} e ^ {-\\frac{(12 - 10.7) ^ 2}{2 \\times 3.1}}\\]\ncan be calculated with\n\n(1 / (sqrt(2 * pi * (3.1) ^ 2))) * exp(-(12 - 10.7) ^ 2 / (2 * 3.1))\n\n[1] 0.09798692\n\n\n\n\n1.3.2 Saving your code\nWhen you analyze your own data, we strongly recommend that you keep a record of all commands used, along with copious notes, so that weeks or years later you can retrace the steps of your earlier analysis.\nIn Posit Cloud, you can create a plain text file (sometimes called a script), which contains R commands that can be reloaded and used at a later date. We have created a scratch file where you can enter and save your commands while you’re learning the tools. Click on “scratch.R” in the lower-right:\n\nThat will open a mostly blank text file above the Console that looks like this:\n\nYou can copy and paste any commands that you want from the Console, or type directly here. (When you copy and paste, do not include the &gt; prompt in the script.) Save this script for later reference by hitting “Save” under the “File” menu. In the future you can open this file to have those commands available for use again.\nIt is a good habit to type all your commands in the script window and run them from there, rather than typing directly into the console. This lets you save a record of your session so that you can more easily re-create what you have done later.\nFYI, if you want to create a new, blank R script, here’s how: under the menu at the top, choose “File”, then “New File”, and then “R Script”. Follow the prompts to save the new file.\n\n\n1.3.3 Comments\nIn scripts, it can be very useful to save a bit of text which is not to be evaluated by R. You can leave a note to yourself (or a collaborator) about what the next line is supposed to do, what its strengths and limitations are, or anything else you want to remember later. To leave a note, we use “comments”, which are a line of text that starts with the hash symbol #. Anything on a line after a # will be ignored by R.\n\n# This is a comment. Running this in R will \n# have no effect.\n\n\n\n1.3.4 Functions\nMost of the work in R is done by functions. A function has a name and one or more arguments. For example, log(4) is a function that calculates the log in base \\(e\\) for the value 4 given as input.\nSometimes functions have optional input arguments. For the function log(), for example, we can specify the optional input argument base to tell the function what base to use for the logarithm. If we don’t specify the base variable, it has a default value of base = e. To get a log in base 10, for example, we would use:\n\nlog(4, base = 10)\n\n[1] 0.60206\n\n\n\n\n1.3.5 Defining variables\nIn R, we can store information of various sorts by assigning them to variables. For example, if we want to create a variable called x and give it a value of 4, we would write\n\nx &lt;- 4\n\nThe middle bit of this—a less than sign and a hyphen typed together to make something that looks a little like a left-facing arrow – tells R to assign the value on the right to the variable on the left. After running the command above, whenever we use x in a command it would be replaced by its value 4. For example, if we add 3 to x, we would expect to get 7.\n\nx + 3\n\n[1] 7\n\n\nVariables in R can store more than just simple numbers. They can store lists of numbers, functions, graphics, etc., depending on what values get assigned to the variable.\nWe can always reassign a new value to a variable. If we now tell R that x is equal to 32\n\nx &lt;- 32\n\nthen x takes its new value:\n\nx\n\n[1] 32\n\n\n\n\n1.3.6 Names\nNaming variables and functions in R is pretty flexible.\nA name has to start with a letter, but that can be followed by any combination of letters, numbers, and underscores (_). Names cannot have spaces or any character other than letters, numbers, and underscores, for example $, -, and % are not allowed. Technically, periods (.) are allowed in names, but not reccomended excpet for specific uses outside the scope of this course.\nNames in R are case-sensitive, which means that Weights and weights are completely different things to R. This is a common and incredibly frustrating source of errors in R.\nIt’s a good idea to have your names be as descriptive as possible, so that you will know what you meant later on when looking at it. (However, if they get too long, it becomes painful and error prone to type them each time we use them, so this, as with all things, requires moderation.)\nSometimes clear naming means that it is best to have multiple words in the name, but we can’t have spaces. Therefore a common approach is like we saw in the previous section, to chain the words with underscores (not hyphens!), as in weights_before_hospital. (Another solution to make separate words stand out in a variable name is to vary the case: weightsBeforeHospital. This is called “Camel Case” because the capital letters are like camel humps.)\n\n\n1.3.7 Vectors\nOne useful feature of R is the ability to apply functions to an entire collection of numbers. The technical term for a set of numbers is “vector”. For example, the following code will create a vector of six numbers:\n\n c(78, 85, 64, 54, 102, 98.6)\n\n[1]  78.0  85.0  64.0  54.0 102.0  98.6\n\n\nc() is a function that creates a vector, containing the items given in its arguments. To help you remember, you could think of the function c() meaning to “combine” some elements into a vector.\nLet’s add a little extra here to make the computer remember this vector. Let’s assign it to a variable, called temperatureF (because these numbers are actually a set of temperatures in degrees Fahrenheit):\n\ntemperatureF &lt;- c(78, 85, 64, 54, 102, 98.6)\n\nThe combination of the less than sign and the hyphen makes an arrow pointing from right to left—this tells R to assign the stuff on the right to the name on the left. In this case we are assigning a vector to the variable temperatureF.\nInputting this into R causes no obvious output, but R will now remember this vector of temperatures under the name temperatureF. We can view the contents of the vector temperatureF by simply typing its name:\n\ntemperatureF\n\n[1]  78.0  85.0  64.0  54.0 102.0  98.6\n\n\nThe power of vectors is that R can do the same calculation on all elements of a vector with one command. For example, to convert a temperature in Fahrenheit to Celsius, we would want to subtract 32 and multiply times 5/9. We can do that for all the numbers in this vector at once:\n\ntemperatureC &lt;- (temperatureF - 32) * 5 / 9\ntemperatureC\n\n[1] 25.55556 29.44444 17.77778 12.22222 38.88889 37.00000\n\n\nTo pull out one of the numbers in this vector, we add square brackets after the vector name, and inside those brackets put the index of the element we want. (The “index” is just a number giving the location in the vector of the item we want. The first item has index 1, etc.) For example, the second element of the vector temperatureC is\n\ntemperatureC[2]\n\n[1] 29.44444\n\n\nOne of the ways to slip up in R is to confuse the [square brackets] which pull out an element of a vector, with the (parentheses), which is used to enclose the arguments of a function.\nVectors can also operate mathematically with other vectors. For example, imagine you have a vector of the body weights of patients before entering hospital (weight_before_hospital) and another vector with the same patient’s weights after leaving hospital (weight_after_hospital). You can calculate the change in weight for all these patients in one command, using vector subtraction:\n\nweight_before_hospital &lt;- c(100, 102)\nweight_after_hospital &lt;- c(98, 99)\n\nweight_change_during_hospital &lt;- weight_before_hospital - weight_after_hospital\n\nThe result will be a vector that has each patient’s change in weight.\n\n\n1.3.8 Basic calculation examples\nIn this course, we’ll learn how to use a few dozen functions, but let’s start with a couple of basic ones.\nThe function mean() does just what it sounds like: it calculates the sample mean (that is, the average) of the vector given to it as input. For example, the mean of the vector of the temperatures in degrees Celsius from above is 26.81481:\n\nmean(temperatureC)\n\n[1] 26.81481\n\n\nAnother simple (and simply named) function calculates the sum of all numbers in a vector: sum().\n\nsum(temperatureC)\n\n[1] 160.8889\n\n\nTo count the number of elements in a vector, use length().\n\nlength(temperatureC)\n\n[1] 6\n\n\nThis shows that there are 6 temperature values in the vector that make up the vector temperatureC.\n\n\n1.3.9 Reading a data file\nIn this course, we have saved the data in a “comma-separated variable” format. All files in this format ought to have “.csv” as the end of their file name. A CSV file is a plain text file, easily read by a wide variety of programs. Each row in the file (besides the first row) is the data for a given individual, and for each individual each variable is listed in the same order, separated by commas. It’s important to note that you can’t have commas anywhere else in the file, besides the separators.\nThe first row of a CSV file should be a “header” row, which gives the names of each variable, again separated by commas.\nFor examples in this tutorial, let’s use a data set about the passengers of the RMS Titanic. One of the data sets in the folder of data attached to this lab is called “titanic.csv”. This is a data set of 1313 passengers from the voyage of this ship, which contains information about some personal info about each passenger as well as whether they survived the accident or not.\nTo import a CSV file into R, use the read.csv() function as in the following command. (This assumes that you have set the working directory to the labs folder, as we described above.)\n\ntitanic_data &lt;- read.csv(\"data/titanic.csv\")\n\nThis looks for the file called titanic.csv in the folder called data. Here we have given the name titanic_data to the object in R that contains all this passenger data. Of course, if you wanted to load a different data set, you would be better off giving it a more apt name than “titanic_data”.\nTo see if the data loads appropriately, you might want to run the command\n\nsummary(titanic_data)\n\n passenger_class        name                age            embarked        \n Length:1313        Length:1313        Min.   : 0.1667   Length:1313       \n Class :character   Class :character   1st Qu.:21.0000   Class :character  \n Mode  :character   Mode  :character   Median :30.0000   Mode  :character  \n                                       Mean   :31.1942                     \n                                       3rd Qu.:41.0000                     \n                                       Max.   :71.0000                     \n                                       NA's   :680                         \n home_destination       sex              survive         \n Length:1313        Length:1313        Length:1313       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n\n\nwhich will list all the variables and some summary statistics for each variable.\n\n\n1.3.10 Intro to data frames\nA data frame is a way that R can store a data set on a number of individuals. A data frame is a collection of columns; each column contains the values of a single variable for all individuals. The values of each individual occur in the same order in all the columns, so the first value for one variable represents the same individual as the first value in the lists of all other variables.\nThe function read.csv() loads the data it reads into a data frame.\nThe data frame is usually given a name, which is used to tell R’s functions which data set to use. For example, in the previous section we read in a data set to a data frame that we called titanic_data. This data frame now contains information about each of the passengers on the Titanic. This data frame has seven variables, so it has seven columns (passenger_class, name, age, embarked, home_destination, sex, and survive).\nVery importantly, we can grab one of the columns from a data frame by itself. We write the name of the data frame, followed by a $, and then the name of the variable.\nFor example, to show a list of the age of all the passengers on the Titanic, use\n\ntitanic_data$age\n\nThis will show a vector that has the values for this variable age, one for each individual in the data set.\nNote, when looking at long vectors or data frames, it’s convenient to use the head function, which only shows the first 6 elements, not the whole huge vector or data frame.\n\nhead(titanic_data$age)\n\n[1] 29.0000  2.0000 30.0000 25.0000  0.9167 47.0000\n\n\n\n\n1.3.11 Adding a new column\nSometimes we would like to add a new column to a data frame. The easiest way to do this is to simply assign a new vector to a new column name, using the $.\nFor example, to add the log of age as a column in the titanic_data data frame, we can write\n\ntitanic_data$log_age &lt;- log(titanic_data$age)\n\nYou can run the command head(titanic_data) to see that log_age is now a column in titanic_data.\n\n\n1.3.12 Choosing subsets of data\nSometimes we want to do an analysis only on some of the data that fit certain criteria. For example, we might want to analyze the data from the Titanic using only the information from females. The easiest way to do this is to use the subset function.\nIn the titanic data set there is a variable named sex. We can create a new data frame that includes only the data from passengers recorded as female with the following command:\n\ntitanic_female_data &lt;- subset(titanic_data, sex == \"female\")\nhead(titanic_female_data)\n\n   passenger_class                                       name age    embarked\n1              1st                  Allen,MissElisabethWalton  29 Southampton\n2              1st                   Allison,MissHelenLoraine   2 Southampton\n4              1st  Allison,MrsHudsonJ.C.(BessieWaldoDaniels)  25 Southampton\n7              1st              Andrews,MissKorneliaTheodosia  63 Southampton\n9              1st    Appleton,MrsEdwardDale(CharlotteLamson)  58 Southampton\n12             1st Astor,MrsJohnJacob(MadeleineTalmadgeForce)  19   Cherbourg\n              home_destination    sex survive   log_age\n1                   StLouis,MO female     yes 3.3672958\n2  Montreal,PQ/Chesterville,ON female      no 0.6931472\n4  Montreal,PQ/Chesterville,ON female      no 3.2188758\n7                    Hudson,NY female     yes 4.1431347\n9            Bayside,Queens,NY female     yes 4.0604430\n12                  NewYork,NY female     yes 2.9444390\n\n\nThis new data fame will include all the same columns as the original titanic_data, but it will only include the rows for which the sex was “female”.\nNote that the syntax here requires a double == sign. In R (and many other computer languages), the double equal sign creates a statement that can be evaluated as TRUE or FALSE. Here we are asking, for each individual, whether sex is “female”."
  },
  {
    "objectID": "lab01.html#questions-for-lab-report",
    "href": "lab01.html#questions-for-lab-report",
    "title": "1  Getting started",
    "section": "1.4 Questions for Lab Report",
    "text": "1.4 Questions for Lab Report\nYour lab report is due before the start of next week’s lab. When you’re finished, save it, and your TA can access it on the Cloud.\n\nFor each week, use the R script called “report.R” to save the commands that you use to answer the questions, as well as the answers themselves.\n\nOpen the report.R file. Start by editing comments with your name at the top. \nSave the script regularly as you work! To save, go to “File &gt; Save” or use command-S shortcut. \nFor each of the questions below, write the question number as a comment, followed by any R code you use to do the question, and give the answers as comments. It might look something like this:\n\n\n\n# Student's name\n# BIOL 220 Lab 11\n# 2023-01-12\n\n# Questions\n\n# 1. I followed directions to set up my lab report\n\n# 2. Yes, I got the same answers!\n\n# 3. Only text answers, no code\n# Answer part 1\n# Answer part 2\n\n# 4. Text and code\n# a.\nx &lt;- c(1, 2, 3) # you can also comment like this\n\n# b.\nmean(x)\n\n# The mean of c(1, 2, 3) is 2. Here's what that means...\n\n\nRun the Learning the tools commands in R from your “scratch.R” script. Did you get the same answers as shown in the text? (Answer “yes”, “no”, or a more detailed explanation. You don’t need to re-run all the code and output here.)\nFor each of the following, come up with a variable name that would be appropriate to use in R for the listed variable:\n\n\n\n\nVariable\nName in R\n\n\n\n\nBody temperature in Celsius\n\n\n\nHow much aspirin is given per dose for a patient\n\n\n\nNumber of televisions per person\n\n\n\nHeight (including neck and extended legs) of giraffes\n\n\n\n\n\nUse R to calculate:\n\n\\(15 \\times 17\\)\n\\(13^3\\)\n\\(\\text{log}_e(14)\\) (natural log)\n\\(\\text{log}_{10}(100)\\) (base 10 log)\n\\(\\sqrt{81}\\)\n\nWeddell seals live in Antarctic waters and take long strenuous dives in order to find fish to feed upon. Researchers (Williams et al. 2004) wanted to know whether these feeding dives were more energetically expensive than regular dives (perhaps because they are deeper, or the seal has to swim further or faster). They measured the metabolic costs of dives using the oxygen consumption of 10 animals (in ml O\\(_2\\) / kg) during a feeding dive. Here are the data:\n\n71.0, 77.3, 82.6, 96.1, 106.6, 112.8, 121.2, 126.4, 127.5, 143.1\n\nFor the same 10 animals, they also measured the oxygen consumption in non-feeding dives. With the 10 animals in the same order as before, here are those data:\n\n42.2, 51.7, 59.8, 66.5, 81.9, 82.0, 81.3, 81.3, 96.0, 104.1\n\n\nMake a vector for each of these lists, and give them appropriate names.\nConfirm (using R) that both of your vectors have the same number of individuals in them.\nCreate a vector called metabolism_difference by calculating the difference in oxygen consumption between feeding dives and nonfeeding dives for each animal.\nWhat is the average difference between feeding dives and nonfeeding dives in oxygen consumption?\nThe arithmetic mean is calculated by adding up all the numbers and dividing by how many numbers there are. Calculate the mean of these numbers using sum() and length(). Did you get the same answer as with using mean()?\nAnother appropriate way to represent the relationship between these two numbers would be to take the ratio of O\\(_2\\) consumption for feeding dives over the O\\(_2\\) consumption of nonfeeding dives. Make a vector which gives this ratio for each seal.\nSometimes ratios are easier to analyze when we look at the log of the ratio. Create a vector which gives the log of the ratios from the previous step. (Use the natural log.) What is the mean of this log-ratio?\n\nThe data file called “countries.csv” in the data folder contains information about all the countries on Earth1. Each row is a country, and each column contains a variable.\n\nUse read.csv() to read the data from this file into a data frame called countries.\nUse summary() to get a quick description of this data set. What are the first three variables?\nUsing the output of summary(), how many countries are from Africa in this data set?\nWhat kinds of variables (i.e., categorical or numerical) are continents, cell_phone_subscriptions_per_100_people_2012, total_population_in_thousands_2015, and fines_for_tobacco_advertising_2014? (Don’t go by their variable names – look at the data in the summary results to decide.)\nAdd a new column to your countries data frame that has the difference in ecological footprint between 2012 and 2000. What is the mean of this difference? (Note: this variable will have “missing data”, which means that some of the countries do not have data in this file for one or the other of the years of ecological footprint. By default, R doesn’t calculate a mean unless all the data are present. To tell R to ignore the missing data, add an option to the mean() command that says na.rm=TRUE. We’ll learn more about this later.)\n\nUsing the countries data again, create a new data frame called africa_data, that only includes data for countries in Africa. What is the sum of the total_population_in_thousands_2015 for this new data frame?"
  },
  {
    "objectID": "lab01.html#footnotes",
    "href": "lab01.html#footnotes",
    "title": "1  Getting started",
    "section": "",
    "text": "These data mainly come from the World Health Organization, but the Continent list comes from https://datahub.io/ and the ecological footprint and cell phone data come from http://www.nationmaster.com.↩︎"
  },
  {
    "objectID": "lab02.html#goals",
    "href": "lab02.html#goals",
    "title": "2  Graphics in R",
    "section": "2.1 Goals",
    "text": "2.1 Goals\n\nKnow how to load packages to expand the capabilities of R\nKnow some basic graphical formats and when they are useful.\nMake graphs in R, such as histograms, bar charts, box plots, and scatter plots.\nBe able to suggest improvements to basic graphs to improve readability and accurate communication\n\nAs will be the case with all labs, this lab will be completed using Posit Cloud. If needed, refer back to “Setup” and “Lab 1” for instructions on access and setting up your workspace for this lab. Here is the direct link to the Posit Cloud shared workspace for Lab 2: xyz."
  },
  {
    "objectID": "lab02.html#learning-the-tools",
    "href": "lab02.html#learning-the-tools",
    "title": "2  Graphics in R",
    "section": "2.2 Learning the Tools",
    "text": "2.2 Learning the Tools\n\n2.2.1 Extending R’s capabilities with packages\nR has a lot of power in its basic form, but one of the most important parts about R is that it is expandable by the work of other people. These expansions are usually released in “packages”.\nEach package needs to be installed on your computer only once, but to be used it has to be loaded into R during each session.\nTo install a package in RStudio, click on the packages tab from the sub-window with tables for Files, Plots, Packages, Help, and Viewer. Immediately below that will be a button labeled “Install” – click that and a window will open.\n\nIn the second row (labeled “Packages”), type ggplot2. Make sure the box for “Install dependencies” near the bottom is clicked, and then click the “Install” button at bottom right. This will install the graphics package ggplot2.\n\nAlternatively, you can also use a function to install packages:\n\ninstall.packages(\"ggplot2\")\n\nInstalling a package only needs to be done once on a given computer or a given Posit Cloud Workspace, and that package is permanently available.\n\n\n2.2.2 Loading a package\nOnce a package is installed, it needs to be loaded into R during a session if you want to use it. You do this with the function called library().\n\nlibrary(ggplot2)\n\nNow we can start making graphics with the ggplot2 package.\n\n\n2.2.3 ggplot\nWhile base R has ample graphic capabilities, functions from the ggplot2 package are becoming the de facto standard for scientific graphics because they allow more easy customization of plots.\nTo make a graph with ggplot2, you need to specify at least two elements in your command. The first uses the function ggplot itself, to specify which data frame you want to visualize and also which variables are to be plotted. The second part tells R what kind of graph to make, using a geom function. The odd part is that these two parts are put together with a + sign. It’s simplest to see this with an example. We’ll draw a histogram with ggplot in the next section.\n\n\n2.2.4 Histograms\nUseful when:\n\nResponse variable is numerical\n\nA histogram represents the frequency distribution of a numerical variable in a sample.\nLet’s see how to make a basic histogram using the age data from the Titanic data set. Make sure you have loaded the data (using read.csv()) into a data frame called titanic_data.\n\ntitanic_data &lt;- read.csv(\"data/titanic.csv\")\n\nHere’s the ggplot2 code to make a simple histogram of age:\n\nggplot(titanic_data, aes(x = age)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nNotice that there are two functions called here, put together in a single command with a + sign. You don’t have to put a line break after the + (R ignores it), but it makes the code more readable. The first function is ggplot, and it has two input arguments. The first is titanic_data, this is the name of the data frame containing the variables that we want to graph. The second input to ggplot is an aes function. In this case, the aes function tells R that we want Age to be the \\(x\\)-variable (i.e. the variable that is displayed along the \\(x\\)-axis). The “aes” stands for “aesthetics”.\nThe second function in this command is geom_histogram(). This is the part that tells R that the “geometry” of our plot should be a histogram.\nRunning this should give a plot that look something like this:\n\n\n\n\n\nThis is not the most beautiful graph in the world, but it conveys the information. At the end of this tutorial we’ll see a couple of options that can make a ggplot graph look a little better.\n\n\n2.2.5 Bar graphs\nUseful when:\n\nResponse variable is categorical\n\nA bar graph plots the frequency distribution of a categorical variable. With ggplot, the syntax for a bar graph is very similar to that for a histogram. For example, here is a bar graph for the categorical variable sex in the titanic data set:\n\nggplot(titanic_data, aes(x = sex)) + \n  geom_bar(stat = \"count\")\n\nAside from specifying a different variable for \\(x\\) in the aes function, we use a different geom function here, geom_bar, and specify the statistic we want to draw, which is the count (or frequency) of the different categories. The result should look like this:\n\n\n\n\n\n\n\n2.2.6 Boxplots\nUseful when:\n\nExplanatory variable is categorical\nResponse variable is numerical\n\nA boxplot is a convenient way of showing the distribution of a numerical variable in multiple groups. Here’s the code to draw a boxplot for age in the titanic data set, separately for each recorded sex:\n\nggplot(titanic_data, aes(x = sex, y = age)) + \n  geom_boxplot()\n\nNotice that the \\(y\\) variable here is age, and \\(x\\) is the categorical variable sex that goes on the \\(x\\)-axis. The other new feature here is the new geom function, geom_boxplot().\n\n\n\n\n\nHere the thick bar in the middle of each boxplot is the median of that group. The upper and lower bounds of the box extend from the first to the third quartile. The vertical lines are called whiskers, and they cover most of the range of the data (except when data points are pretty far from the median, then they are plotted as individual dots, as on the male boxplot).\n\n\n2.2.7 Scatterplots\nUseful when:\n\nExplanatory variable is numerical\nResponse variable is numerical\n\nScatterplots shows the relationship between two numerical variables.\nThe titanic data set does not have two numerical variables, so let’s use a different data set. We will plot the relationship between sea surface temperature and species richness of reef fishes as compiled by Barneche et. al (2019). These data come from many different published fish surveys conducted by many different researchers all around the world. Barneche and colleagues compiled those data to try to understand what environmental variables predict the species richness of reef fish. Let’s find out!\nYou can load the data with:\n\nreef_fish &lt;- read.csv(\"data/global-reef-fish.csv\")\n\nTo make a scatter plot of the variables temp_C and spp_richness with ggplot, you need to specify the \\(x\\) and \\(y\\) variables, and use geom_point():\n\n# Side note:  I've added a line break between arguments in ggplot()\n# This has no effect on the code, but makes it easier to read IMO\nggplot(reef_fish, \n       aes(x = temp_C, y = spp_richness)) +\n  geom_point()\n\nThe result look like this:\n\n\n\n\n\n\n\n2.2.8 Better looking graphics with options\nThe code we have listed here for graphics barely scratches the surface of what ggplot2, and R as a whole, are capable of. Not only are there far more choices about the kinds of plots available, but there are many, many options for customizing the look and feel of each graph. You can choose the font, the font size, the colors, the style of the axes labels, etc., and you can customize the legends and axes legends nearly as much as you want.\nLet’s dig a little deeper into just a couple of options that you can add to any of the forgoing graphs to make them look a little better. For example, you can change the text of the \\(x\\)-axis label or the \\(y\\)-axis label by using xlab() or ylab(). Let’s do that for the scatterplot, to make the labels a little nicer to read for humans.\n\nggplot(reef_fish, \n       aes(x = temp_C, y = spp_richness)) +\n  geom_point() +\n  xlab(\"Temperature (degrees C)\") +\n  ylab(\"Species richness\")\n\nThe labels that we want to add are included in quotes inside the xlab() and ylab() functions. Here is what appears:\n\n\n\n\n\nIt can also be nice to remove the default gray background, to make what some feel is a cleaner graph. Try adding\n\n+ theme_minimal()\n\nto the end of one of your lines of code making a graph, to see whether you prefer the result to the default design.\n\n\n2.2.9 Color palettes\nIt is important to use a palette that will be clear to color blind individuals and, in some cases, to those who view a printed version in greyscale. There are bewildering array of options, but the viridis palettes accomplish these goals well (read more here). We’ll revisit the histogram example above and view the age distribution on the Titanic by sex (multiple histogram). This is a bit more advanced than what we’ve covered so far, but hang in there. We’ll go step-by-step.\nThe cool thing about ggplot2 is we can assign a large number of graphical features (size, color, fill, shape, line type, etc.) to variables on our data. We’ll do that using the fill = ... argument in the aes() function to make the fill of the bars dependent on sex.\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nThat works, but it’s pretty ugly. For one thing, the bars are stacked on top of one another, so it’s hard to see the separate histograms for males and females. We’ll fix that by using the position = ... argument in the geom_histogram() function like this:\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  # I will interleave comments to explain what's going on\n  # position = position_identity() stops the bars from stacking\n  geom_histogram(position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nWell, that’s worse! Now the male bars are blocking the female bars. Let’s add a couple more arguments, making the color 50% transparent using the alpha = ... argument. I’ll also make the color around the bars black so we can see them better.\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  # alpha = 0.5 makes bars transparent\n  # color = \"black\" adds black lines around bars\n  geom_histogram(alpha = 0.5, color = \"black\", position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nBetter, but not great. Let’s use the facet_grid() function to put the histograms on separate panels. For this, we have to put sex in quotes (learning when things need to be quoted or not is frustrating).\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  # facet_grid() makes separate panels for each sex\n  facet_grid(rows = \"sex\") +\n  geom_histogram(alpha = 0.5, color = \"black\", position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nPretty good. Now, let’s finally add the viridis color palette. ggplot2 has some built in functions that you can just add using the + operator to change the color, like this:\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  facet_grid(rows = \"sex\") +\n  geom_histogram(alpha = 0.5, color = \"black\", position = position_identity()) +\n  # This function changes the color palette\n  scale_fill_viridis_d()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n2.2.10 Getting help\nThe help pages in R are the main source of help, but the amount of detail might be off-putting for beginners. For example, to explore the options for ggplot(), enter the following into the R Console.\n\nhelp(ggplot)\n\n# you can also use\n?ggplot\n\nThis will cause the contents of the manual page for this function to appear in the Help window in RStudio Cloud. These manual pages are often frustratingly technical. What many of us do instead is simply google the name of the function—there are a great number of resources online about R."
  },
  {
    "objectID": "lab02.html#questions",
    "href": "lab02.html#questions",
    "title": "2  Graphics in R",
    "section": "2.3 Questions",
    "text": "2.3 Questions\n\nUse the data from “countries.csv” to practice making some graphs.\n\nRead the data from the file “countries.csv” in the “data” folder. (Hint: we did this in the last lab - you need to use read.csv(), use the correct path, and give the object a name.)\nMake sure that you have run library(ggplot2). Why is this necessary for the remainder of this question?\nMake a histogram to show the frequency distribution of values for measles_immunization_oneyearolds, a numerical variable. (This variable gives the percentage of 1-year-olds that have been vaccinated against measles.) Describe the pattern that you see.\nMake a bar graph to show the numbers of countries in each of the continents. (The categorical variable continent indicates the continent to which countries belong.)\nDraw a scatterplot that shows the relationship between the two numerical variables life_expectancy_at_birth_male and life_expectancy_at_birth_female.\n\nThe ecological footprint is a widely-used measure of the impact a person has on the planet. It measures the area of land (in hectares) required to generate the food, shelter, and other resources used by a typical person and required to dispose of that person’s wastes. Larger values of the ecological footprint indicate that the typical person from that country uses more resources.\nThe countries data set has two variables showing the ecological footprint of an average person in each country. ecological_footprint_2000 and ecological_footprint_2012 show the ecological footprints for the years 2000 and 2012, respectively.\n\nPlot the relationship between the ecological footprint of 2000 and of 2012.\nDescribe the relationship between the footprints for the two years. Does the value of ecological footprint of 2000 seem to predict anything about its value in 2012?\nFrom this graph, does the ecological footprint tend to go up or down in the years between 2000 and 2012? Did the countries with high or low ecological footprint change the most over this time? (Hint: you can add a one-to-one line to your graph by adding + geom_abline(intercept = 0, slope = 1) to your ggplot() command. This will make it easier to see when your points are above or below the line of equivalence.)\n\nPlotting categorical and numerical variables: use the countries data again. Plot the relationship between continent and female life expectancy at birth. Describe the patterns that you see.\nMuchala (2006) measured the length of the tongues of eleven different species of South American bats, as well as the length of their palates (to get an indication of the size of their mouths). All of these bats use their tongues to feed on nectar from flowers. Data from the article are given in the file “BatTongues.csv”. In this file, both Tongue Length and Palette Length are given in millimeters. Each value for tongue length and palate length is a species mean, calculated from a sample of individuals per species.\n\nImport the data and inspect it using summary(). You can call the data set whatever you like, but in one of the later steps we’ll assume it is called bat_tongues.\nDraw a scatter plot to show the association between palate length and tongue length, with tongue length as the response variable. Describe the association: is it positive or negative? Is it strong or weak?\nAll of the data points that went into this graph have been double checked and verified. With that in mind, what conclusion can you draw from the outlier on the scatterplot?\nLet’s figure out which species is the outlier. To do this, we’ll use the subset function from Lab 1. Remember, the function subset gives us the row (or rows) of a data frame that has a certain property. Looking at the graph, we can tell that the point we are interested in has a very long tongue_length, at least over 80 mm long! Use subset to figure out the species name of this unusually long-tongued bat.\nThe unusual species is Anoura fistulata (See a photo here). This species has an outrageously long tongue, which it uses to collect nectar from a particular flower (can you guess what feature of the flower has led to the evolution of such a long tongue?). See the article by Muchala (2006) to learn more about the biology of this strange bat.\n\nImprove your figure! Pick one of the plots you made using R today. What could be improved about this graph to make it a more effective presentation of the data?"
  },
  {
    "objectID": "lab03.html#goals",
    "href": "lab03.html#goals",
    "title": "3  Workikng with data",
    "section": "3.1 Goals",
    "text": "3.1 Goals\n\nLearn to create data files\nExplore the importance of random sampling\nUnderstand variance and standard deviation\nUnderstand mean and median\n\nAs will be the case with all labs, this lab will be completed using Posit Cloud. If needed, refer back to “Setup” and “Lab 1” for instructions on access and setting up your workspace for this lab. Here is the direct link to the Posit Cloud shared workspace for Lab 2: xyz."
  },
  {
    "objectID": "lab03.html#learning-the-tools",
    "href": "lab03.html#learning-the-tools",
    "title": "3  Workikng with data",
    "section": "3.2 Learning the Tools",
    "text": "3.2 Learning the Tools\n\n3.2.1 Structure of a good data file\nData files appear in many formats, and different formats are sometimes preferable for different tasks. But there is one way to structure data — called “long” format — that is extremely useful for most things that you will want to do in statistics and R.\nLong format is actually very simple. Every row in the data set is a unique individual. Every column is a variable being measured on those individuals.\nFor example, last week in Question 4 we looked at some data about the tongue and palate lengths of several species of bats. There were three variables in that data set, the species name, tongue length, and palate length. Here each “individual” is a species. Here is that data in long format—each row is an individual. There are three columns, one for each variable:\n\n\n\n\n\nspecies\npalate_length\ntongue_length\n\n\n\n\nLichonycteris obscura\n10.0\n36.1\n\n\nGlossophaga comissarisi\n10.7\n26.6\n\n\nGlossophaga soricina\n11.4\n30.2\n\n\nAnoura caudifer\n11.6\n36.7\n\n\nHylonycteris underwoodi\n13.4\n36.7\n\n\nAnoura geoffroyi\n13.8\n39.6\n\n\nLonchophylla robusta\n14.3\n42.6\n\n\nAnoura fistulata\n12.4\n85.2\n\n\nAnoura cultrata\n14.3\n34.3\n\n\nLeptonycteris curosoae\n16.0\n40.2\n\n\nChoeronycteris mexicana\n18.0\n52.1\n\n\n\n\n\n\n\n3.2.2 Creating a data file\nWhen you have new data that you want to get into the computer in a format that R can read, it is often easiest to do this outside of R. A spreadsheet program like Excel (or a freely available program like Google Sheets) is a straightforward way to create a .csv file that R can read. For the lab today, we’ll use Google Sheets.\n\nLog into Google Drive using your userid@hawaii.edu credentials.\nCreate a new Google Sheet\n\n\n\nName the new sheet “BatTongues2”\n\n\n\nEnter data\n\nIn the first row of your new spreadsheet, write your variable names, one for each column. (Be sure to give them good names that will work in R following principles of spreadsheet organized in Broman & Woo (2018). Mainly, don’t have any spaces in a variable name and make sure that it doesn’t start with a number or contain punctuation marks. See Lab 01 for more about naming variables.)\nOn the rows immediately below that first row, enter the data for each individual, in the correct column. Here’s what the spreadsheet would look like for the bat data after they are entered:\n\n\nDownload as a .csv file. Go to “File &gt; Download &gt; Comma-spearate values (.csv, current sheet)”\n\n Save to the Desktop (or elsewhere that is convenient) as “BatTongues2.csv”\n\nSaving a spreadsheet in a format that R can read is very straightforward. In these tutorials, we are using .csv files (which stands for comma separated values).\n\nUpload file to RStudio Cloud\n\nIn your lab03 project, click on the “data” directory under the Files tab:\n\nThen click the Upload option:\n\nYou should see the following dialog box:\n\nClick “Choose File” and navigate to where you saved “BatTongues2.csv” and select it.\n\nClick OK. Then you should see “BatTongues2.csv” in your data directory.\n\nLook at “BatTongues2.csv” in the RStudio Cloud viewer by clicking the file name and selecting “View File”\n\nIt should something like this:"
  },
  {
    "objectID": "lab03.html#questions",
    "href": "lab03.html#questions",
    "title": "3  Workikng with data",
    "section": "3.3 Questions",
    "text": "3.3 Questions\n\nReview: Import “BatTongues2.csv” using the read.csv(), name the data.frame bat_tongues, and make a scatterplot using the ggplot2 package. It should look the same as last week unless you made a typo.\nCalculate the sample variance and standard deviation.\nThe variance and standard deviation are common descriptions of the variability in a population. If we’re plotting the data in a histogram, these are associated with the spread of histogram. By convention the population variance and standard deviations are often denoted \\(\\sigma ^ 2\\) and \\(\\sigma\\), respectively (\\(\\sigma\\), pronounced “sig-ma”, is a lowercase Greek letter). The sample variance and standard deviations are often denoted \\(s^2\\) and \\(s\\), respectively. In both cases, the standard deviation is simply the square root of the variance.\nThe equations for the sample variance and standard deviation are:\n\\[ s ^ 2 = \\frac{\\Sigma^{n}_{i=1} (Y_i - \\bar{Y}) ^ 2}{n - 1} \\] \\[ s = \\sqrt{\\frac{\\Sigma^{n}_{i=1} (Y_i - \\bar{Y}) ^ 2}{n - 1}} \\]\n\nCalculate the sample variance of the palate_length variable. Note that if you named the column something other than palate_length, you will have to use your name. Here’s a couple hints to get started:\n# Create a vector and call it Y\nY &lt;- bat_tongues$palate_length\n\n# Calculate the mean of Y\nY_bar &lt;- mean(Y)\n\n# Calculate the sample size, n\nn &lt;- length(Y)\n\n# Calculate the squared deviations (Y_i - Y_bar) ^ 2\nsquared_devs &lt;- (Y - Y_bar) ^ 2\nOnce you’re done, you can check your answer using the var() function in R\nvar(bat_tongues$palate_length)\nNow calculate the standard deviation and check your result using the sd() function.\n\n\n\nLearning when to use mean versus median. For this exercise, we’ll use a data set on leaf area from Wright et al. 2017.\n\nImport the file “wright_etal_2017.csv” from the data directory using the read.csv() function. Call the data.frame leafsize.\nMake a histogram of leaf size (units of cm\\(^2\\)) using ggplot2. The variable name is leafsize_cm2. It should look something like this: \nDescribe the distribution of leaf size. Do you expect the mean or median to be larger? Check your answer using the mean() and median() functions. Make sure to use the na.rm = TRUE argument to ignore missing data. Which value would you use to describe the location (aka central tendency) of leaf size in this data set?\nNow let’s look at the power of log-transformation. First, use the + scale_x_log10() function in ggplot2 to plot leaf size on a log\\(_{10}\\)-transformed scale. Modify your previous histogram using this function. The output should look something like this: \nNow describe the shape of this distribution. Would you expect the mean or median to be higher? Check your guesses by first creating a new column called log10_leafsize_cm2. Hint: you’ll need to use the $ operator and the log(..., base = 10) or log10(...) functions. Then use mean() and median() on the new column.\nNow, would you still use the same function (mean() or median()) you selected in part c. to describe the location of the log-transformed leaf size? Explain your answer.\n\n\n\nMake a random sample.\nLet’s assume that the data set represents the entire population of leaf sizes in the world (it doesn’t, but let’s just assume). Now we’ll look at the property of random samples from this population.\n\nTo make our lives simpler, let’s first filter out the missing values using the subset() function.\nleafsize1 &lt;- subset(leafsize, !is.na(leafsize_cm2))\nNote how we now have two data frames, leafsize and leafsize1. leafsize1 has all the same columns as the original, but without rows containing missing values. The command !is.na() (pronounced “bang! is-dot-na”) tells subset which values are NOT NA. In R, ! (“bang!”) means NOT.\nWhat is the sample size now? You can use the nrow() function to figure this out. Assign the output of nrow to a variable named n_leaves.\nTo take a random sample, all members of the population must have the same chance of being chosen for our sample. In R, the function sample() can randomly choose integers from a given range. For example, to randomly sample 5 individuals from 25 possibilities, we can use:\nsample(25, size = 5)\nNow randomly sample 5 leaf sizes and assign the output to a vector called i:\ni &lt;- sample(n_leaves, size = 5)\nThe square brackets [ ] in R let you extract portions of a vector. Extract the values of the random leaves you sampled using the square brackets and assign the output as leafsize1_sample; you can run the following code:\nleafsize1_sample &lt;- leafsize1$leafsize_cm2[i]\nUse R to calculate the mean of the leaf sizes for these 5 leaves in your random sample. The result is an estimate of the mean leaf size in your population.\nMake another random sample of 5 leaves, and calculate the mean of this sample. Did you get a different number from the mean of the first sample? Why do you think the second sample mean is different from the first?"
  },
  {
    "objectID": "lab04.html#goals",
    "href": "lab04.html#goals",
    "title": "4  The Sampling Distribution",
    "section": "4.1 Goals",
    "text": "4.1 Goals\n\nUnderstand the sampling distribution of an estimate\nInvestigate sampling error\nCalculate standard error of the mean\nCalculate confidence intervals"
  },
  {
    "objectID": "lab04.html#learning-the-tools",
    "href": "lab04.html#learning-the-tools",
    "title": "4  The Sampling Distribution",
    "section": "4.2 Learning the Tools",
    "text": "4.2 Learning the Tools\n\n4.2.1 Simulating your own sampling distributions\nJust like we did in lecture, we will simulate our own sampling distribution of the mean. For this lab, we will use the iris dataset to demonstrate how. You’ll take many random samples from the iris dataset, calculate the sample mean \\(\\bar{Y}\\) for each, and plot the distribution.\n\n4.2.1.1 Randomly sampling rows\nWe used the replicate function in lecture, but the dplyr package offers tools that are perhaps even more intuitive. You can take a random sample of rows in your data using the dplyr function slice_sample(). For example, to sample 5 rows at random from the iris data set, you would do the following:\n\nlibrary(dplyr)\n\nsamp &lt;- slice_sample(iris, n = 5)\nsamp\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          6.1         2.8          4.0         1.3 versicolor\n2          6.7         3.0          5.2         2.3  virginica\n3          6.8         3.2          5.9         2.3  virginica\n4          4.9         2.4          3.3         1.0 versicolor\n5          5.6         2.9          3.6         1.3 versicolor\n\n\nNow try on your to increase the sample size 10, how would you do this?\n\n\n4.2.1.2 Repeated sampling\nTo simulate a sampling distribution, we need to repeatedly randomly sample the “population” (in this case, we’re pretending the iris data set is the entire population). The infer package has a convenient function rep_slice_sample() that will repeat slice_sample() many times. It creates a new column called replicate to index each replicate sample. To randomly sample 5 rows 4 times from Iris setosa, we do:\n\n# first load the infer pacakge\nlibrary(infer)\n\n# let's look at just the species setosa, so we'll need to subset our data\njust_setosa &lt;- subset(iris, Species == \"setosa\")\n\nmany_setosa_samp &lt;- rep_slice_sample(just_setosa, n = 5, reps = 4)\nhead(many_setosa_samp)\n\n# A tibble: 6 × 6\n# Groups:   replicate [2]\n  replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n      &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         1          5.8         4            1.2         0.2 setosa \n2         1          5.4         3.4          1.5         0.4 setosa \n3         1          4.6         3.6          1           0.2 setosa \n4         1          4.8         3.4          1.9         0.2 setosa \n5         1          4.6         3.4          1.4         0.3 setosa \n6         2          5.1         3.8          1.5         0.3 setosa \n\n\nNote: the infer package converted the iris data.frame to something called a tibble, for our purposes, think of a data.frame and a tibble as equivalent.\nNow, let’s take 1000 samples of 10 from each species. I’ll show you the code for Iris setosa, then fill in the ___ sections below to do it for it the other species. After, you’ll need to combine the results before summarizing and plotting.\n\n# let's use `set.seed` so we can compare answers\nset.seed(123)\n\njust_setosa &lt;- subset(iris, Species == \"setosa\")\nmany_setosa_samp10 &lt;- rep_slice_sample(just_setosa, n = 10, reps = 1000)\n\njust_versicolor &lt;- subset(iris, Species == ___)\nmany_versicolor_samp10 &lt;- rep_slice_sample(___, n = 10, reps = 1000)\n\njust_virginica &lt;- subset(iris, Species == ___)\nmany_virginica_samp10 &lt;- rep_slice_sample(___, n = 10, reps = 1000)\n\nIf you used the same seed (123) in the above code then you should get these same answers:\n\nmany_setosa_samp10\n\n# A tibble: 10,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1         1          4.8         3.1          1.6         0.2 setosa \n 2         1          5.8         4            1.2         0.2 setosa \n 3         1          4.3         3            1.1         0.1 setosa \n 4         1          4.7         3.2          1.3         0.2 setosa \n 5         1          4.5         2.3          1.3         0.3 setosa \n 6         1          4.4         3.2          1.3         0.2 setosa \n 7         1          5.5         3.5          1.3         0.2 setosa \n 8         1          4.6         3.2          1.4         0.2 setosa \n 9         1          4.8         3.4          1.9         0.2 setosa \n10         1          5           3            1.6         0.2 setosa \n# ℹ 9,990 more rows\n\nmany_versicolor_samp10\n\n# A tibble: 10,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1         1          6.6         3            4.4         1.4 versicolor\n 2         1          5.7         3            4.2         1.2 versicolor\n 3         1          5.6         2.9          3.6         1.3 versicolor\n 4         1          5.5         2.6          4.4         1.2 versicolor\n 5         1          6.1         2.9          4.7         1.4 versicolor\n 6         1          4.9         2.4          3.3         1   versicolor\n 7         1          5.6         2.7          4.2         1.3 versicolor\n 8         1          5.5         2.3          4           1.3 versicolor\n 9         1          6.9         3.1          4.9         1.5 versicolor\n10         1          5.1         2.5          3           1.1 versicolor\n# ℹ 9,990 more rows\n\nmany_virginica_samp10\n\n# A tibble: 10,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    \n 1         1          5.9         3            5.1         1.8 virginica\n 2         1          6.3         2.8          5.1         1.5 virginica\n 3         1          7.1         3            5.9         2.1 virginica\n 4         1          6.4         3.1          5.5         1.8 virginica\n 5         1          6.9         3.1          5.1         2.3 virginica\n 6         1          7.2         3            5.8         1.6 virginica\n 7         1          6.3         2.9          5.6         1.8 virginica\n 8         1          6.7         3.3          5.7         2.1 virginica\n 9         1          5.8         2.7          5.1         1.9 virginica\n10         1          6.3         2.7          4.9         1.8 virginica\n# ℹ 9,990 more rows\n\n\nRemember, we only need to use set.seed in situations where we’re trying to compare output from random sampling. You can delete set.seed after you compared your output to mine.\nYou can use the rbind function to combine all three sets of sampling distributions into a single tibble.\n\niris_sample_dists &lt;- rbind(many_setosa_samp10, \n                           many_versicolor_samp10,\n                           many_virginica_samp10)\n\niris_sample_dists\n\n# A tibble: 30,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1         1          4.8         3.1          1.6         0.2 setosa \n 2         1          5.8         4            1.2         0.2 setosa \n 3         1          4.3         3            1.1         0.1 setosa \n 4         1          4.7         3.2          1.3         0.2 setosa \n 5         1          4.5         2.3          1.3         0.3 setosa \n 6         1          4.4         3.2          1.3         0.2 setosa \n 7         1          5.5         3.5          1.3         0.2 setosa \n 8         1          4.6         3.2          1.4         0.2 setosa \n 9         1          4.8         3.4          1.9         0.2 setosa \n10         1          5           3            1.6         0.2 setosa \n# ℹ 29,990 more rows\n\n\nNow we have a very large set of samples to examine.\n\n\n\n4.2.2 Sample mean \\(\\bar{Y}\\)\nTo look at the distribution of the sample mean, we first need to calculate the sample mean for all 1000 replicates per species. We will use two helpful functions from dplyr to first group the iris_sample_dists data.frame by species and replicate, and then calculate the mean for each species with the summarize function. Let’s first look at the sampling distribution of the mean of sepal length:\n\nsepal_length_sample_dists &lt;-  group_by(iris_sample_dists, Species, replicate)\nsepal_length_sample_dists &lt;- summarize(sepal_length_sample_dists, \n                                       Y_bar = mean(Sepal.Length))\n\nsepal_length_sample_dists\n\n# A tibble: 3,000 × 3\n# Groups:   Species [3]\n   Species replicate Y_bar\n   &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;\n 1 setosa          1  4.84\n 2 setosa          2  4.93\n 3 setosa          3  4.87\n 4 setosa          4  4.86\n 5 setosa          5  5.04\n 6 setosa          6  5.1 \n 7 setosa          7  5   \n 8 setosa          8  5.05\n 9 setosa          9  4.87\n10 setosa         10  5.05\n# ℹ 2,990 more rows\n\n\nModify the code above to calculate sample means for Petal.Length.\n\n\n4.2.3 Plot the sampling distribution \\(\\bar{Y}\\)\nWe can apply the ggplot() tools we’ve already learned to plot a multiple histogram to compare the sampling distributions in each species.\n\nggplot(sepal_length_sample_dists, aes(Y_bar, fill = Species)) +\n    geom_histogram(alpha = 0.5, position = \"identity\", bins = 30) +\n    scale_fill_viridis_d()\n\n\n\n\nSee if you remember how to use facet_grid() to put each Specie in it’s own panel like this:\n\n\n\n\n\n\n\n4.2.4 Standard error of the mean\nThe standard error of the mean helps us quantify our uncertainty about our estimate of the population mean given our sample size. We can calculate a hypothetical standard error for the perfect random sample of size \\(n\\) by dividing the population standard deviation by \\(\\sqrt{n}\\): \\(\\sigma / \\sqrt{n}\\). Let’s pretend that the iris data set is the entire “population”, the population standard deviation for Sepal.Length is:\n\n\n\n\n\nSpecies\n\\(\\sigma\\)\n\n\n\n\nsetosa\n0.35\n\n\nversicolor\n0.51\n\n\nvirginica\n0.62\n\n\n\n\n\nNow calculate the hypothetical standard error of the mean for a sample size of 10. You should get:\n\n\n\n\n\nSpecies\n\\(\\sigma\\)\n\\(\\sigma/\\sqrt{n}\\)\n\n\n\n\nsetosa\n0.35\n0.1106797\n\n\nversicolor\n0.51\n0.1612762\n\n\nvirginica\n0.62\n0.1960612\n\n\n\n\n\nLet’s compare this hypothetical standard error of the mean to what we obtain from our simulations. Remember that the standard error of the mean is simply the standard deviation of the sampling distribution. That means we can get the answer by using the sd() function on our simulated sampling distribution.\n\nsepal_length_se &lt;- group_by(sepal_length_sample_dists, Species)\nsepal_length_se &lt;- summarize(sepal_length_se, SE_Ybar = sd(Y_bar))\nsepal_length_se\n\n# A tibble: 3 × 2\n  Species    SE_Ybar\n  &lt;fct&gt;        &lt;dbl&gt;\n1 setosa       0.101\n2 versicolor   0.147\n3 virginica    0.182\n\n\nNotice that we had to first group by Species, then summarize by taking the standard deviation of all of our sample means.\nAre the population standard errors of the mean close to what you calculated from the simulations? Are the standard errors what you expected given the multiple histogram figure above?\n\n\n4.2.5 Sample standard error of the mean\nThe sample standard error (\\(\\mathrm{SE}_{\\bar{Y}}\\)) quantifies our uncertainty in our estimate of the population mean, \\(\\bar{Y}\\). Specifically, \\(\\mathrm{SE}_{\\bar{Y}}\\) is the standard deviation of sampling distribution for \\(\\bar{Y}\\). The equation for the \\(\\mathrm{SE}_{\\bar{Y}}\\) is the sample standard deviation divided by the square-root of the sample size:\n\\[ \\mathrm{SE}_{\\bar{Y}} = \\frac{s}{\\sqrt{n}} \\]\nThere’s no function in R to calculate \\(\\mathrm{SE}_{\\bar{Y}}\\), but you know the functions for sample standard deviation and square-root. Use R to calculate the sample standard error of the mean for the following numbers:\n\n2.16 -0.79 -0.18 1.62 -0.98 -1.15 -0.15 1.34 1.96 1.74\n\nYou should get \\(\\mathrm{SE}_{\\bar{Y}}\\) = 0.419.\n\n\n4.2.6 95% confidence intervals\nConfidence intervals are a way to show the plausible range of parameter values given the data. 95% confidence intervals will include the true population parameter 95% of the time. We’ll learn ways to calculate confidence intervals for different parameters throughout the class. Today, we’ll use the “2 SE” rule to approximate 95% confidence intervals for the sample mean \\(\\bar{Y}\\). The lower bound and upper bounds of the approximate 95% confidence interval using the 2 SE rule are:\n\\[ \\text{lower CI}: \\bar{Y} - 2 \\times \\mathrm{SE}_{\\bar{Y}} \\] \\[ \\text{upper CI}: \\bar{Y} + 2 \\times \\mathrm{SE}_{\\bar{Y}} \\] Use the mean() functions and standard error of the mean to calculate the confidence interval for the data you used in the last section. You should get:\n\n\n\n\n\n\\(\\bar{Y}\\)\nLower CI\nUpper CI\n\n\n\n\n0.557\n-0.2813638\n1.395364"
  },
  {
    "objectID": "lab04.html#questions",
    "href": "lab04.html#questions",
    "title": "4  The Sampling Distribution",
    "section": "4.3 Questions",
    "text": "4.3 Questions\nAll questions are about the sampling distribution of the sample mean, \\(\\bar{Y}\\)\n\nImport data\nWe’ll use a dataset about leaf sizes from Wright et al. (2017). We’ll pretend that this is population of all leaf sizes in the world and look at the properties of random samples from the population.\nUse the read.csv(), $, [, and/or dplyr functions to\n\nread-in the dataset\nmake a data.frame with only the latitude and leafsize_cm2 columns\nremove all rows with missing values from leafsize_cm2\nsubset the data to only tropical latitudes between -23.43655° and 23.43655°\nassign this data.frame to the name leafsize\n\nHints:\n\nto get ONE column you can use ...$column_name, to get multiple columns, you can use ...[, c(\"column_name1\", \"column_name2\")]\nremember the select function\nyou can figure out if a value is missing with the is.na function\nlatitudes between -23.43655° and 23.43655° is the same as abs(latitude) &lt; 23.43655\n\nIf you’ve done everything correctly, you should get the same values for the population mean seen below:\nmean(leafsize$leafsize_cm2)\n[1] 65.97642\nCreate 1000 replicates each of sample sizes of 64, 256, and 1024 from the leafize data.frame you generated in a. I’ll show you the code for \\(n = 64\\), then you should copy and modify it to make similar objects called sample_dist256 and sample_dist1024. Then use rbind() to combine them into an object called sample_dists.\n# create replicate samples\nsample_dist64 &lt;- rep_slice_sample(leafsize, n = 64, reps = 1e3)\n\n# add a column recording the sample size\nsample_dist64$sample_size &lt;- 64\n\n# create replicate samples\nsample_dist256 &lt;- ___\n\n# add a column recording the sample size\n___ &lt;- 256\n\n# create replicate samples\nsample_dist1024 &lt;- ___\n\n# add a column recording the sample size\n___ &lt;- 1024\n\n# combine using `rbind`\nsample_dists &lt;- ___\nUse the group_by() and summarize() functions to calculate the sample mean for each level of sample size (64, 256, or 1024) and replicate. Make sure you assign the output a name so you can use it to make a plot in the next part.\nMake a multi-panel histogram with separate panels for each sample size. It should look something like this, but will not be exactly the same because the simulations are random.\n\nHow does the location and width of the sampling distribution for \\(\\bar{Y}\\) change as \\(n\\) increases?"
  },
  {
    "objectID": "lab07.html#goals",
    "href": "lab07.html#goals",
    "title": "5  Probability",
    "section": "5.1 Goals",
    "text": "5.1 Goals\n\nExchange contact info with your groups\nBe able to estimate probabilities from data\nUse sample to simulate events with different probabilities\nUse rules of probability to evaluate if different events in a dataset are likely to be independent or not"
  },
  {
    "objectID": "lab07.html#exchange-contact-info",
    "href": "lab07.html#exchange-contact-info",
    "title": "5  Probability",
    "section": "5.2 Exchange contact info!",
    "text": "5.2 Exchange contact info!\nBefore starting the main content of this lab, take a few minutes to introduce yourselves to your group members and exchange email addresses so you can communicate about your group projects."
  },
  {
    "objectID": "lab07.html#learning-the-tools",
    "href": "lab07.html#learning-the-tools",
    "title": "5  Probability",
    "section": "5.3 Learning the Tools",
    "text": "5.3 Learning the Tools\n\n5.3.1 More fun with penguins!\nWe’ll use our old friend the Palmer penguins data set. Let’s first look at some simple probabilities:\n\nlibrary(palmerpenguins)\n\n# probability species is \"Adelie\"\nPr_adelie &lt;- sum(penguins$species == \"Adelie\") / nrow(penguins)\nPr_adelie\n\n[1] 0.4418605\n\n\nSo there is a 0.44 probability that if we grabbed a penguin out of this dataset it would belong to the Adelie species. The random trial here is “grabbing” a penguin, and the event of interest is that its species is Adelie.\nNotice how we’re calculating the probability. First we use penguins$species == \"Adelie\" to ask R to tell us TRUE ever time it finds the species is Adelie and FALSE ever time it finds the spcies to not be Adelie.\nThen we wrap those TRUE and FALSE values in sum:\n\nsum(penguins$species == \"Adelie\")\n\n[1] 152\n\n\nThat sum tells us the total number of TRUE values, aka the total number of penguins of species Adelie in the data.\nFinally we divide by nrow(penguins) to turn the count into a probability. That’s the definition of probability: the proportion of times an event is true.\nWe can also look at probabilities of numerical data, like the bill length\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- sum(penguins$bill_length_mm &lt; 45) / nrow(penguins)\nPr_bill_l_less45\n\n[1] NA\n\n\nWe got NA?! The reason is that the bill length column has some missing data. We’ve dealt with that before by using subset to remove rows with missing data. Here we will learn a new approach: telling sum to ignore NA values:\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- sum(penguins$bill_length_mm &lt; 45, na.rm = TRUE) /\n    nrow(penguins)\nPr_bill_l_less45\n\n[1] 0.5116279\n\n\nSo there is a 0.51 probability of randomly grabbing a penguin from these data and its bill being less than 45 mm long.\nInterestingly, taking the sum of something and then dividing by sample size is also the definition of the mean, so we can take a shortcut and just use the mean function:\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- mean(penguins$bill_length_mm &lt; 45, na.rm = TRUE)\nPr_bill_l_less45\n\n[1] 0.5146199\n\n\nSame answer.\nNow let’s calculate some joint probabilities. What’s the probability that the bill length is less than 45 AND the species is Adelie? To save us some typing, let’s call this probability pAl45\n\npAl45 &lt;- mean(penguins$bill_length_mm &lt; 45 &\n                  penguins$species == \"Adelie\", \n              na.rm = TRUE)\npAl45\n\n[1] 0.4314869\n\n\nInteresting, \\(Pr(\\text{bill length} &lt; 45)\\) and \\(Pr(\\text{bill length} &lt; 45 \\text{ \\& Adelie})\\) are about the same, could that mean that bill length and species are independent? To figure out, we need to calculate \\(Pr(\\text{bill length} &lt; 45)\\) and \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) and see if the two probabilities are roughly the same.\nThe expression \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) tells us that we already know the species is Adelie. That means to caculate the probability bill length &lt; 45 GIVEN Adelie, we need to ignore all the rest of the data\n\n# subest to just Adelie\njustAdelie &lt;- subset(penguins, penguins$species == \"Adelie\")\n\n# now the conditional probability, is calculated just by counting up\n# cases where bill length &lt; 45\npl45GivenA &lt;- mean(justAdelie$bill_length_mm &lt; 45, na.rm = TRUE)\npl45GivenA\n\n[1] 0.9801325\n\n\nRecall that \\(Pr(\\text{bill length} &lt; 45)\\) = 0.51 which is vastly different from \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) = 0.98 so we conclude that no, bill length and species are not independent.\nWe can confirm this by visualizing the histograms: different penguin species have different distributions of bill lengths, again confirming that the two variables are not indipendent.\n\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm, fill = species)) +\n    geom_histogram() + \n    facet_grid(rows = vars(species)) +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d()"
  },
  {
    "objectID": "lab07.html#simulating-random-events-with-sample",
    "href": "lab07.html#simulating-random-events-with-sample",
    "title": "5  Probability",
    "section": "5.4 Simulating random events with sample",
    "text": "5.4 Simulating random events with sample\nYou’ve seen us use the sample function all the time. We can take a random sample of integers:\n\n# randomly sample 3 numbers between 1 and 10\nsample(1:10, 3)\n\n[1] 7 5 4\n\n\nWe can also sample characters:\n\n# sample 5 letters from A, B, C with replacement\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE)\n\n[1] \"C\" \"A\" \"B\" \"A\" \"A\"\n\n\nTry running the above code without setting replace = TRUE\n\nsample(c(\"A\", \"B\", \"C\"), 5)\n\nYou’ll get an error like this:\n\n\nError in sample.int(length(x), size, replace, prob) : \n  cannot take a sample larger than the population when 'replace = FALSE'\n\n\nThat’s because we’re trying to sample 5 random events from only 3 possible outcomes. We have to “replace” each outcome once we sampled it.\nWe can also sample with replacement when our number of random draws is less than the total number of outcomes:\n\n# randomly sample 3 numbers between 1 and 10\nsample(1:10, 3, replace = TRUE)\n\n[1]  8 10  5\n\n\nWe can also change the frequency of different outcomes by changing the probability that R assigns to them!\n\n# randomly sample 3 numbers between 1 and 10\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE, prob = c(0.8, 0.1, 0.1))\n\n[1] \"A\" \"A\" \"A\" \"A\" \"A\"\n\n\nThe above code will yield 80% “A”, and 10% of both “B” and “C” if we run it over and over.\nThe probabilities need to sum to 1 across all the outcomes because that’s one of the rules of probability. R will automatically do that for you, even if you don’t re-scale the probabilities yourself. For example, this code is equivalent:\n\n# randomly sample 3 numbers between 1 and 10\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE, prob = c(8, 1, 1))\n\n[1] \"A\" \"A\" \"A\" \"A\" \"A\"\n\n\nThe exact events will be different because each time you run it is a random trial."
  },
  {
    "objectID": "lab07.html#questions",
    "href": "lab07.html#questions",
    "title": "5  Probability",
    "section": "5.5 Questions",
    "text": "5.5 Questions\nIn questions 1–4 you will simulate data using the sample function and use the resulting data.frame to evaluate probabilities\n\nUse sample to make a data.frame with 40 rows and 2 columns: x and y. Column x should be filled with a random sample of the the integers 1 and 2, each with equal probability; column y should be filled with a random sample of the letters A and B, each with equal probability.\nThe data.frame you made in (1.) should have two independent columns (i.e. you were not instructed to use any code that would cause the events in column y to depend on the events in column x). To confirm their independence calculate \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) and evaluate if they are close to the same value. If at first they are not, try re-running the code a few times (each time will be different, you’re making random data!). In most of the times you re-run the code, the difference between \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) should be between -0.1 and 0.1\nGiven that we simulate y and x as independent, why do we find that \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) and not exactly equal? If we simulated a data.frame with 80 rows, would you expect \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) to be more closely equal or more different? Why?\nDiscuss in 2 to 3 sentence how you could change the code in (2.) to simulate non-independence between the events in column x and column y. You do not need to implement this code, just discuss\n\nQuestions 5–7 will revisit the visualization of probabilities as overlapping circles that we used in class. NOTE: this is the same concept as what we covered in class, but the actual probabilities will be different\n\nFill in the probability figure. Use the made up data below to calculate the probabilities of all the events in the figure (e.g. what is \\(Pr(\\text{length} &gt; 40)\\)? What about \\(Pr(\\text{length} \\leq 40 \\text{ \\& species} \\neq \\text{uhu})\\)?)\n\n\n\n\n\n\n\n\n\n\nBelow, we have highlighted just two parts of the probability figure, use the above made up data to calculate the probabilities in this figure.\n\n\n\nUsing conditional probabilities (e.g. \\(Pr(A | B)\\)), would you say that the event of \\(length &gt; 40\\) and \\(species = uhu\\) are independent or not?"
  },
  {
    "objectID": "lab08.html#goals",
    "href": "lab08.html#goals",
    "title": "6  Statistical Null Hypothesis Testing",
    "section": "6.1 Goals",
    "text": "6.1 Goals\n\nReview steps of null hypothesis statistical testing\nGenerate null distributions through repeated random sampling\nTest hypotheses with null distributions"
  },
  {
    "objectID": "lab08.html#learning-the-tools",
    "href": "lab08.html#learning-the-tools",
    "title": "6  Statistical Null Hypothesis Testing",
    "section": "6.2 Learning the Tools",
    "text": "6.2 Learning the Tools\n\n6.2.1 Hypothesis testing\nThe remainder of this course works within the null hypothesis statistical testing (NHST) framework that we will introduce this week. As we will discover, there are six core steps in NHST:\n\nState \\(H_0\\) and \\(H_A\\)\nCalculate the test statistic\nGenerate the null distribution\nCalculate the \\(p\\)-value\nDecide:\n\nReject \\(H_0\\) if \\(p \\le \\alpha\\)\nFail to reject \\(H_0\\) if \\(p &gt; \\alpha\\)\n\n\nFor the statistical tests that we encounter during this course, the equations to calculate appropriate test statistics, null distribution of the test statistic, and \\(p\\)-value are well understood. However, the concepts of null hypothesis testing can often be made more obvious by making a computer simulate the null distribution for us. Such “computationally intensive” null distributions also have important applications beyond learning the concepts—there are many real-world situations where no mathematical equation has ever been derived to calculate the necessary test statistic or null distribution for specific data or null hypotheses. In these cases, the only workable solution is to use a computer to simulate a null distribution.\nLet’s have a look at some data and a hypothesis where a “computational null distribution” will both be informative for learning, and also necessary because no mathematical equation exists to define an appropriate null distribution.\n\n\n6.2.2 Coral reef fish across Polynesia\nThe Hawaiian Islands and Rapa Nui are unique in the world for their geographic isolation. We might hypothesize that this isolation makes it difficult for organisms to disperse to these islands. Even fish (turns out they swim) might have a difficult time getting there. As such, we might hypothesize that compared to the rest of Polynesia, Hawaiʻi and Rapa Nui might have fewer species of coral reef-associated fishes. Let’s test that out. We have a dataset from Barneche et. al (2019) that compiles surveys of reef fish from across the globe. The Barneche et. al data report total numbers of species found at different sites. We’ll look at just a subset of those sites to test our scientific hypothesis about species richness of reef fish in Polynesia.\nLet’s go through the steps of null hypothesis testing:\n\n\n1. State \\(H_0\\) and \\(H_A\\)\n\n\\(H_0\\): Between the two groups “Distant Polynesia” (i.e. Hawaiʻi and Rapa Nui) and “Core Polynesia”, there will be no difference in mean species richness\n\\(H_A\\): There will be a difference in mean species richness between “Distant Polynesia” and “Core Polynesia”\n\n\n\n2. Calculate the test statistic\nWhat is the test statistic? The null hypothesis says there will be no difference in the mean richness. So our test statistic will be (mean species richness of Core Polynesia) - (mean species richness of Distant Polynesia).\nTo calculate that, we need to read-in the data,\n\nreef_fish &lt;- read.csv(\"data/reef_fish.csv\")\n\n\n# have a look at the data\nView(reef_fish)\n\nWe can see there are columns for region, polynesia_isolation, site, lon (longitude), lat (latitude), and richness (species richness). The polynesia_isolation column was added special for this lab. It has values core_polynisia, distant_polynisia, and NA. NA is for everything all sites outside of Polynesia. Before we calculate our test statistic, we need to subset our data to just Polynesia. We can do that like this:\n\n# \"pol_tri\" for polynesian triangle \npol_tri_fish &lt;- subset(reef_fish, !is.na(reef_fish$polynesia_isolation))\n\n\n# have a look\nView(pol_tri_fish)\n\nNow we will use our friends group_by and summarize from dplyr to help us calculate the test statistic.\n\nlibrary(dplyr)\ngroups &lt;- group_by(pol_tri_fish, polynesia_isolation)\ngroup_means &lt;- summarize(groups, ybar = mean(richness))\n\n# have a look (this is a small data frame, so we donʻt need\n# to use the `View` function)\ngroup_means\n\n# A tibble: 2 × 2\n  polynesia_isolation  ybar\n  &lt;chr&gt;               &lt;dbl&gt;\n1 core_polynesia      105. \n2 distant_polynesia    82.5\n\n\nNow from group_means we can calculate the test statistic of the difference in the means\n\ntest_stat &lt;- diff(group_means$ybar)\ntest_stat\n\n[1] -22.5\n\n\nNote: test_stat is the mean of core_polynesia minus the mean of distant_polynesia. The fact that test_stat is negative means that on average there is higher fish species richness in core_polynesia, which aligns with out scientific hypothesis, but will we reject the statistical null or not? For that we need to…\n\n\n3. Generate the null distribution\nRemember a null distribution seeks to capture what our test statistic would look like if the null hypothesis were true. If the null hypothesis were true, then it shouldn’t matter for reef fish richness if we did a survey in Core Polynesia or in Distant Polynesia. That means we can simulate the null distribution by repeatedly shuffling the values of the column polynesia_isolation and then following the same steps for calculating calculating the test statistic. Let’s look at how we would shuffle polynesia_isolation and calculate the test statistic.\n\n# make a new copy of the data for purposes of making\n# the null distribution\npol_fish_null &lt;- pol_tri_fish\n\nNow we can re-make the polynesia_isolation column as a random shuffle of itself. First, have a look at the behavior of the sample function\n\n# make a simple vector of letters\nx &lt;- c(\"A\", \"A\", \"B\", \"B\")\nx\n\n[1] \"A\" \"A\" \"B\" \"B\"\n\n\n\nx &lt;- sample(x)\nx\n\n[1] \"A\" \"B\" \"B\" \"A\"\n\n\nWe started with x being a orderly sequence A A B B and then used sample to generate a random sequence that happens to be A B B A. If we ran sample(x) again, we’d likely get a different random reshuffling of the letters.\nNow let’s use that approach to make our null distribution. For the null distribution we need to make many many many random re-shufflings. But first we’ll look at how we do it just once:\n\n# remember we already made `pol_fish_null` as a copy\n# of the real data\n\n# reshuffle the group identities\npol_fish_null$polynesia_isolation &lt;- sample(pol_fish_null$polynesia_isolation)\n\n# follow the same steps as before for calculating the test statistic\ngroups_null &lt;- group_by(pol_fish_null, polynesia_isolation)\ngroup_means_null &lt;- summarize(groups_null, ybar = mean(richness))\n\ntest_stat_null &lt;- diff(group_means_null$ybar)\ntest_stat_null\n\n[1] -9.088235\n\n\nGreat! Now we just need to do that over and over again! Luckily we can ask R to do the work for us. We can use the replicate function to do the same task over and over. Let’s look at a simple example first.\nSuppose we want to simulate the distribution of outcomes of rolling two dice (and adding their values). We can do one roll like this:\n\ndie1 &lt;- sample(6, 1)\ndie2 &lt;- sample(6, 1)\ndie1 + die2\n\n[1] 9\n\n\nDo do that 20 times, we just copy and paste the above code into replicate:\n\ndice_rolls &lt;- replicate(20, {\n    die1 &lt;- sample(6, 1)\n    die2 &lt;- sample(6, 1)\n    die1 + die2\n})\n\ndice_rolls\n\n [1]  2 10 10 10  9  5 10  7  4  8  5  8  7  7  3  6  4  7  9  9\n\n\nCool, we got 20 random outcomes! Notice that we had to paste the three lines of code into “squiggly brackets” {} within the replicate function. That’s just our way of letting R know that those three lines of code all need to be run together, that the whole set of three lines is what we want R to compute 20 times. With this structure, we could ask R to make 20 replicates, or 20,000!\nNow we can use the same approach to make the null distribution. Let’s do 1000 re-shufelings to make our null distribution.\n\nnull_dist &lt;- replicate(1000, {\n    # reshuffle the group identities\n    pol_fish_null$polynesia_isolation &lt;- \n        sample(pol_fish_null$polynesia_isolation)\n    \n    # follow the same steps as before for calculating the test statistic\n    groups_null &lt;- group_by(pol_fish_null, polynesia_isolation)\n    group_means_null &lt;- summarize(groups_null, ybar = mean(richness))\n    \n    test_stat_null &lt;- diff(group_means_null$ybar)\n    test_stat_null\n})\n\n# the output of `replicate` will be a vector with 1000 null test statistics\n# let's just look at the first little bit\nhead(null_dist)\n\n[1]   1.058824  30.352941 -17.647059   4.588235 -18.264706  -1.235294\n\n\nLet’s visualize the null distribution as a histogram\n\nlibrary(ggplot2)\n\n# notice we need to make a data.frame in order for ggplot to work\nnull_dist_df &lt;- data.frame(null_test_stat = null_dist)\n\nggplot(null_dist_df, aes(x = null_test_stat)) +\n    geom_histogram()\n\n\n\n\n\n\n4. Calculate the \\(p\\)-value\nNow we calculate the \\(p\\)-value. Remember, our default assumption is that we are considering a two-tailed alternative hypothesis. So we need to calculate \\(Pr(\\text{lower tail}) + Pr(\\text{upper tail})\\). Our test statistic is -22.5 which is negative, so our lower tail probability will be the proportion of times that null_dist is less than or equal to test_stat, and the upper tail probability will be the proportion of times that null_dist is greater than or equal to -1 * test_stat. Let’s put that in code:\n\nlower_tail &lt;- mean(null_dist &lt;= test_stat)\nupper_tail &lt;- mean(null_dist &gt;= -1 * test_stat)\npval &lt;- lower_tail + upper_tail\npval\n\n[1] 0.043\n\n\nWow cool! our \\(p\\)-value is 0.043. What does that mean?\n\n\n5. Decide: reject \\(H_0\\) or fail to reject \\(H_0\\)\nThat is the question. Since we’re in biostatistics and the convention in our field is to set \\(\\alpha = 0.05\\), then we will reject \\(H_0\\) because \\(p \\le \\alpha\\), i.e., it is true that 0 \\(\\le 0.05\\)."
  },
  {
    "objectID": "lab08.html#questions",
    "href": "lab08.html#questions",
    "title": "6  Statistical Null Hypothesis Testing",
    "section": "6.3 Questions",
    "text": "6.3 Questions\nA common hypothesis in the study of biodiversity is that species richness is higher in the tropics. The tropics are defined as any place between the latitudes of -23.43615° and 23.43615°, i.e. 23.43615° south of the equator and 23.43615° north of the equator. Reef ecosystems largely exist only within the tropics, but there are a few interesting and unique types of reef ecosystems outside the tropic latitudes as well. That means we can use our reef fish dataset to test whether reef species richness is higher in the tropics or not.\nIn questions 1–7 you will conduct the steps of testing this scientific hypothesis with statistical null hypothesis testing.\n\nFirst we need to do some data manipulation to add a column to our reef fish data that tells us if the sites are tropical or temperate (temperate refers to areas outside the tropics). To get you started, recall this is how we can add a column:\nreef_fish$trop_or_temp &lt;- \"tropical\"\nView(reef_fish)\nNow we’ve added one column, but all the values in that column say “tropical”. We need to manipulate that new trop_or_temp column to say “temperate” for all latitudes outside the tropics. Here’s the code to do that for the Northern Hemisphere:\nreef_fish$trop_or_temp[reef_fish$lat &gt;= 23.43615] &lt;- \"temperate\"\nNow your job is to do the same for the Southern Hemisphere.\nNow that we have our column trop_or_temp telling us which group each data point belongs to, we need to state our null and alternative hypotheses\nNow calculate our test statistic\nNow we generate the null distribution. Let’s do that in two steps:\n\nRefer to the code where we shuffled the polynesia_isolation of pol_fish_null. Use that example to make one random reshuffling of the trop_or_temp column and caculate one null test statistic\nNow use the code from (4a.) and refer to the use of the replicate function to simulate a null distribution with 1000 replicates\n\nPlot a histogram of the null distribution\nCalculate the \\(p\\)-value for the two-tailed alternative hypothesis\nDecide whether we reject or fail to reject the null hypothesis"
  },
  {
    "objectID": "lab09.html#goals",
    "href": "lab09.html#goals",
    "title": "7  The Binomial Distribution and Analysis of Proportion Data",
    "section": "7.1 Goals",
    "text": "7.1 Goals\n\nGain familiarity with the Binomial Distribution\nTest null hypotheses with the Binomial Test\nGenerate and interpret 95% confidence intervals of proportions"
  },
  {
    "objectID": "lab09.html#learning-the-tools",
    "href": "lab09.html#learning-the-tools",
    "title": "7  The Binomial Distribution and Analysis of Proportion Data",
    "section": "7.2 Learning the Tools",
    "text": "7.2 Learning the Tools\n\n7.2.1 Binomial distribution\nLet’s get familiar with the binomial distribution using a classic example: flipping a coin. We will simulate the coin flipping process using sample:\n\nsample(c(\"H\", \"T\"), size = 1, replace = TRUE, prob = c(0.5, 0.5))\n\n[1] \"H\"\n\n\nThe above code simulates one coin toss that will come up either heads (\"H\") or tails (\"T\"). We are simulating a fair coin, meaning equal probabilities of heads and tails, thus we specify prob = c(0.5, 0.5). We could simulate the outcome of tossing a coin 4 times like this:\n\nsample(c(\"H\", \"T\"), size = 4, replace = TRUE, prob = c(0.5, 0.5))\n\n[1] \"H\" \"H\" \"T\" \"H\"\n\n\nNow let’s modify the code for 4 coin tosses to represent a biased coin, say one that comes up tails 70% of the time\n\nsample(c(\"H\", \"T\"), size = 4, replace = TRUE, prob = c(0.3, 0.7))\n\n[1] \"T\" \"T\" \"T\" \"H\"\n\n\nLet’s arbitrarily say that a “success” is if the coin lands tails up. We can count the number of successes like this:\n\n# get some coin tosses (we're again using a fair coin)\ntoss &lt;- sample(c(\"H\", \"T\"), size = 4, replace = TRUE, prob = c(0.5, 0.5))\ntoss\n\n[1] \"H\" \"H\" \"H\" \"T\"\n\n# count successes\nnsuccess &lt;- sum(toss == \"T\")\nnsuccess\n\n[1] 1\n\n\nThe number of successes in 4 tosses is exactly the kind of situation where a binomial distribution is relevant. In this example, the probability of success is \\(p = 0.5\\) and the number of trials is \\(n = 4\\).\nLet’s run the above code 20 times to estimate the frequencies of each potential outcome that will approximate the binomial distribution with \\(p = 0.5\\) and \\(n = 4\\).\nTo do that, simply run the above code 20 times and record the number of successes from each iteration. You can write down the success on paper, or the computer, or team up with a partner. Once you’re done, enter those data into a simple vector using the c function. You’re aiming to get something like this (you’re numbers will be different though):\n\n# vector of the number of tails across 20 trials\nntails &lt;- c(1, 2, 2, 1, 0, \n            1, 0, 2, 2, 4, \n            1, 3, 2, 2, 2, \n            1, 3, 3, 2, 2)\n\n# use the `table` function to get frequencies\nfreqs &lt;- table(ntails)\nfreqs\n\nntails\n0 1 2 3 4 \n2 5 9 3 1 \n\n\nLet’s compare our estimated frequencies to the prediction from the binomial distribution. We will use dbinom to get the probability of each outcome directly from the binomial distribution. Remember, that for \\(n = 4\\) trials, all the possible outcomes are 0, 1, 2, 3, 4, which we can write in R code as 0:4:\n\ndbinom(0:4, size = 4, prob = 0.5)\n\n[1] 0.0625 0.2500 0.3750 0.2500 0.0625\n\n\nCompare that to the frequencies divided by 20 (dividing by the number of simulations turns frequencies into probabilities)\n\nfreqs / 20\n\nntails\n   0    1    2    3    4 \n0.10 0.25 0.45 0.15 0.05 \n\n\nRemember your numbers will be different, you might in fact not even have all the outcomes represented in your simulation. What we observe is that there is a general agreement between our simulation and the true probabilities from dbinom: 2 successes is the most probable outcome, 0 and 4 are the least likely.\nLet’s also compare the probability of multiple events as estimated from our simulation and pbinom. Let’s look at the probability of 3 or fewer tails\n\n# calculate the estimated probability\nsum(ntails &lt;= 3) / 20\n\n[1] 0.95\n\n# compare to the probability given by pbinom\npbinom(3, size = 4, prob = 0.5)\n\n[1] 0.9375\n\n\nPretty close!\n\n\n7.2.2 Binomial tests and confidence intervals\nDoing a binomial test in R is much easier than doing one by hand.\nThe function binom.test() will do an exact binomial test. It requires three pieces of information in the input.\n\nx: the number of successes\nn: the number of trials\np: the proportion stated by the null hypothesis.\n\nFor example, let’s consider the hypothetical feeding trial discussed in lecture with pulelehua Kamehameha caterpillars. We imagined we did a feeding trial with 30 replicates between māmaki and Urtica. Out of those 30 trials we imagined the caterpillars choose māmaki 21 times. Let’s test the null hypothesis of no preference using binom.test. We need to tell binom.test the number of successes (x = 21), the number trials (n = 30) and the null hypothesis probability (p = 0.5).\n\nbinom.test(x = 21, n = 30, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  21 and 30\nnumber of successes = 21, number of trials = 30, p-value = 0.04277\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5060410 0.8526548\nsample estimates:\nprobability of success \n                   0.7 \n\n\nIn this case, the output of the function gives quite a bit of information. One key element that we will be looking for is the P-value; in this case R tells us that the P-value is 0.04277. This is the P-value that corresponds to a two-tailed test.\nThe binom.test() function also gives an estimate of the proportion of successes (in this case 0.7). It also gives an approximate 95% confidence interval. The method used to calculate this confidence interval is different from the method we discussed in lecture. When calculating confidence intervals for proportion data, you are welcome to use either the output of binom.test or the method we covered in lecture involving qbinom."
  },
  {
    "objectID": "lab09.html#questions",
    "href": "lab09.html#questions",
    "title": "7  The Binomial Distribution and Analysis of Proportion Data",
    "section": "7.3 Questions",
    "text": "7.3 Questions\nLet’s imagine a new feeding trial of pulelehua Kamehameha caterpillars. This time we will imagine a comparison inspired by Bongar et al. (2024). They looked at māmaki versus a close relative that happens to be invasive in Hawaiʻi: Cecropia obtusifolia. If pulelehua Kamehameha caterpillars could thrive on Cecropia obtusifolia that would be great.\nInspired by the results of Bongar et al. (2024), let’s suppose that out of 50 feeding trials where pulelehua Kamehameha caterpillars are given a choice between māmaki and Cecropia obtusifolia, they choose māmaki 38 times.\n\nUse binom.test() to calculate the estimated proportion of choosing māmaki. What is the 95% confidence interval for this proportion? What is the two-tailed \\(P\\)-value of the null hypothesis?\nBased on the results form binom.test you just produced does it seem promising that pulelehua Kamehameha caterpillars will be able to use Cecropia obtusifolia as a food plant?\nBased on the 95% confidence intervals, do you think the proportion of times the caterpillars choose māmaki over Urtica versus the proportion of times they choose māmaki over Cecropia obtusifolia are different from one another? Explain your answer. The hypothetical experiment comparing choice between māmaki and Urtica comes from our lectures, please refer to lecture slides for details on this hypothetical experiment.\nFind your own proportion data on the Internet or at home if you are doing this after lab. Develop a null hypothesis, state it in your report, and use binom.test() to test it. Did you reject your null hypothesis? Explain why or why not."
  },
  {
    "objectID": "lab10.html#goals",
    "href": "lab10.html#goals",
    "title": "8  Frequency and Contingency Analysis",
    "section": "8.1 Goals",
    "text": "8.1 Goals\n\nGain familiarity with the \\(\\chi^2\\) Distribution\nCalculate the \\(\\chi^2\\) test statistic and associated degrees of freedom\nTest null hypotheses with the \\(\\chi^2\\) null distribution"
  },
  {
    "objectID": "lab10.html#learning-the-tools",
    "href": "lab10.html#learning-the-tools",
    "title": "8  Frequency and Contingency Analysis",
    "section": "8.2 Learning the Tools",
    "text": "8.2 Learning the Tools\n\n8.2.1 The \\(\\chi^2\\) distribution\nLet’s get familiar with the \\(\\chi^2\\) distribution. We’ll also learn about a new kind of R function, one for generating random samples from any probability distribution. Because we’re focused on the \\(\\chi^2\\) distribution, we’ll use the random sampling function for that distribution: rchisq. We can use rchisq to generate a random sample of any size from a \\(\\chi^2\\) distribution with any degrees of freedom using a classic example: flipping a coin. We will simulate the coin flipping process using sample:\n\nrchisq(10, df = 3)\n\n [1] 7.2124230 1.4098802 1.0806319 4.1717771 3.9022538 1.2359550 3.1626169\n [8] 1.9682343 2.5572440 0.8559613\n\n\nThe above is a random sample of 10 numbers from the \\(\\chi^2\\) distribution with 3 degrees of freedom.\nWe can take a larger sample, put it in a data.frame, and make a histogram to see approximately what the \\(\\chi^2\\) distribution with 3 degrees of freedom looks like:\n\n# generate the sample\nchi2_df3 &lt;- rchisq(1000, df = 3)\n\n# put it in a data.frame\nsamp &lt;- data.frame(chi2 = chi2_df3)\n\n# plot it\nlibrary(ggplot2)\n\nggplot(samp, aes(x = chi2)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nCool! Let’s keep building on this and visualize the histograms of both df = 3 and df = 6. We can do this by adding to our samp data.frame, both extra rows for a random sample with df = 6 and also a column to keep track of the degrees of freedom.\n\n# first add a column for degrees of freedom\nsamp$df &lt;- 3\n\n# now make a new data.frame for df = 6 (next we'll combine the data.frames)\nchi2_df6 &lt;- rchisq(1000, df = 6)\nsamp_df6 &lt;- data.frame(chi2 = chi2_df6, df = 6)\n\n# now add that new data.frame onto `samp`\nsamp &lt;- rbind(samp, samp_df6)\n\n\n# visualize the results \nView(samp)\n\nNow we can make a faceted histogram to see how degrees of freedom impacts the shape of the \\(\\chi^2\\) distribution\n\nggplot(samp, aes(x = chi2)) +\n    geom_histogram() +\n    facet_grid(rows = vars(df))\n\n\n\n\n\n\n\n\nAs we expected, the bigger the degrees of freedom, the more the distribution shifts to the right.\nLet’s also calculate some probabilities from these random samples. Let’s calculate the probability of observing a \\(\\chi^2\\) test statistic of 7.8 or greater given 3 degrees of freedom\n\n# recall probability is just the proportion of times an event happens\nn_event &lt;- sum(chi2_df3 &gt;= 7.8)\nn_event / length(chi2_df3)\n\n[1] 0.041\n\n\nSo the probability \\(Pr(\\chi^2 \\ge 7.8 | df = 3) \\approx\\) 0.041. We are working with random samples here, so your numbers will likely be a little different. If \\(Pr(\\chi^2 \\ge 7.8 | df = 3) \\approx\\) 0.041 seems suspiciously close to 0.05 that’s because I choose 7.8 very deliberately. 7.8 is approximately the critical value for a \\(\\chi^2\\) distribution with 3 degrees of freedom.\nRecall that a critical value is the value that cuts a probability distribution at the desired \\(\\alpha\\), AKA significance level. To calculate the critical value we use qchisq:\n\n# this returns the value that divides a chi-sqrd distribution\n# into 95% to the left, 5% to the right\nqchisq(0.95, df = 3)\n\n[1] 7.814728\n\n# this does the same\nqchisq(0.05, df = 3, lower.tail = FALSE)\n\n[1] 7.814728\n\n\n\n\n8.2.2 \\(\\chi^2\\) goodness of fit test\nHere’s a question: does the ethnic composition of faculty at the University of Hawaiʻi match the ethnic composition of the state? If not, then something funny might be going on like racist hiring practices, or systemic racism leading to discrepancies in higher education leading to different proportions of qualifications (e.g. holding a graduate degree) across ethnicities.\nThere are also acceptable reasons for differential ethnic representation, such as if UH wants to have thriving Asian Studies or Black/African Studies or Hawaiian Studies programs, we will need to hire qualified faculty for those positions who will likely come from those ethnic backgrounds more often than not.\nIt should be noted that discrepancies in ethnic group representation in skilled labor jobs is sometimes used to claim there are “biological” differences between ethnic groups in terms of intelligence. This is false. A great read on this topic is Superior by Angela Saini*. I wish I could say that “historically there were claims about ‘biological’ differences…” but these claims persist all too readily into our modern times.\nSo let’s get to it!\nHere are the data on ethnic proportions in the Hawaiʻi and counts of different ethnic groups who represent our faculty at UH:\n\n\n\n\n\n\n\n\n\n\nethnicity\nstate_pop_proportion\nnumber_uh_faculty\n\n\n\n\nAmerican Indian and Alaska Native\n0.016\n41\n\n\nBlack or African American\n0.025\n63\n\n\nChinese\n0.100\n349\n\n\nFilipino\n0.178\n247\n\n\nJapanese\n0.162\n544\n\n\nKorean\n0.024\n135\n\n\nNative Hawaiian\n0.152\n453\n\n\nOther Pacific Islander\n0.030\n42\n\n\nVietnamese\n0.007\n18\n\n\nWhite\n0.306\n1670\n\n\n\n\n\nThe source for population proportions in Hawaiʻi comes from the state of Hawaiʻi report “Demographic, Social, Economic, and Housing Characteristics for Selected Race Groups in Hawaii” and the numbers of faculty by ethnic group comes from the UH Mānoa Institutional Research Office. Because those two reporting bodies divide ethnicities slightly differently or do not report on others I had to leave some groups out entirely (e.g. Indian, not reported by the State).\nThese data are in your data folder in posit cloud, you can read them in like this\n\nuh_fac &lt;- read.csv(\"data/uh_faculty_ethnicity.csv\")\n\nHow would we use a \\(\\chi^2\\) goodness of fit test for these data? We’re going to leave that to you to answer in the Questions section. Here, we’ll look at a simpler example. First let’s make that example. Suppose we have data on 4 groups A, B, C, D, and their expected proportions. We can make such a data set like this:\n\nfake_data &lt;- data.frame(group = c(\"A\", \"B\", \"C\", \"D\"), \n                        prop = c(0.15, 0.5, 0.05, 0.3), \n                        count = c(15, 17, 10, 8))\n\n# have a look\nfake_data\n\n  group prop count\n1     A 0.15    15\n2     B 0.50    17\n3     C 0.05    10\n4     D 0.30     8\n\n\nNow we can walk through the steps of calculating a \\(\\chi^2\\) statistic and comparing it to the null distribution.\n\n8.2.2.1 Calculate the \\(\\chi^2\\) statistic\nFirst we need the expected counts\n\n# just the expected proportion times total number of observations\nfake_data$expected &lt;- fake_data$prop * sum(fake_data$count)\n\nfake_data\n\n  group prop count expected\n1     A 0.15    15      7.5\n2     B 0.50    17     25.0\n3     C 0.05    10      2.5\n4     D 0.30     8     15.0\n\n\nWe can see that we’re not violating the assumptions of the \\(\\chi^2\\) distribution so let’s calculate the \\(\\chi^2\\) test statistic:\n\nchi2stat &lt;- sum((fake_data$count - fake_data$expected)^2 / \n                    fake_data$expected)\n\n\n\n8.2.2.2 Identify the null distribution\nWe’re dealing with frequency data, and doing a lab about \\(\\chi^2\\) distributions, so our null distribution is a \\(\\chi^2\\) distribution, but with what degrees of freedom? There are 4 groups, so df = 3.\n\n\n8.2.2.3 Calculate the \\(P\\)-value and decide if we reject the null\n\npchisq(chi2stat, df = 3, lower.tail = FALSE)\n\n[1] 8.147653e-08\n\n\nThat’s a tiny \\(P\\)-value, so yes, we reject the null at an \\(\\alpha = 0.05\\) level."
  },
  {
    "objectID": "lab10.html#contingency-analysis",
    "href": "lab10.html#contingency-analysis",
    "title": "8  Frequency and Contingency Analysis",
    "section": "8.3 Contingency analysis",
    "text": "8.3 Contingency analysis\nContingency analysis is very similar to a \\(\\chi^2\\) goodness of fit test, but in contingency analysis we have 2 categorical variables. We still compare the observed frequencies with the expected using a \\(\\chi^2\\) test statistic, and we still use a \\(\\chi^2\\) distribution for the null distribution.\nOur question is often a little different: with contingency analysis we are often asking, is there an association between these two categorical variables?\nFor example, do manu o Kū prefer nesting in certain types of trees? Manu o Kū are the friendly (yes I am anthropomorphism) little white terns that fly around campus. Last year, students went out and looked in random trees to see if the manu o Kū were nesting in the trees. Do the manu prefer a certain species? Here the two categorical variables are tree species and yes/no is there a nest. Here is a peek at the data\n\n\n\n\n\ntree\nnest\n\n\n\n\nAlbizia\nno\n\n\nKukui\nno\n\n\nCassia\nno\n\n\nKukui\nno\n\n\nAlbizia\nno\n\n\nCassia\nyes\n\n\n\n\n\nThat is just the first few rows, there are 183 rows in total. How do we do a \\(\\chi^2\\) contingency analysis with these kind of data? Again, let’s get there using a simpler fake dataset\n\nfake_2var &lt;- data.frame(group1 = sample(c(\"A\", \"B\"), 25, replace = TRUE), \n                        yes_no = sample(c(\"yes\", \"no\"), 25, replace = TRUE))\n\nfake_2var\n\n   group1 yes_no\n1       A     no\n2       B     no\n3       A    yes\n4       A     no\n5       B     no\n6       B     no\n7       A     no\n8       A     no\n9       A     no\n10      A     no\n11      B    yes\n12      A    yes\n13      A     no\n14      B     no\n15      B    yes\n16      B     no\n17      B     no\n18      A    yes\n19      B     no\n20      A     no\n21      A    yes\n22      B    yes\n23      B    yes\n24      B    yes\n25      B    yes\n\n\nWe will use the chisq.test function for the contingency analysis rather than calculate everything by hand. The function chisq.test can be used in one of two ways.\nFirst, we can calculate the contingency table from the data and pass that table to chisq.test:\n\n# calculate the contingency table\nfake_tab &lt;- table(fake_2var)\n\n# have a look, remember this is random so yours may be different\nfake_tab\n\n      yes_no\ngroup1 no yes\n     A  8   4\n     B  7   6\n\n\nNow we can pass this to chisq.test\n\nchisq.test(fake_tab)\n\nWarning in chisq.test(fake_tab): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  fake_tab\nX-squared = 0.060096, df = 1, p-value = 0.8063\n\n\nNotice, we are again getting a warning that the assumptions of the \\(\\chi^2\\) test might not be met. That’s ok, this is just an example.\nThe other way we can use the chisq.test function is by directly passing it the categorical variables\n\nchisq.test(fake_2var$group1, fake_2var$yes_no)\n\nWarning in chisq.test(fake_2var$group1, fake_2var$yes_no): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  fake_2var$group1 and fake_2var$yes_no\nX-squared = 0.060096, df = 1, p-value = 0.8063\n\n\nThat really couldn’t be simpler! Under the hood, chisq.test is calculating the contingency table for us.\nWe might as well touch on what to do in real life if you get the warning about the \\(\\chi^2\\) test assumptions not being met. You can instead use a test called “Fisher’s exact test.” This approach is more computationally intensive than the \\(\\chi^2\\) test, so back in the day people didn’t like doing it, but now there’s really no harm. The code is the same too, just swap out the function name:\n\nfisher.test(fake_2var$group1, fake_2var$yes_no)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  fake_2var$group1 and fake_2var$yes_no\np-value = 0.6882\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.2610602 11.8938828\nsample estimates:\nodds ratio \n  1.677387 \n\n\nYou can see it tells us some extra stuff, but our primary concern for now is the \\(P\\)-value, which is still very non-significant."
  },
  {
    "objectID": "lab10.html#questions",
    "href": "lab10.html#questions",
    "title": "8  Frequency and Contingency Analysis",
    "section": "8.4 Questions",
    "text": "8.4 Questions\n\nKeep building on the code you ran to make samp. Add another random sample of 1000 numbers, but this time from a \\(\\chi^2\\) distribution with 9 degrees of freedom.\nMake a faceted histogram with all three distributions captured in samp.\nCalculate the critical value for \\(\\alpha = 0.05\\) for the \\(\\chi^2\\) distribution with 9 degrees of freedom\nUse R code to estimate the probability from your random sample from the \\(\\chi^2\\) distribution with 9 degrees of freedom of values greater than or equal to the critical value you just calculated\nFollowing the steps we used to do a \\(\\chi^2\\) goodness of fit test for fake_data, preform a \\(\\chi^2\\) goodness of fit test to answer the question of whether the ethnic composition of UH faculty resembles the ethnic composition of Hawaiʻi. What do you conclude?\nFollowing the steps we used to do a contingency analysis for fake_2var, preform a contingency analysis for the manu o Kū nesting data. Do the manu have a preference for which type of tree to nest in, or do they seem to choose at random? You can read in those data like this:\n\n\nmanuoku &lt;- read.csv(\"data/manuoku.csv\")"
  },
  {
    "objectID": "lab12.html#goals",
    "href": "lab12.html#goals",
    "title": "9  The normal distribution and sample means",
    "section": "9.1 Goals",
    "text": "9.1 Goals\n\nVisualize properties of the normal distribution.\nUnderstand the Central Limit Theorem.\nCalculate sampling properties of sample means.\nDecide whether a data set likely comes from a normal distribution"
  },
  {
    "objectID": "lab12.html#learning-the-tools",
    "href": "lab12.html#learning-the-tools",
    "title": "9  The normal distribution and sample means",
    "section": "9.2 Learning the Tools",
    "text": "9.2 Learning the Tools\nThis week we will mainly focus on some exercises to better understand the nature of the normal distribution. We will also learn a couple of tools that help us decide whether a particular data set is likely to have come from population with an approximately normal distribution.\nMany statistical tests assume that the variable being analyzed has a normal distribution. Fortunately, many of these tests are fairly robust to this assumption—that is, they work reasonably well even when this assumption is not quite true, especially when sample size is large. Therefore it is often sufficient to be able to assess whether the data come from a distribution whose shape is even approximately normal (the bell curve).\nA good way to start is to simply visualize the frequency distribution of the variable in the data set by drawing a histogram. Let’s use the age of passengers on the Titanic for our example.\n\ntitanic_data &lt;- read.csv(\"data/titanic.csv\" )\n\nRemember we can use ggplot() to draw histograms.\n\nggplot(titanic_data, aes(x = age)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\nLooking at this histogram, we see that the frequency distribution of the variable is not exactly normal; it is slightly asymmetric and there seems to be a second mode near 0. On the other hand, like the normal distribution, the frequency distribution has a large mode near the center of the distribution, frequencies mainly fall off to either side, and there are no outliers. This is close enough to normal that most methods would work fine.\n\n9.2.1 QQ Plot\nAnother graphical technique that can help us visualize whether a variable is approximately normal is called a quantile plot (or a QQ plot). The QQ plot shows the data on the vertical axis ranked in order from smallest to largest (“sample” in the figure below). On the horizontal axis, it shows the expected value of an individual with the same quantile if the distribution were normal (“theoretical” in the same figure). The QQ plot should follow more or less along a straight line if the data come from a normal distribution (with some tolerance for sampling variation).\nQQ plots can be made in R using a function called geom_qq(). Add geom_qq_line() to draw a line through that QQ plot to make the linear relationship easier to see. The only weird thing about these geom’s is that you need to specify sample in the aes() function.\n\nggplot(titanic_data, aes(sample = age)) + \n  geom_qq() +\n  geom_qq_line()\n\n\n\n\nThis is what the resulting graph looks like for the Titanic age data. The dots do not land along a perfectly straight line. In particular the graph curves at the upper and lower end. However, this distribution definitely would be close enough to normal to use most standard methods, such as the t-test.\nIt is difficult to interpret QQ plots without experience. One of the goals of today’s exercises will be to develop some visual experience about what these graphs look like when the data is truly normal. To do that, we will take advantage of a function built into R to generate random numbers drawn from a normal distribution. This function is called rnorm().\nThe function rnorm() will return a vector of numbers, all drawn randomly from a normal distribution. It takes three arguments:\n\nn: how many random numbers to generate (the length of the output vector)\nmean: the mean of the normal distribution to sample from\nsd: the standard deviation of the normal distribution\n\nFor example, the following command will give a vector of 20 random numbers drawn from a normal distribution with mean 13 and standard deviation 4:\n\nrnorm(n = 20, mean = 13, sd = 4)\n\n [1] 16.792979 16.126867 13.015562 16.009959 10.404565 13.589773 14.282029\n [8] 13.915484 11.675587 19.807885 12.764934 17.054901  9.863561 18.769282\n[15]  5.328493  7.181308 17.909157 12.561528 11.910125 11.944393\n\n\nLet’s look at a QQ plot generated from 100 numbers randomly drawn from a normal distribution:\n\nnormal_data &lt;- data.frame(Y = rnorm(n = 100, mean = 13, sd = 4))\nggplot(normal_data, aes(sample = Y)) + \n  geom_qq() +\n  geom_qq_line()\n\n\n\n\nThese points fall mainly along a straight line, but there is some wobble around that line even though these points were in fact randomly sampled from a known normal distribution. With a QQ plot, we are looking for an overall pattern that is approximately a straight line, but we do not expect a perfect line. In the exercises, we’ll simulate several samples from a normal distribution to try to build intuition about the kinds of results you might get.\nWhen data are not normally distributed, the dots in the quantile plot will not follow a straight line, even approximately. For example, here is a histogram and a QQ plot for the population size of various counties, from the data in wright_etal_2017.csv. These data are very skewed to the right, and do not follow a normal distribution at all.\n\nleafsize &lt;- read.csv(\"data/wright_etal_2017.csv\")\n\nggplot(leafsize, aes(x = leafsize_cm2)) + \n  geom_histogram()\n\n\n\nggplot(leafsize, aes(sample = leafsize_cm2)) + \n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n9.2.2 Transformations\nWhen data are not normally distributed, we can try to use a simple mathematical transformation on each data point to create a list of numbers that still convey the information about the original question but that may be better matched to the assumptions of our statistical tests. We’ll see more about such transformations in chapter 13 of Whitlock and Schluter, but for now let’s learn how to do one of the most common data transformations, the log-transformation.\nWith a transformation, we apply the same mathematical function to each value of a given numerical variable for individual in the data set. With a log-transformation, we take the logarithm of each individual’s value for a numerical variable.\nWe can only use the log-transformation if all values are greater than zero. Also, it will only improve the fit of the normal distribution to the data in cases when the frequency distribution of the data is right-skewed.\nTo take the log transformation for a variable in R is very simple. We simply use the function log(), and apply it to the vector of the numerical variable in question. For example, to calculate the log of age for all passengers on the Titanic, we use the command:\n\nlog(titanic_data$age)\n\nThis will return a vector of values, each of which is the log of age of a passenger."
  },
  {
    "objectID": "lab12.html#activities",
    "href": "lab12.html#activities",
    "title": "9  The normal distribution and sample means",
    "section": "9.3 Activities",
    "text": "9.3 Activities\n\n9.3.1 Activity 1: Distribution of sample means\nWe return to the applet we used in chapter 3, located at http://www.zoology.ubc.ca/~whitlock/Kingfisher/SamplingNormal.htm to investigate three points made in the text.\nPoint 1: The distribution of sample means is normal, if the variable itself has a normal distribution.\nFirst, hit “COMPLETE SAMPLE OF 10” and “CALCULATE MEAN” a few times, to remind yourself of what this applet does. (It takes a sample from the normal distribution shown when you click “SHOW POPULATION”. The top panel shows a histogram of that sample, and the bottom panel shows the distribution of sample means from all the previous samples.)\nNext, hit the “MEANS FOR MANY SAMPLES” button. This button makes a large number of separate samples at one go, all of the same sample size, to save you from making the samples one by one. Notice that the sample mean differs from sample to sample. The sample mean produced by random sampling from a probability distribution is itself a random variable.\nLook at the distribution of sample means. Does it seem to have a normal distribution? Click the checkbox by “SHOW SAMPLING DISTRIBUTION” off to the right, which will draw the curve for a normal distribution with the same mean and variance as this distribution of sample means.\nPoint 2: The standard deviation of the distribution of sample means is reduced with larger sample sizes.\nThe standard deviation of the population is controlled by the right slider marked with \\(\\sigma\\). The sample size is set by left slider. (The default when it opens is set to \\(n=10\\).)\nFor \\(n=10\\), have the applet calculate a large number of sample means as you did in the previous exercise. If each sample size is 10 and the standard deviation is 30 (as in the default), what do you predict the standard deviation of the sample means to be? (Use the equation you have learned in class to make this calculation.)\nChange the sample size to \\(n=100\\), and recalculate many sample means. Calculate the predicted standard deviation of all the sample means. Should the sampling distribution of sample means be wider or narrower with this larger sample size than in the previous case with a smaller sample size? What do you observe in the simulations?\nPoint 3: The distribution of sample means is approximately normal no matter what the distribution of the variable, as long as the sample size is large enough. (The Central Limit Theorem)\nLoad another web page: http://www.zoology.ubc.ca/~whitlock/Kingfisher/CLT.htm\nThis will simulate a very skewed distribution of data, showing the number of cups of coffee drunk per week for a population of university students. Click on “COFFEE” to see the distribution of the variable among individual students. Describe the ways that this looks different from a normal distribution. Set \\(n=2\\) for the sample size, and simulate many sample means. Does the distribution of sample means look normal? Is it closer to normal in its shape than the distribution of individuals in the population?\nNow set \\(n=25\\) and simulate many sample means. How does the distribution of sample means look now? It should look much more like a normal distribution, because of the Central Limit Theorem."
  },
  {
    "objectID": "lab12.html#questions",
    "href": "lab12.html#questions",
    "title": "9  The normal distribution and sample means",
    "section": "9.4 Questions",
    "text": "9.4 Questions\n\nLet’s use R’s random number generator for the normal distribution to build intuition for how to view and interpret histograms and QQ plots. Remember, the lists of values generated by rnorm() come from a population that truly have a normal distribution.\n\nGenerate a list of 10 random numbers from a normal distribution with mean 15 and standard deviation 3, using the following command:\nnormal_data &lt;- data.frame(Y = rnorm(n = 10, mean = 15, sd = 3))\nUse geom_histogram() to plot a histogram of these numbers from part a. You will want to change binwidth for visual clarity.\nPlot a QQ plot from the numbers in part a.\nRepeat a through c several times (at least a dozen times). For each, look at the histograms and QQ plots. Think about the ways in which these look different from the expectation of a normal distribution and remember that each of these samples comes from a truly normal population.\n\nRepeat the procedures of Question 1, except this time have R sample 250 individuals for each sample. You can use the same command as in Question 1, but now set n = 250. Do the graphs and QQ plots from these larger samples look more like the normal expectations than the smaller sample you already did? Why do you think that this is?\nThe file “mammals.csv” contains information on the body mass of various mammal species.\n\nUse ggplot() to plot the distribution of body mass, and describe its shape. Does this look like it has a normal distribution?\nUse geom_qq() and geom_qq_line() to plot a QQ plot for body mass. Does the data fall approximately along a straight line in the QQ plot? If so, what does this imply about the fit of these data to a normal distribution?\nTransform the body mass data with a log-transformation. Repeat steps (a) and (b) on the transformed data, does the log-transformation bring the data closer to normal or further from it?\nCalculate the mean of log body mass and a 95% confidence interval for this mean. (You may want to refer back to Lab 4 for the R commands to do this.)"
  },
  {
    "objectID": "lab11.html#goals",
    "href": "lab11.html#goals",
    "title": "10  R functions quick reference sheet",
    "section": "10.1 Goals",
    "text": "10.1 Goals\n\nHelp make your own quick reference sheet of the most important R functions we’ve gone over so far\n\nThis lab will be a little different from the others. In the Learning the Tools section we’ll take a guided tour of the start of a quick reference sheet. Then in the Questions section, you’ll be tasked with completing the reference sheet."
  },
  {
    "objectID": "lab11.html#learning-the-tools",
    "href": "lab11.html#learning-the-tools",
    "title": "10  R functions quick reference sheet",
    "section": "10.2 Learning the Tools",
    "text": "10.2 Learning the Tools\nAfter completing this lab you will have a quick reference guide for the following functions/commands:\n“Base R” stuff (i.e. you don’t need to load a package for these functions/commands)\n\n#\nlibrary\n&lt;-\nc_______________________[q1]\n:\nmean____________________[q2]\nsd______________________[q3]\nsum_____________________[q4]\nlength\ndata.frame\nnrow____________________[q5]\nhead\nView____________________[q6]\n$_______________________[q7]\n==\n&lt;=______________________[q8]\n&lt;_______________________[q9]\n&gt;=______________________[q10]\n&gt;_______________________[q11]\n!=______________________[q12]\nsubset\n[\nsample__________________[q13]\n\nFunctions from package dplyr\n\ngroup_by\nsummarize\n\nFunctions from package ggplot2\n\nggplot\naes\ngeom_point\ngeom_bar________________[q14]\ngeom_boxplot____________[q15]\ngeom_histogram__________[q16]\nvars\nfacet_grid\nscale_color_viridis_c\nscale_color_viridis_d___[q17]\n\nFunctions and commands that have [qXYZ] next to them will be saved for you as questions, all the rest we will cover together here in Learning the Tools. Let’s dive in!\n\n10.2.1 #\nDefinition: The special “comment” character, this charachter tells R to ignore what you’re typing, those words are for human eyes only\nExamples:\n\n# this is a comment, use it to explain your code for a human to understand\n# each new line of a comment needs a new `#`\n\n\n\n10.2.2 library\nDefinition: Use library to load a package, this allows you to use the functions defined by that package, and any data contained in that package.\nExamples:\n\n# load a package for its data\nlibrary(palmerpenguins)\n\n# now we can, e.g., use the `penguins` data from *palmerpenguins* \n# in a plot\n\n# first load the package *ggplot2*\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm)) + \n    geom_histogram()\n\n\n\n\n\n\n10.2.3 &lt;-\nDefinition: The special “assignment” character, tells R to assign the stuff on the right hand side to the name on the left hand side\nExamples:\n\nany_name_you_want &lt;- \"anything you want, like this string of characters\"\nany_name_you_want\n\n[1] \"anything you want, like this string of characters\"\n\n\n\n\n10.2.4 :\nDefinition: A quick way to make a vector of consecutive integers\nExamples:\n\nx &lt;- 1:3\nx\n\n[1] 1 2 3\n\n\n\n\n10.2.5 length\nDefinition: Tells you the length of a vector\nExamples:\n\ny &lt;- c(\"a\", \"b\", \"c\")\nlength(y)\n\n[1] 3\n\n\n\n\n10.2.6 data.frame\nDefinition: A function to create a data.frame object. You tell it what columns you want by passing the names and values of the columns as arguments\nExamples:\n\n# note: we will make columns called \"group\" and \"measurement\" and to give\n#       values to those columns we use the equal size (=), **NOT** the \n#       assignment character (&lt;-)\nmy_data &lt;- data.frame(group = c(\"a\", \"b\", \"c\"), \n                      measurement = 1:3)\n\nmy_data\n\n  group measurement\n1     a           1\n2     b           2\n3     c           3\n\n\n\n\n10.2.7 head\nDefinition: Look at the first few rows a data.frame\nExamples:\n\nmy_data &lt;- data.frame(group = c(\"a\", \"b\", \"c\"), \n                      measurement = 1:3)\n\n# with this short data.frame `head` will just print all the rows,\n# for a longer data.frame, only the first 6 rows will be printed\nhead(my_data)\n\n  group measurement\n1     a           1\n2     b           2\n3     c           3\n\n\n\n\n10.2.8 ==\nDefinition: Asks a “yes/no” question: does the object on the left hand size equal the object on the right hand side, yes (TRUE) or no (FALSE)?\nExamples:\n\n1 == 2\n\n[1] FALSE\n\n# also works for each element of vectors, but the vectors need to be the\n# same length, and be the same type (e.g. both numeric)\nz &lt;- c(1, 5, 7)\nw &lt;- c(1, 2, 7)\n\nz == w\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n10.2.9 &\nDefinition: Combines “TRUE/FALSE” questions with “and”. Like “is this TRUE, and this TRUE?” If either is not true, the & gives us FALSE\nExamples:\n\nmy_data &lt;- data.frame(group = c(\"a\", \"b\", \"c\"), \n                      measurement = 1:3)\n\nmy_data$group == \"a\" & my_data$measurement &lt;= 2\n\n[1]  TRUE FALSE FALSE\n\n\n\n\n10.2.10 |\nDefinition: Combines “TRUE/FALSE” questions with “or”. Like “is this TRUE, or this TRUE?” If either is true, the | gives us TRUE\nExamples:\n\nmy_data &lt;- data.frame(group = c(\"a\", \"b\", \"c\"), \n                      measurement = 1:3)\n\nmy_data$group == \"a\" | my_data$measurement &lt;= 2\n\n[1]  TRUE  TRUE FALSE\n\n\n\n\n10.2.11 is.na\nDefinition: Asks a “TRUE/FALSE” question: is a value missing (NA) or not?\nExamples:\n\nmy_values &lt;- c(1.1, 4.3, NA)\nis.na(my_values)\n\n[1] FALSE FALSE  TRUE\n\n\n\n\n10.2.12 subset\nDefinition: Returns a subset of a data.frame based on some condition being met. The condition is usually whether the values in one of the vectors meet some kind of criterion\nExamples:\n\nmy_data &lt;- data.frame(group = c(\"a\", \"b\", \"c\"), \n                      measurement = 1:3)\n\nmy_data_sub &lt;- subset(my_data, my_data$measurement &lt;= 2)\nmy_data_sub\n\n  group measurement\n1     a           1\n2     b           2\n\n\n\n\n10.2.13 [\nDefinition: Called the “square bracket” this command is used to extract specific elements of a vector or data.frame\nExamples:\n\ny &lt;- c(\"a\", \"b\", \"c\")\ny[1:2]\n\n[1] \"a\" \"b\"\n\n\n\n\n10.2.14 group_by\nDefinition: A function from the dplyr package, this function tells R to split a data.frame into groups according to one or more of the columns in the data.frame. This function is almost always used in combination with summarize. We will show an example of both working together\n\n\n10.2.15 summarize\nDefinition: A function provided by the package dyplyr to calculate a statistic from the column or columns of a data.frame. This function is almost always used in combination with group_by\nExamples:\n\n# load package to get access to `penguins` data set\nlibrary(palmerpenguins)\n\n# load package to get access to functions\nlibrary(dplyr)\n\n# group the penguins by their species ID\npenguis_by_species &lt;- group_by(penguins, species)\n\n# calculate a statistic like the mean bill length\n# notice that we're telling the `mean` function to ignore NA values by\n# specifying `na.rm = TRUE`\nbill_by_species &lt;- summarize(penguis_by_species, \n                             mean_length = mean(bill_length_mm, na.rm = TRUE))\n\n\n\n10.2.16 ggplot\nDefinition: The main function from package ggplot2 for setting up a plot. This function doesn’t make the plot itself, but it tells R what data.frame to use and which columns from that data.frame will go on the x and y axes. The x and y axes are set up wit the aes function. We will show an example after we define aes and the geom_* family of functions\n\n\n10.2.17 aes\nDefinition: The function from package ggplot2 that tells R what data will go across the x and y axes\n\n\n10.2.18 geom_point\nDefinition: The function from package ggplot2 that draws a scatter plot of points\nExamples:\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    geom_point()\n\n\n\n\n\n\n10.2.19 vars\nDefinition: A function from package ggplot2 that is used to refer to a specific column when doing complex plotting like adding facets or colors that display information from the data. This will be more clear with the example from the facet_grid function\n\n\n10.2.20 facet_grid\nDefinition: A function from package ggplot2 that allows us to make multiple plots all at once based on a categorical column in our data set (each unique category in the data will get its own plot)\nExamples:\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    geom_point() +\n    facet_grid(vars(species)) # notice that we have to use `vars` here\n\n\n\n\n\n\n10.2.21 scale_color_viridis_c\nDefinition: A function from package ggplot2 that creates a color scale corresponding to a continuous variable. The “viridis” color palette is a very useful color scale because is very well aligned to how humans perceive color changes\nExamples:\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, \n                     color = bill_length_mm)) +\n    geom_point() +\n    scale_color_viridis_c()\n\n\n\n# note that the above example doesn't make efficient use of color because\n# the color scale simply shows the same info as the x axis, a better use\n# of color would be to show a third data dimension like this:\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, \n                     color = body_mass_g)) +\n    geom_point() +\n    scale_color_viridis_c()"
  },
  {
    "objectID": "lab11.html#questions",
    "href": "lab11.html#questions",
    "title": "10  R functions quick reference sheet",
    "section": "10.3 Questions",
    "text": "10.3 Questions\nNow it’s your turn! In the reprot.R script, create a reference guide for yourself following the same format, like this:\n\n## &lt;function name&gt;\n\n## **definition:** &lt;write your definition here&gt;\n\n## **examples:**\n&lt;write some R code here giving examples&gt;\n# &lt;use comments to help explain your examples&gt;\n\nThe report.R script will already be formatted for you\nHere’s the functions you’ll need to define and give examples for:\n\nc\nmean\nsd\nsum\nnrow\nView\n$\n&lt;=\n&lt;\n&gt;=\n&gt;\n!=\nsample\ngeom_bar\ngeom_boxplot\ngeom_histogram\nscale_color_viridis_d"
  },
  {
    "objectID": "lab15.html#goals",
    "href": "lab15.html#goals",
    "title": "11  Correlation",
    "section": "11.1 Goals",
    "text": "11.1 Goals\n\nCalculate a correlation coefficient and the coefficient of determination\nTest hypotheses about correlation\nUse the rank-based correlation when normality assumptions are not met"
  },
  {
    "objectID": "lab15.html#learning-the-tools",
    "href": "lab15.html#learning-the-tools",
    "title": "11  Correlation",
    "section": "11.2 Learning the Tools",
    "text": "11.2 Learning the Tools\nThis week and next week we will look at methods to understand the relationship between two numerical variables, using correlation and regression.\nTo demonstrate the new R commands this week, we will use the penguin data set from the palmerpenguins package in R. These data record the measurements of different body dimensions of three species of penguins. For simplicity, we will focus just on the chinstrap penguin. Let’s first load the package and subset the data to just chinstraps.\n\nlibrary(palmerpenguins)\nchinstrap &lt;- subset(penguins, species == \"Chinstrap\")\n\nLet’s look at the correlation between body mass and flipper length. We might expect the heavier the penguin, the bigger the flippers it needs. But before we go further, it is wise to plot a scatterplot to view the relationship of the two variables.\n\nggplot(chinstrap, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nThese data seem to have a moderately strong, positive relationship. Note: this is just an exploratory plot, we haven’t made it look high quality, we’re just getting a sense of what the data look like.\nCalculating a correlation coefficient in R is straightforward. The function cor() calculates the correlation between the two variables given as input:\n\ncor(chinstrap$body_mass_g, chinstrap$flipper_length_mm)\n\n[1] 0.6415594\n\n\nAs we predicted from the graph, the correlation coefficient of these data is positive and fairly strong.\nTo test a hypothesis about the correlation coefficient or to calculate its confidence interval, use cor.test(). It takes as input the names of vectors containing the variables of interest.\n\ncor.test(chinstrap$body_mass_g, chinstrap$flipper_length_mm)\n\n\n    Pearson's product-moment correlation\n\ndata:  chinstrap$body_mass_g and chinstrap$flipper_length_mm\nt = 6.7947, df = 66, p-value = 3.748e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4759352 0.7632368\nsample estimates:\n      cor \n0.6415594 \n\n\nThe output gives many bits of information we might want. After, re-stating the names of the variables being used, the output gives us the test statistic t, degrees of freedom, and P-value of a test of the null hypothesis that the population correlation coefficient is zero. In this case the P-value is quite small, \\(P = 3.748 \\times 10^{-9}\\). After that we have the 95% confidence interval for the correlation coefficient and finally the estimate of the correlation coefficient itself.\nThe name R gives this correlation test is unfortunate. Pearson of course was a terrible eugenicist, and not the only person in the world to ever come up with the idea of correlation. At the very least we know the Indian statistician Anil Kumar Gain developed the same idea. For these reasons, despite the naming convention in R, we will call this type of correlation Product-moment correlation. This is a more descriptive name because the mathematical form of this correlation coefficient comes from multiplying the moments of the data—in statistics, moments are any expression like this \\(\\sum (Y_i - \\bar{Y})^b\\) where \\(b\\) can be any integer. For correlation \\(b = 2\\).\n\n11.2.1 Rank-based correlation\nThe function cor.test() can also calculate a rank-based correlation, if we add the option method = \"spearman\" to the command. Rank-based correlation is helpful when the data are not normally distributed and/or when the relationship between variables is not linear. This is because it looks for a correlation between the ranks (first biggest, second biggest, etc.) of the data values rather than the values themselves.\n\ncor.test(chinstrap$body_mass_g, chinstrap$flipper_length_mm, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  chinstrap$body_mass_g and chinstrap$flipper_length_mm\nS = 17264, p-value = 3.983e-10\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6704871 \n\n\nThe output here is similar to what we described above with product-moment correlation. We call this kind of correlation rank-based because that is more descriptive, and again removes the name of a eugenicist (Spearman) who just happened to receive historical credit for this type of analysis."
  },
  {
    "objectID": "lab15.html#activities",
    "href": "lab15.html#activities",
    "title": "11  Correlation",
    "section": "11.3 Activities",
    "text": "11.3 Activities\n\n11.3.1 Developing an intuition for correlation coefficients.\nIn a web browser, open the app at http://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/\nThis app is simple, it will plot some data in a scatterplot, and you guess the correct correlation coefficient for those data. Select one of the three choices and click the little circle next to your choice. Most people find this pretty challenging at first, but that is the point—to let you develop a better intuition about what a given value of a correlation coefficient means for how strong a relationship is between two numerical variables.\nKeep trying new data sets (by clicking the “Simulate new data” button) until you feel like you can get it right most of the time."
  },
  {
    "objectID": "lab15.html#questions",
    "href": "lab15.html#questions",
    "title": "11  Correlation",
    "section": "11.4 Questions",
    "text": "11.4 Questions\n1. Telomeres length and aging\nThe ends of chromosomes are called telomeres. These telomeres are shortened a bit during each cell cycle as DNA is replicated. One of their purposes is to protect more valuable DNA in the chromosome from degradation during replication. As people get older and their cells have replicated more often, their telomeres shorten. There is evidence that these shortened telomeres may play a role in aging. Telomeres can be lengthened in germ cells and stem cells by an enzyme called telomerase, but this enzyme is not active in most healthy somatic cells. (Cancer cells, on the other hand, usually express telomerase.)\nGiven that the length of telomeres is biologically important, it becomes interesting to know whether telomere length varies between individuals and whether this variation is inherited. A set of data was collected by Nordfjäll et al. (2005) on the telomere length of fathers and their children; these data are in the file “telomere-inheritance.csv”.\n\nCreate a scatter plot showing the relationship between father and offspring telomere length.\nDo the data require any transformation to be bivariate normal before correlation?\nWhat’s the product-moment correlation between father and offspring telomere length? What’s the null hypothesis? Do you reject or fail to reject the null hypothesis?\n\n2. Brain-body mass allometry in mammals\nLarger animals tend to have larger brains. But is the increase in brain size proportional to the increase in body size? A set of data on body and brain size of 62 mammal species was collated by Allison and Cicchetti (1976), and these data are in the data set “mammals.csv”. The file contains columns giving the species name, the average body mass (in kg) and average brain size (in g) for each species.\n\nPlot brain size against body size. Is the relationship linear?\nFind a transformation (for either or both variables) that makes the relationship between these two variables linear.\nIs there statistical evidence that brain size is correlated with body size? Assume that the species data are independent."
  },
  {
    "objectID": "lab13.html#goals",
    "href": "lab13.html#goals",
    "title": "12  Visualizing data for hypothesis testing",
    "section": "12.1 Goals",
    "text": "12.1 Goals\n\nVisualize the data for your group project in a way that helps address your scientific hypotheses and predictions\nGo from scientific questions, predictions, and data visualizations to null hypothesis testing"
  },
  {
    "objectID": "lab13.html#learning-the-tools",
    "href": "lab13.html#learning-the-tools",
    "title": "12  Visualizing data for hypothesis testing",
    "section": "12.2 Learning the Tools",
    "text": "12.2 Learning the Tools\nHere are the steps for visualizing data with the goal of testing scientific hypotheses:\n\nState the predictions that derive from your hypotheses\nIdentify which variables in the data are implicated by those predictions\nPlan your plot:\n\nbased on the types of variables, what kinds of plot(s) will you need?\nfor example:\n\ncontinuous explanatory variable and continuous response variable: scatter plot\ndiscrete/categorical explanatory variable and continuous response variable: box plot\n\n\nManipulate your data to make it ready for plotting\nMake the plot!\n\nHand-in-hand with visualization we will over the steps of null hypothesis testing, they are:\n\nState null and alternative hypotheses\nCalculate test statistic\nIdentify null distribution\nCalculate \\(P\\)-value\nDecide:\n\nReject null\nFail to reject null\n\n\nTo walk through these steps we will use a hypothetical example about arthropod biomass in Hawaiʻi across different environments and over time.\nLet’s imagine we have data on arthropod biomass from two different collection methods: malaise traps and pitfalls (no need to worry about what those are, just know we will have two biomass columns in our data, one dedicated to each collection method). For simplicity we will imagine there are only 3 arthropod species, 10 field sites, and 1 environmental measurement (mean annual precipitation = MAP).\nWe will simulate this dataset. No need to worry about understanding this code, just copy-paste it and run it so you can follow along with the rest of this section:\n\n# copy, paste, and run all the below code:\n\nset.seed(123)\n\nmapdat &lt;- runif(10, 10, 100)\ny &lt;- mapdat + rnorm(length(mapdat), 0, 15)\n\nq &lt;- quantile(y, probs = seq(0, 1, length.out = 11))\nq[q == min(q)] &lt;- min(q) - 0.1\n\nsite &lt;- cut(y, q, labels = paste0(\"site_\", 1:10))\n\ndat &lt;- data.frame(site = site, MAP = mapdat)\ndat &lt;- dat[rep(1:nrow(dat), each = 3), ]\ndat$species &lt;- rep(paste0(\"species_\", 1:3), 10)\n\ndat$total_mass_malaise &lt;- 0.1 * dat$MAP + \n    as.numeric(as.factor(dat$species)) * 2 + \n    rnorm(nrow(dat), 0, 2)\n\ndat$total_mass_pitfall &lt;- 6 - 3 * as.numeric(as.factor(dat$species)) + \n    dat$total_mass_malaise + rnorm(nrow(dat), 0, 2)\n\nrownames(dat) &lt;- NULL\n\nNow we can have a look at the data:\n\nView(dat)\n\nNow let’s talk about some example predictions we could make:\n\n12.2.1 Prediction: total biomass increases with more rainfall\nWe’ve stated our prediction, now we need to identify the variables:\n\nThe explanatory variable is mean annual precipitation (MAP)\nThe response variable is total biomass\n\nNow we plan out plot:\n\nBoth explanatory and response variables are continuous so we will make a scatter plot\n\nNow we have to manipulate our data to get the variables we need. Depending on how the data are formatted, this could be no work, or it could require a fair bit of code. In our case, we need total biomass, which will require a few lines of code.\nLet’s start by adding the masses from the malaise and pitfall columns, and storing that new total total mass in a new column called total_mass:\n\ndat$total_mass &lt;- dat$total_mass_malaise + dat$total_mass_pitfall\n\nEasy enough. Now, we also need to sum the masses across all three species, because our prediction does not care about species, just total arthropod biomass. For this task we will use tools from the dplyr package:\n\nlibrary(dplyr)\n\n# we will group the data by site and ignore species\ndat_no_spp &lt;- group_by(dat, site)\n\n# now use summarize to get the mass across all species \ndat_no_spp &lt;- summarize(dat_no_spp, \n                        MAP = mean(MAP), \n                        total_mass = sum(total_mass))\n\n# have a look at what we made:\nhead(dat_no_spp)\n\n# A tibble: 6 × 3\n  site     MAP total_mass\n  &lt;fct&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 site_1  46.8       46.8\n2 site_2  14.1       35.7\n3 site_3  51.1       62.3\n4 site_4  59.6       53.1\n5 site_5  35.9       42.8\n6 site_6  57.5       64.5\n\n\nNotice a few things about the above code:\n\nWe made a new data.frame called dat_no_spp rather than writing-over the original data. It’s always a good idea to not write-over the original data, you might want to use the original for other caculations later on\nWe used group_by and summarize to calculate the variable we needed. Here we did write-over the name dat_no_spp because we used multiple lines of code to build up that data.frame\n\nNow we can plot our prediction!\n\nlibrary(ggplot2)\n\nggplot(dat_no_spp, aes(x = MAP, y = total_mass)) +\n    geom_point()\n\n\n\n\nThere you have it! You should take a little time to make the plot look nicer, but we have all the data visualized that we need.\nThis prediction seems well suited to a correlation test. The null hypothesis is that there is zero correlation between MAP and total mass (i.e. \\(H_0 \\text{: } \\rho = 0\\)). The alternative hypothesis is that MAP and total mass have a non-zero correlation (i.e. \\(H_A \\text{: } \\rho \\neq 0\\)).\nThe test statistic is the sample correlation coefficient \\(r\\). And the null distribution is the \\(t\\) distribution with \\(df = n - 2\\) degrees of freedom.\nWe can use the cor.test function to run this null hypothesis test\n\n# recall from the previous lab that we already made the dat_no_spp\n# data frame\n\ncor.test(dat_no_spp$MAP, dat_no_spp$total_mass)\n\n\n    Pearson's product-moment correlation\n\ndata:  dat_no_spp$MAP and dat_no_spp$total_mass\nt = 8.3598, df = 8, p-value = 3.177e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7870093 0.9877622\nsample estimates:\n      cor \n0.9472517 \n\n\nNow we decide: the \\(P\\)-value is much less than \\(\\alpha = 0.05\\) so we reject the null of no correlation.\nNow let’s have a look at the next prediction we had from the last lab:\n\n\n12.2.2 Another example prediction: Malaise traps will catch less biomass of species 1 compared to pitfall traps\nWe stated the prediction above, now we need to identify the variables:\n\nExplanatory: type of trap, malaise or pitfall (this is categorical)\nResponse: biomass of species 1 (this is continuous)\n\nGiven these variables, we now plan our plot: we’ll make a boxplot with type of trap on the x-axis and mass on the y-axis\nNow again comes the more labor-intensive part of manipulating the data. Our current data.frame has separate columns for each type of trap. To work with ggplot, we instead need one column that says the name of the trap (malaise or pitfall) and another column with the mass. We can achieve this through a few steps:\nFirst make two data.frames, one for malaise and one for pitfall:\n\nmalaise &lt;- dat[, c(\"site\", \"MAP\", \"species\", \"total_mass_malaise\")]\nmalaise$trap_type &lt;- \"malaise\"\n\n\npitfall &lt;- dat[, c(\"site\", \"MAP\", \"species\", \"total_mass_pitfall\")]\npitfall$trap_type &lt;- \"pitfall\"\n\nWhat we did above was ask for R to give us a data.frame with specific columns (the ones we named in quotes) and assign that data.frame to a new object name. Then we added a new column called trap_type and stored the appropriate name of the trap in that column.\nNow we need to make sure the column names are the same across the two data.frames\n\ncolnames(malaise) &lt;- c(\"site\", \"MAP\", \"species\", \"total_mass\", \"trap_type\")\ncolnames(pitfall) &lt;- c(\"site\", \"MAP\", \"species\", \"total_mass\", \"trap_type\")\n\nNow we can combine those two data.frames and we’ll have what we need:\n\ndat_by_trap &lt;- rbind(malaise, pitfall)\n\nNow we can make the plot!\n\n# notice that we're subsetting the data to just look at species 1\nggplot(subset(dat_by_trap, dat_by_trap$species == \"species_1\"), \n       aes(x = trap_type, y = total_mass)) +\n    geom_boxplot()\n\n\n\n\nAgain, you should take some time to beautify your plot, but all the info from the data are now visualized!\nHere we have a categorical explanatory variable and a numerical response variable. This is well-suited for a \\(t\\)-test. Because we have two categories, we might be tempted to do a two-sample \\(t\\)-test.\nOur hypotheses are:\n\n\\(H_0\\): \\(\\mu_\\text{malaise} = \\mu_\\text{pitfall}\\)\n\\(H_A\\): \\(\\mu_\\text{malaise} \\neq \\mu_\\text{pitfall}\\)\n\nThe test-statistic is the difference in the two sample means divided by the pooled standard error. But wait! Perhaps we should iterate on this a little. Each replicate of the pitfall traps and malaise traps are located at the same site, so really we have a paired \\(t\\)-test.\nThe null and alternative hypotheses stay the same, but the test statistic will change because the standard error calculation is different between two-sample and paired \\(t\\)-tests. Our degrees of freedom, and thus are null distribution, will also be very different between the two-sample \\(t\\)-test and the paired \\(t\\)-test. For the paired \\(t\\)-test the degrees of freedom are \\(df = \\text{(number of pairs)} - 1\\).\nThe t.test function will handle these calculations for us\n\n# recall from the last lab we already made the dat_by_trap data frame\njust_sp1 &lt;- subset(dat_by_trap, dat_by_trap$species == \"species_1\")\n\n# MAKE SURE TO SET paired = TRUE!!!\nt.test(just_sp1$total_mass ~ just_sp1$trap_type, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  just_sp1$total_mass by just_sp1$trap_type\nt = -5.9705, df = 9, p-value = 0.00021\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.923264 -2.217636\nsample estimates:\nmean difference \n       -3.57045 \n\n\nNow we decide: the \\(P\\)-value is less than \\(\\alpha = 0.05\\) so we reject the null of no difference between the means."
  },
  {
    "objectID": "lab13.html#questions",
    "href": "lab13.html#questions",
    "title": "12  Visualizing data for hypothesis testing",
    "section": "12.3 Questions",
    "text": "12.3 Questions\nNow it’s your turn to to walk through the steps of visualizing your data for testing your hypotheses. Complete these answers in the google doc attached to the assignment in google classroom. For each of your two scientific hypotheses:\n\nState the prediction that derive from your hypothesis\nIdentify which variables in the data are implicated by those predictions\nPlan your plot: which type(s) of plot(s) will you need to make, and which data will go on which axis/axes?\nManipulate your data to make it ready for plotting\n\nthis you do not need to copy into the google doc, but make sure you save your script in posit cloud\n\nMake the plot!\n\ntake a screenshot of your plot(s) and paste the screenshot(s) into the google doc\n\nState null and alternative hypotheses\nCalculate test statistic (and include a written justification for why this test statistic is appropriate)\nIdentify null distribution (and include a written justification for why this null distribution is appropriate)\nCalculate \\(P\\)-value\nDecide:\n\nReject null\nFail to reject null"
  },
  {
    "objectID": "lab16.html#stub",
    "href": "lab16.html#stub",
    "title": "13  Regerssion",
    "section": "13.1 Stub",
    "text": "13.1 Stub\nMore soon"
  },
  {
    "objectID": "lab17.html#stub",
    "href": "lab17.html#stub",
    "title": "14  Analysis of Variance",
    "section": "14.1 Stub",
    "text": "14.1 Stub\nMore soon"
  }
]