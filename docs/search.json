[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "process-models-workshop",
    "section": "",
    "text": "These are the materials for the second 2 days of the “Multidimensional Biodiversity Data” workshop first offered in Albequerque, NM in June 2023 by the RoLE Model team. For the first two days’ materials, see the course website here.\nTo work through these materials asynchronously, you will need some software packages installed on your computer. We have provided a Docker image with the necessary packages already set up. To get going with this, see the setup instructions.\nThe first days’ materials (from “Theory/conceptual motivation for process models” through to “Intro to machine learning inference with neutral simulations”) are also available as video lectures on YouTube:\n\nTheory and motivation of process modeling\nThe Unified Neutral Theory of Biodiversity as a process model\nInferring the parameters of neutral theory from model outcomes\n\n\n\n\n\n\n\nTime\nDay 1\nDay 2\n\n\n\n\n9-9:30\nReview of part I and follow-up questions\nReview of previous days topics (Discuss dreamed of ideas)\n\n\n9:30-10:30\nFinding multidimensional biodiversity data and and Theory/conceptual motivation for process models\nIntro to the MESS model & Getting started with MESS simulations\n\n\n10:30-10:45\nCoffee Break\nCoffee Break (Group photo)\n\n\n10:45-12\nCoding exercise: A simple neutral model of biodiversity\nSimulation exercise: Using MESS API mode in RStudio to develop and test theoretical intuition\n\n\n12-1\nLunch Break\nLunch Break\n\n\n1-2:30\nSimulation Exercise: Exploring neutral model behavior\nSimulation based machine learning inference (Classification)\n\n\n2:30-2:45\nCoffee Break\nCoffee Break\n\n\n2:45-4:00\nIntro to machine learning inference with neutral simulations\nSimulation based machine learning inference (Regression), small group discussions about empirical applications & Wrap-up\n\n\n\n\n\n\n\n\n\nRoLE Workshop Group Photo\n\n\n\n\n\nWe gratefully acknowledge funding support from NSF awards DBI-2208901 to Renata Diaz and DBI-2104147 to the RoLE model team."
  },
  {
    "objectID": "software_setup.html",
    "href": "software_setup.html",
    "title": "2  Software setup",
    "section": "",
    "text": "For this workshop, we prepared a virtual image containing all the necessary software, packages and settings we will need. To clone this image into your machine, you first need to download and install Docker Desktop from this link. Make sure to choose the right version for your computer (Windows, Linux, MacOS Intel Chip or MacOS Apple Chip).\nOnce installed, open Docker and use the search bar on the top to look for iovercast/mess. This is where the image is stored. \nOnce the image is found (the name iovercast/mess will show up in the drop-down list), you need to choose the processor type for your computer. If you are using a M1/M2 MacBook, in the tag drop-down menu, choose mac-m1. For any other architecture (Windows, Linux and non-M1/M2 MacBooks), choose intel. Then click on Pull. \nOnce the pull is done, click on the Images option in the menu to the left. You should be able to see the new iovercast/mess image listed there. To get it running, click on the Play icon under Actions. \nMake sure to add a port for the image. When prompted for Optional settings, click on the arrow to open the drop-down options and type 8787 in the Host port option. Then click on Run. \nIn the new screen, click on the 8787:8787 link below the image name. \nA page for rstudio-server will open on your internet browser, prompting you for the username and password. Type rstudio for both username and password. \nIf an RStudio screen opens on your brower, you are ready to start working."
  },
  {
    "objectID": "intro_process_models.html",
    "href": "intro_process_models.html",
    "title": "3  Theory and motivation of process modeling",
    "section": "",
    "text": "What is a process modeling approach?\nWhat kinds of questions can process models explore (that other methods cannot)?\nWhat are the limitations or constraints of process modeling?"
  },
  {
    "objectID": "intro_process_models.html#lesson-objectives",
    "href": "intro_process_models.html#lesson-objectives",
    "title": "3  Theory and motivation of process modeling",
    "section": "3.2 Lesson objectives",
    "text": "3.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nDescribe what defines a process model.\nEvaluate the advantages and limitations of a process modeling approach.\nGenerate questions in ecology and evolution that could be addressed using process modeling.\nUnderstand the unified neutral theory of biodiversity as an example of a well-established process model in ecology."
  },
  {
    "objectID": "intro_process_models.html#lesson-outline",
    "href": "intro_process_models.html#lesson-outline",
    "title": "3  Theory and motivation of process modeling",
    "section": "3.3 Lesson outline",
    "text": "3.3 Lesson outline\nSlides\nVideo lectures:\n\nTheory and motivation of process modeling\nThe Unified Neutral Theory of Biodiversity as a process model\nVideo on playing neutral theory as a game, and in R\n\n\n3.3.1 (Lecture + discussion) What do we mean by process modeling, anyway?\nKey points:\n\nIn a process model, we play a “game”, with a set of entities following a system of defined rules, and see how the outcomes of the game vary as we change the rules.\nThese models are often nondeterministic and often do not have closed-form, analytical solutions.\n\n\n\n3.3.2 (Lecture + discussion) What are the applications of process modeling for eco-evo?\nKey points:\n\nA process model offers almost unlimited flexibility for modeling complex, interacting processes.\nEverything depends on the rules we define! So it is very important to understand the rules of the game we are playing.\n\nDiscussion/brainstorm:\n\nWhat are some possible applications of process modeling in your area of interest?\n\n\n\n3.3.3 (Lecture + discussion) What are the constraints we encounter in a process modeling approach?\nKey points:\n\nProcess models are computationally expensive! Modern computing strategies help, but high dimensionality is a serious constraint.\nIssues like model identifiability and correlation != causation. Just because our mechanism produces results that match some data, doesn’t necessarily mean our mechanism is the same mechanism that produced that data.\nProcess models are themselves nondeterministic, so must be run many times.\n\n\n\n3.3.4 (Example) The Unified Neutral Theory of Biodiversity\nKey points:\n\nHubbell’s UNTB is a process model. Simply put, it is a game played with a set of simple rules, with emergent properties that we find interesting.\nFor the rest of today, we’ll be using UNTB as an example process model.\n\n\n\n3.3.5 (Lecture + game of chance) Playing neutral theory\nKey points:\n\nThe rules of neutral theory\nWe can play neutral theory manually. However, computers would make it considerably faster!"
  },
  {
    "objectID": "coding_neutral_theory.html",
    "href": "coding_neutral_theory.html",
    "title": "4  Coding exercise: A simple neutral model of biodiversity",
    "section": "",
    "text": "How do we implement a process model in R?\nHow do we use this model to explore how parameter settings affect outcomes?"
  },
  {
    "objectID": "coding_neutral_theory.html#lesson-objectives",
    "href": "coding_neutral_theory.html#lesson-objectives",
    "title": "4  Coding exercise: A simple neutral model of biodiversity",
    "section": "4.2 Lesson objectives",
    "text": "4.2 Lesson objectives\n\nCode up UNTB in R.\nChange parameter settings and inspect the outcomes."
  },
  {
    "objectID": "coding_neutral_theory.html#lesson-outline",
    "href": "coding_neutral_theory.html#lesson-outline",
    "title": "4  Coding exercise: A simple neutral model of biodiversity",
    "section": "4.3 Lesson outline",
    "text": "4.3 Lesson outline\n\n4.3.1 (Group project) Code up UNTB in R!\n\n\n4.3.2 (Backup)\nSource the pre-coded UNTB located here\n\n\n4.3.3 (Breakout groups or individual) Change parameter settings manually and visualize outcomes."
  },
  {
    "objectID": "neutral_theory_sims.html",
    "href": "neutral_theory_sims.html",
    "title": "5  Simulation Exercise: Exploring neutral model behavior",
    "section": "",
    "text": "How do we systematically explore parameter space for a process model?\nHow do the outcomes of UNTB simulations change over wide ranges of parameter space?"
  },
  {
    "objectID": "neutral_theory_sims.html#lesson-objectives",
    "href": "neutral_theory_sims.html#lesson-objectives",
    "title": "5  Simulation Exercise: Exploring neutral model behavior",
    "section": "5.2 Lesson objectives",
    "text": "5.2 Lesson objectives\n\nWrite code to explore parameter space for UNTB.\nVisualize and interpret plots of outcomes (Hill numbers) for exploratory neutral simulations."
  },
  {
    "objectID": "neutral_theory_sims.html#lesson-outline",
    "href": "neutral_theory_sims.html#lesson-outline",
    "title": "5  Simulation Exercise: Exploring neutral model behavior",
    "section": "5.3 Lesson outline",
    "text": "5.3 Lesson outline\nSlides\n\n5.3.1 (Coding exercise) Set up the infrastructure to explore ranges of parameters\nOr (backup) source the script here\nAs a group, write a wrapper to run our untb code over ranges of values for M and nu.\nOr (backup) source the script here\n\n\n5.3.2 (Breakout groups) Explore ranges of parameters\n\n\n5.3.3 (Dicussion) Interpret outcomes\nDiscussion questions:\n\nEach group describe their outcomes\nDid each simulation come out the same?\nDo we see unique mapping of M, Nu to Hill numbers? What relationships do we see?\nHow long did it take to run?\n\nKey points:\n\nIndividual simulations come out differently, but the overall behaviors are consistent.\nA single outcome variable (e.g. hill0) can come about from multiple combinations of parameters.\nRunning lots of models as a for loop is slow!"
  },
  {
    "objectID": "ml_inference_neutral.html",
    "href": "ml_inference_neutral.html",
    "title": "6  Inferring the parameters of UNTB from outcomes",
    "section": "",
    "text": "How do we use the outcomes of a process model to guess at underlying processes (parameter values)?\nHow can we leverage machine learning (random forest) to infer parameter values from the outcomes of a neutral model?\nWhat are the limiting factors in inferring process from outcome in a process model?"
  },
  {
    "objectID": "ml_inference_neutral.html#learning-objectives",
    "href": "ml_inference_neutral.html#learning-objectives",
    "title": "6  Inferring the parameters of UNTB from outcomes",
    "section": "6.2 Learning objectives",
    "text": "6.2 Learning objectives\n\nUnderstand the outcome-to-parameter inference model.\nFit and evaluate a random forest model to our UNTB data.\nIdentify the limiting features of this approach (particularly model identifiability) and brainstorm possible solutions."
  },
  {
    "objectID": "ml_inference_neutral.html#lesson-outline",
    "href": "ml_inference_neutral.html#lesson-outline",
    "title": "6  Inferring the parameters of UNTB from outcomes",
    "section": "6.3 Lesson outline",
    "text": "6.3 Lesson outline\nSlides\nVideo lecture: Inferring the parameters of neutral theory from model outcomes\n\n6.3.1 (Lecture/discussion) Going from outcome to parameters\nKey question:\n\nHow could we use a process model to better understand actual data?\n\nKey points:\n\nAs a starting point, we can see if we can recover generative parameters when we know how they were produced.\n\n\n\n6.3.2 (Lecture/discussion) Random forest regression\nKey points:\n\nTo guess at the parameters that generated some outcome data, we’re writing a model of the general form parameter ~ results. This isn’t necessarily a linear or otherwise tidy relationship, so we’ll use machine learning.\nRandom forest can do regression with many parameters predicting nonlinear relationships.\nWe will begin by using a random forest to try to recover parameter values for known sims.\n\n\n\n6.3.3 (Lecture) Fitting a random forest to our neutral data\n\n\n6.3.4 (Discussion) What problems arise in the random forest?\nKey points:\n\nMultiple sets of parameters can lead to the same outcomes, making it difficult to infer backwards.\n\n\n\n6.3.5 (Discussion) How might we address these challenges?\nKey points:\n\nMore data dimensions can break a many-to-one mapping.\nIt remains critical that the underlying process be appropriate."
  },
  {
    "objectID": "day_1_wrapup.html",
    "href": "day_1_wrapup.html",
    "title": "7  Day 1 wrap-up",
    "section": "",
    "text": "One up/one down.\nOr everyone share 1 discovery or question from the day."
  },
  {
    "objectID": "day_1_wrapup.html#key-points-from-today",
    "href": "day_1_wrapup.html#key-points-from-today",
    "title": "7  Day 1 wrap-up",
    "section": "7.2 Key points from today",
    "text": "7.2 Key points from today\n\nProcess models are games, with rules and nondeterministic (but consistent) outcomes.\nThe UNTB is a foundational process model for ecology."
  },
  {
    "objectID": "day_1_wrapup.html#looking-toward-tomorrow",
    "href": "day_1_wrapup.html#looking-toward-tomorrow",
    "title": "7  Day 1 wrap-up",
    "section": "7.3 Looking toward tomorrow",
    "text": "7.3 Looking toward tomorrow\n\nTomorrow, we’ll generalize from UNTB to do more involved, multidimensional process modeling.\nGet some rest!"
  },
  {
    "objectID": "intro_mess.html",
    "href": "intro_mess.html",
    "title": "8  Introduction to the MESS Model",
    "section": "",
    "text": "What is the MESS Model?\nHow does MESS differ from other biodiversity models?\nWhat are some example applications of MESS?\nWhere can I get a cool MESS logo sticker?"
  },
  {
    "objectID": "intro_mess.html#lesson-objectives",
    "href": "intro_mess.html#lesson-objectives",
    "title": "8  Introduction to the MESS Model",
    "section": "8.2 Lesson objectives",
    "text": "8.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nDescribe the (high-level) concept for MESS.\nSituate MESS in the wider process modeling state space.\nFormulate scientific questions and decide if/how MESS can be used to explore them."
  },
  {
    "objectID": "intro_mess.html#planned-exercises",
    "href": "intro_mess.html#planned-exercises",
    "title": "8  Introduction to the MESS Model",
    "section": "8.3 Planned exercises",
    "text": "8.3 Planned exercises\n\n8.3.1 Process-based modeling with the Massive Eco-evolutionary Synthesis Simulations (MESS) Model\n\n\n\nNeutral Assembly Process\n\n\n\n\n8.3.2 Overview of MESS simulation and analysis workflow\nThe basic steps of MESS model simulation-based machine learning inference are as follows:\n\nStep 1 - Set model parameters based on prior knowledge of empirical system\nStep 2 - Run many, many simulations\nStep 3 - Use ML to infer community assembly process (neutral/competition/filtering)\nSetp 4 - Use ML to estimate key community assembly parameters\nStep 5 - ???\nStep 6 - Profit!!"
  },
  {
    "objectID": "intro_mess.html#key-points",
    "href": "intro_mess.html#key-points",
    "title": "8  Introduction to the MESS Model",
    "section": "8.4 Key points",
    "text": "8.4 Key points\n\nMESS is a process-based model in the direct lineage of Island Biogeograpy Theory and Neutral Biodiversity Theory.\nMESS models the 4 fundamental biodiversity processes: dispersal, speciation, selection, and drift.\nMESS generates joint predictions of multiple biodiversity patterns: abundances, trait values, genetic diversities, and phylogenies."
  },
  {
    "objectID": "mess_getting_started.html",
    "href": "mess_getting_started.html",
    "title": "9  Getting started with the MESS model",
    "section": "",
    "text": "How do I set up and run a MESS simulation?\nWhat are the inputs and outputs of MESS?\nWhat happens when a MESS model runs?\nHow do I view and interpret the results of a MESS simulation?"
  },
  {
    "objectID": "mess_getting_started.html#lesson-objectives",
    "href": "mess_getting_started.html#lesson-objectives",
    "title": "9  Getting started with the MESS model",
    "section": "9.2 Lesson objectives",
    "text": "9.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nDescribe the basic workflow of running a MESS simulation.\nCreate and edit a MESS params file, including setting ranges on parameters.\nUnderstand the meaning of two key MESS model parameters: J and colrate.\nUse the command line to run a basic MESS model.\nDescribe and interpret the basic outputs of a MESS model."
  },
  {
    "objectID": "mess_getting_started.html#planned-exercises",
    "href": "mess_getting_started.html#planned-exercises",
    "title": "9  Getting started with the MESS model",
    "section": "9.3 Planned exercises",
    "text": "9.3 Planned exercises\nThis is the first part of the full tutorial for the MESS model. In this tutorial we’ll focus on the command line interface (CLI), because this is the simplest way to get started, and it will also more likely be how one would run MESS on an HPC. This is meant as a broad introduction to familiarize users with the general workflow, and some of the parameters and terminology.\n\nInstallation\nGetting started with the MESS CLI\nCreating and editing a new params file\nRun simulations using the edited params file\nInspect the output of the simulation runs\nSetting prior ranges on parameters\nHands-on experimentation time\n\n\n9.3.1 Installing MESS\nBecause we are using the self-contained and pre-instaled docker image, you do not need to install anything for this workshop, rather installation instructions are included for completeness and future information should you wish to install MESS on your home cluster.\n\n\n\n\n\n\nMESS installation instructions\n\n\n\n\n\nThe most current implementation of the MESS model is included in the iBioGen package, which is hosted on the iBioGen github and installable either using conda or the remotes R package. Conda is the preferred method, but really either should work.\nFor the command line install, first install conda and then:\nconda create -n iBioGen python=3.10\nconda activate iBioGen\nconda install -c conda-forge -c ibiogen ibiogen\nFor the R install:\n> install.packages(\"remotes\")\n> library(remotes)\n> remotes::install_github('iBioGen/iBioGen')\n\n\n\n\n\n\n\n\n\nWhy do you keep calling it MESS when we’re installing iBioGen?\n\n\n\n\n\nGood question! Think of MESS as the idea or the concept and iBioGen as the implentation. It’s the same relationship between the concept/idea of a car and an actual Honda or Ford, they’re both complicated machines made of tons of small parts that do the same job in slighly different ways.\nThe underlying implementation of iBioGen is python, but we are also working on an implementation of MESS in R which is called roleR. roleR and iBioGen are both complicated software programs made of tons of small parts that implement the idea of the MESS model in slightly different ways. For the purpose of this workshop, because we are interested in the model and not the implementation, we will prefer to refer to it as MESS, in most cases.\n\n\n\n\n\n9.3.2 Getting started with the MESS CLI\nFor this exercise we’ll be using the MESS command line interface (CLI), which just means we’ll run it like a program in a terminal. Lets start by getting our rstudio interface changed a bit to make this part easier. Start by collapsing the ‘source’ pane, and expanding the ‘console’ pane on the left, then choose the ‘terminal’ tab in the console pane.\n\n\n\nGetting started with MESS in the terminal\n\n\nEach grey cell in this tutorial indicates a command line interaction. Lines starting with $ indicate a command that should be executed in a terminal. All lines in code cells beginning with ## are comments and should not be copied and executed. All other lines should be interpreted as output from the issued commands.\n## Example Code Cell.\n\n## e.g. Create an empty file in my home directory called `watdo.txt`\n$ touch ~/watdo.txt\n\n## Elements in code cells surrounded by angle brackets (e.g. < > ) are\n## variables that need to be replaced by the user\n##\n## Make a directory for your project. When YOU run this replace everything\n## (angle brackets and all) with the information that is applicable to you.\n$ mkdir <my_project>\n\n## Print \"wat\" to the screen\n$ echo \"wat\"\nwat\nOk, so now that you’re sitting at a command line, we’re ready to begin. To better understand how to use iBioGen to run a MESS model, let’s take a look at the help argument. We will use some of the iBioGen command line arguments in this tutorial (for example: -n, -p, -s, -c). The complete list of optional arguments and their explanation can be accessed with the -h flag (or --help:\n$ iBioGen -h\nusage: iBioGen [-h] [-n new] [-p params] [-s sims] [-c cores] [-f] [-v] [-q] [-d]\n               [--ipcluster [ipcluster]]\n\noptions:\n  -h, --help            show this help message and exit\n  -n new                create new file 'params-{new}.txt' in current directory\n  -p params             path to params file simulations: params-{name}.txt\n  -s sims               Generate specified number of simulations\n  -c cores              number of CPU cores to use (Default=0=All)\n  -f                    force overwrite of existing data\n  -v                    do not print to stderror or stdout.\n  -q                    do not print anything ever.\n  -d                    print lots more info to iBioGen_log.txt.\n  --ipcluster [ipcluster]\n                        connect to ipcluster profile\n\n  * Example command-line usage:\n    iBioGen -n data                       ## create new file called params-data.txt\n    iBioGen -p params-data.txt            ## run iBioGen with settings in params file\n\n\n9.3.3 Creating and editing a new params file\niBioGen uses a text file to hold all the parameters for a given community assembly scenario. Start by creating a new parameters file with the -n flag. This flag requires you to pass in a name for your simulations. In the example we use simdata but the name can be anything at all. Once you start analysing your own data you might call your parameters file something more informative, like the name of your target community and some details on the settings.\n## Make a new directory, change directory into it, and then print the working\n## directory to make sure you are where you think you are.\n$ cd ~\n$ mkdir MESS\n$ cd MESS\n$ pwd\n\n\n# Create a new params file named 'simdata'\n$ iBioGen -n simdata\n-------------------------------------------------------------\n  iBioGen [v.0.0.9]\n  Island Biodiversity Genomics Analysis Toolkit\n -------------------------------------------------------------\n\n  New file 'params-simdata.txt' created in /home/rstudio/MESS\nThis will create a file in the current directory called params-simdata.txt. The params file lists on each line one parameter followed by a ## mark, then the name of the parameter, and then a short description of its purpose. Lets take a look at it.\n$ cat params-simdata.txt\n------- iBioGen params file (v.0.0.9)-------------------------------------------\nsimdata              ## [0] [simulation_name]: The name of this simulation scenario\n./default_iBioGen    ## [1] [project_dir]: Where to save files\n1                    ## [2] [birth_rate]: Speciation rate\ntaxa                 ## [3] [meta_stop_criterion]: Whether to stop metacommunity on ntaxa or time\n20                   ## [4] [ntaxa]: Number of taxa to simulate if stop is `ntaxa`\n4                    ## [5] [time]: Amount of time to simulate if stop is `time`\nabundance            ## [6] [process]: Whether to evolve `abundance` or growth `rate` via BM\nTrue                 ## [7] [ClaDS]: Whether to allow speciation rate change a la ClaDS\n50000                ## [8] [abundance_mean]: Ancestral abundance at time 0\n0.1                  ## [9] [abundance_sigma]: Rate at which abundance changes if process is `abundance`\n0                    ## [10] [growth_rate_mean]: Ancestral population growth rate at time 0.\n0.01                 ## [11] [growth_rate_sigma]: Rate at which growth rate changes if process is `rate`\n0.1                  ## [12] [ClaDS_sigma]: Rate at which speciation rate changes if ClaDS is True\n0.9                  ## [13] [ClaDS_alpha]: Rate shift if ClaDS is True\n500                  ## [14] [sequence_length]: Length of the genomic region simulated, in base pairs\n1e-08                ## [15] [mutation_rate]: Mutation rate per base per generation\n10                   ## [16] [sample_size]: Number of samples to draw for calculating genetic diversity\nNone                 ## [17] [abundance_scaling]: Scaling abundance to Ne (None, log, ln or a ratio)\n1000                 ## [18] [J]: Number of individuals in the local community\n0.005                ## [19] [colrate]: Rate of colonization into the local community (% / birth event)\nneutral              ## [20] [assembly_model]: Selecting neutral or non-neutral assembly processes\n2                    ## [21] [ecological_strength]: Impact of competition or filtering on fitness\n1                    ## [22] [generation_time]: Generation time of local community taxa (in years)\ntime                 ## [23] [local_stop_criterion]: Stop local community on time or equilibrium\n100                  ## [24] [local_stop_time]: Local community simulation duration (in generations)\n100                  ## [25] [tau_max]: Duration of anagenetic speciation (in generations)\n10                   ## [26] [gene_flow_effect]: Damping effect of gene flow (in generations) on tau_max\n1000                 ## [27] [Ne_scaling]: Scaling individuals to demes in local community\nFalse                ## [28] [rm_duplicates]: Deduplicate seqs before calculating local community pi\n\n\n\n\n\n\nWhy are some options set on the CLI and some things in the params file?\n\n\n\n\n\nGreat question! What’s the difference between a CLI argument and a MESS params file parameter, you may be asking yourself? Well, MESS CLI arguments specify how the simulations are performed (e.g. how many to run, how many cores to use, whether to print debugging information, etc), whereas MESS params file parameters dictate the structure of the simulations to run (e.g. sizes of communities, migration rates, specation rates, etc).\n\n\n\nThe defaults in the params file are all values of moderate size that will generate ‘normal’ looking simulations, and we won’t mess with them for now, but lets just change a couple parameters to get the hang of it. Let’s set J (the # of individuals in the local community)) equal to 500 as this will speed up the simulations (smaller local communities reach equilibrium faster).\nWe will use the text editor built into Rstudio to modify params-simdata.txt and change this parameter:\n\n\n\nOpen the new params-simdata.txt file\n\n\n\n\n\nEdit params-simdata.txt in the source pane\n\n\nDon’t forget to File->Save after editing your params file! Now you’re ready to run some MESS simulations!\n\n\n9.3.4 Run simulations using the edited params file\nThe two most important arguments for the iBioGen command when running simulations are -p which specifies the params file to use, and -s which determines the number of simulations to run.\n## Run 10 simulations using our new params file\n$ iBioGen -p params-simdata.txt -s 10\n\n -------------------------------------------------------------\n  iBioGen [v.0.0.9]\n  Island Biodiversity Genomics Analysis Toolkit\n -------------------------------------------------------------\n    Parallelization disabled.\n    Generating 10 simulation(s).\n  [####################] 100%  Finished 10 simulations in   0:00:01|\nThis is the simplest possible iBioGen command that will run simulations. It works great for 10 simulations, but typically you’ll need to perform 10 thousand or 10 million simulations, so this approach of generating them one at a time (in serial) is woafully time-consuming. 10x more simulations takes on the order of 10x more wall time:\n## Run 100 simulations with our new params file\n$ iBioGen -p params-simdata.txt -s 100\n\n -------------------------------------------------------------\n  iBioGen [v.0.0.9]\n  Island Biodiversity Genomics Analysis Toolkit\n -------------------------------------------------------------\n    Parallelization disabled.\n    Generating 100 simulation(s).\n  [####################] 100%  Finished 100 simulations in   0:00:17\nTo facilitate massive parallelization, the iBioGen CLI provides a parallel computing option which will intelligently handle all the parallelization work for you. When invoked with the -c flag, iBioGen portions out simulations among all the cores as they become available.\n## Run 100 simulations split across 5 cores\n$ iBioGen -p params-simdata.txt -s 100 -c 5\n\n -------------------------------------------------------------\n  iBioGen [v.0.0.9]\n  Island Biodiversity Genomics Analysis Toolkit\n -------------------------------------------------------------\n  establishing parallel connection:\n  host compute node: [5 cores] on d98e2e53d6af\n    Generating 100 simulation(s).\n  [####################] 100%  Finished 100 simulations in   0:00:05 \n\n\n\n\n\n\nThe -c flag is great! Why don’t I put -c 100, it’s going to go so fast!\n\n\n\n\n\nUmmmm, that’s not really how it works, but I appreciate your enthusiasm. While you very well can use -c 100 and it will definitely do what you say and spin up 100 parallel workers, it will not increase the number of actual cpu cores on your computer. In CLI mode the number of -c cores should never exceed the number of cpu cores on your computer, or else you’ll actually reduce performance, because the parallel workers will all be competing for the (limited) cpu resources.\n\n\n\n\n\n9.3.5 Inspect the output of the simulation runs\nThe results from every simulation that you run are written to a file in our project_dir (which defaults to ./default_iBioGen). Lets look in this directory to see what is there:\n$ ls ./default_iBioGen\nsimdata-SIMOUT.csv\n\n\n\n\n\n\nNote\n\n\n\nFirst thing to notice is the strong correlation between the name of the file (simdata-SIMOUT.csv) and the name that we gave when we created the new params file earlier (iBioGen -n simdata). This is not an accident. Output files retain the name of the simulation, and simulation names are enforced to be unique. In this way you can create many new simulations with different parameter values and the output files will not interfere with one another.\n\n\nSimulation parameters and data are written to this file, one simulation per line. You can calculate the number of lines in this file like this:\n$ wc -l ./default_iBioGen/simdata-SIMOUT.csv\n210 default_iBioGen/simdata-SIMOUT.csv\n\n\n\n\n\n\nNote\n\n\n\nIt is worth mentioning that running new simulations using the same params file will simply append to this SIMOUT file, it will never destructively remove previously run simulations.\n\n\nThe simulations results are not exactly human readable (they’re not supposed to be), but we can still take a look at them with the linux command head, which is very similar to the same command in R, printing the first few lines of a file to the screen:\n## peek at the simulations with head\n## -n 2 prints the first 2 lines, the header line and the first simulation\n$ head -n 2 default_iBioGen/simdata-SIMOUT.csv \nbirth_rate meta_stop_criterion ntaxa time process ClaDS abundance_mean abundance_sigma growth_rate_mean growth_rate_sigma ClaDS_sigma ClaDS_alpha sequence_length mutation_rate sample_size abundance_scaling J colrate assembly_model ecological_strength generation_time local_stop_criterion local_stop_time tau_max gene_flow_effect Ne_scaling rm_duplicates meta_obs_ntaxa meta_obs_time local_obs_time(gen) local_obs_time(eq) meta_turnover_rate metadata metatree localdata\n1.0 taxa 20 4.0 abundance True 50000 0.1 0.0 0.01 0.1 0.9 500 1e-08 10 None 1000 0.005 neutral 2.0 1.0 time 100.0 100 10 1000 False 20 5.910768727555379 100 0.46399999999999997 0.0 m2-1:22105:0.0019111111111111115:-0.0006829200581811718:1.0702217505635192,m10-1:3476:0.0:0.029954759281765286:0.6636343206049413,m15-1:223:0.0:0.029283014537300305:0.5055426276547345,m15-2:670:0.0:0.03003612090749081:0.5847635679494314,m19-1:3427:0.0011111111111111111:0.03848397068084088:0.7649142929809856,m19-2:1608:0.0:0.038207089121944114:0.7314808093884317,m14-1:2004:0.0:0.04089408370966607:0.5585651686757594,m14-2:1293:0.0:0.040816815424858335:0.6996249403618324,m18-1:330:0.0:0.03415345354321346:0.7065992405115762,m18-2:970:0.0:0.038339667987257256:0.6235489663117895,m17-2:3131:0.0:0.03791778981710612:0.6703418713827751,m12-1:210:0.0:-0.053137235419017756:0.7994138255784535,m12-2:833:0.0:-0.05309037498020786:0.5571929402898937,m8-1:19:0.0:-0.052762281585036805:0.6374722310714049,m8-2:20:0.0:-0.060391958308555684:0.5714480499584342,m6-2:5957:0.0011111111111111111:-0.05508981820070741:0.8656464071932146,m16-1:102:0.0:-0.05673229279720891:0.7255149234973584,m16-2:144:0.0:-0.05360134116045264:0.6314217505682252,m13-2:219:0.0:-0.05079188594847156:0.679215673797093,m9-2:328:0.0:-0.050723924913676346:0.8206959794604726 ((m2-1:3.61047,(((m10-1:0.704907,(m15-1:0.496668,m15-2:0.496668)0:0.208239)0:0.468363,(m19-1:0.115949,m19-2:0.115949)0:1.05732)0:0.220812,((m14-1:0.515387,m14-2:0.515387)0:0.188654,((m18-1:0.324183,m18-2:0.324183)0:0.0355387,m17-2:0.359722)0:0.34432)0:0.69004)0:2.21639)0:2.3002,((((m12-1:0.651564,m12-2:0.651564)0:0.297055,(m8-1:0.937516,m8-2:0.937516)0:0.0111021)0:0.035412,m6-2:0.98403)0:0.930347,(((m16-1:0.382833,m16-2:0.382833)0:0.170452,m13-2:0.553285)0:0.370263,m9-2:0.923547)0:0.990831)0:3.99629); m2-1:0.0:536.0:246.0:-4.403525126405577:99.0:0.010799999999999999,m10-1:12.0:47.0:33.0:2.484183313499446:95.0:0.002,m14-1:15.0:90.0:21.0:3.4839388617026725:89.0:0.0,m17-2:20.0:33.0:27.0:3.8582368853264155:99.0:0.002,m14-2:29.0:57.0:9.0:3.6464075580381112:86.0:0.007666666666666666,m12-2:41.0:77.0:3.0:-2.842822977344748:65.0:0.002,m19-1:43.0:73.0:22.0:2.8828861745086267:97.0:0.003,m19-2:62.0:15.0:10.0:2.737427008766087:86.0:0.0,m6-2:69.0:71.0:22.0:-4.072370312960245:97.0:0.002,m15-1:97.0:1.0:0.0:4.3143827502503225:97.0:0.0\nThe output here is clipped to show the complete results of one simulation. We can alternatively import the simulated data into R as a dataset, which will allow us to view it in a nicer way. Start by browsing to default_iBioGen in the files pane, then click simdata-SIMOUT.csv and choose “Import Dataset…”:\n\n\n\nImport simulated data into R\n\n\nWhen the Import Text Data box opens the only thing you need to change is the Delimiter which you should set to “Whitespace”, then click “Import”:\n\n\n\nSet delimiter to ‘whitespace’\n\n\nFinally, your data imported are presented in the source pane in a nice tabular view. Don’t worry about understanding all (or really any) of these parameters yet, these are all the model parameters that control the simulations. The data which are output by the simulations are also in there, but you have to scroll way over to the right, and there isn’t really a good way to present those yet, because it’s still raw output and has to be summarized before it makes much sense (which we will do in the next lesson!).\n\n\n\nImported data in the source pane\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe dataset in R won’t automatically reload, so if you run more simulations and want to see the results here you’ll need to repeat the procedure of importing the simdata-SIMOUT.csv we just outlined here.\n\n\nIt seems kind of strange that all the parameters for all the simulations are identical. Wouldn’t it seem useful to be able to vary the parameters in some way to see how the model behaves under different parameter values? Yes, yes it would….\n\n\n9.3.6 Setting prior ranges on parameters\nRather than explicitly specifying model parameters in the params file, let’s say you’re interested in actually estimating them from the observed data. We can do this by simulating over a range of values for each parameter of interest, and then using an ML inference procedure to estimate these paramters. Let’s say you would like to estimate the size of the local community (J) and the migration rate into the local community (colrate). Edit your params file again in the source pane and change these parameters to specify ranges:\n1000-10000        ## [18] [J]: Number of individuals in the local community\n0.001-0.01        ## [19] [colrate]: Rate of colonization into the local community (% / birth event)\n## Run 10 more simulations using the update params file with parameter ranges\n$ iBioGen -p params-simdata.txt -s 10\n\n -------------------------------------------------------------\n  iBioGen [v.0.0.9]\n  Island Biodiversity Genomics Analysis Toolkit\n -------------------------------------------------------------\n    Parallelization disabled.\n    Generating 10 simulation(s).\n  [####################] 100%  Finished 10 simulations in   0:00:03|\nLet’s use the linux command cut to look at just the columns we’re interested in (J and colrate), which are the 17th and 18th columns.\n# -d \" \" indicates the _delimiter_ to use to split columns, a single space\n$ cut -f 17,18 -d \" \" default_iBioGen/simdata-SIMOUT.csv\nJ colrate\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n500 0.005\n1136 0.0012704714138716813\n2186 0.001345609936500913\n5897 0.0022431878203297132\n6243 0.006219848385382574\n1728 0.0013945959644766637\n1178 0.008246283294922729\n1009 0.0018079819887270672\n5229 0.006327492952445476\nAnd you’ll see that these parameter values are now taking a range, as we specified. In Part II of this tutorial you will see how we can combine massive amounts of simulations under varying parameter ranges with machine learning to estimate parameters of the model with real data.\n\n\n9.3.7 Hands-on experimentation: Exploring MESS model behavior with the CLI\nSpend the remaining time looking at the params file, and practicing changing parameters and running simulations. See if you can set parameter values that get the model to break! This can be an informative way to understand how models work! This is an open work session, so if you discover an interesting behavior please discuss with the group."
  },
  {
    "objectID": "mess_getting_started.html#key-points",
    "href": "mess_getting_started.html#key-points",
    "title": "9  Getting started with the MESS model",
    "section": "9.4 Key points",
    "text": "9.4 Key points\n\nMESS is a conceptual model for unifying biodiversity processes and predicting multidimensional biodiversity patterns. iBioGen and roleR are specific implementations of this conceptual model.\nThe iBioGen CLI offers a simple and efficient way to simulate MESS models.\nIn iBioGen, MESS model parameters are contained in a “params” file, and simulation results are written to a “SIMOUT” file.\nPlacing prior ranges on parameters allows to explore model behavior while varying numerous parameters independently."
  },
  {
    "objectID": "mess_simulations.html",
    "href": "mess_simulations.html",
    "title": "10  Using MESS API mode in RStudio to explore hypotheses",
    "section": "",
    "text": "How do I run MESS from inside RStudio?\nHow do I get simulation results in a format that is useful?\nHow do I visualize simulation results to gain insight?\nHow do I use MESS to explore/generate theoretical predictions?"
  },
  {
    "objectID": "mess_simulations.html#lesson-objectives",
    "href": "mess_simulations.html#lesson-objectives",
    "title": "10  Using MESS API mode in RStudio to explore hypotheses",
    "section": "10.2 Lesson objectives",
    "text": "10.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nRun MESS simulations in R\nSummarize and plot simulation results\nFormulate hypotheses for how tweaking parameters will affect model outputs\nUse MESS to test these hypotheses in silico\nSynthesize the outcomes"
  },
  {
    "objectID": "mess_simulations.html#planned-exercises",
    "href": "mess_simulations.html#planned-exercises",
    "title": "10  Using MESS API mode in RStudio to explore hypotheses",
    "section": "10.3 Planned exercises",
    "text": "10.3 Planned exercises\n\nImport MESS in R and run simulations in API mode\nLoad, inspect, and plot simulation results\nGroup brainstorming of params to tweak\nBreakout to test hypothesized model behavior given different parameter values\nRegroup to debrief/synthesize results\n\n\n10.3.1 Import MESS in R and run simulations in API mode\nThe MESS CLI is a simple and powerful way to run simulations, but the iBioGen package also provies an API mode (in both R and python) which lets you dive under the hood of the CLI mode a bit. You have all the power of the CLI mode, yet more flexibility. In the source pane of RStudio open a new RScript and import the iBioGen package.\n\n\n\n\n\n\nThe reticulate library\n\n\n\nA little bookkeeping is first required in loading the reticulate library, which is a binding layer between R and python, allowing seemless access to python modules from within R code.\n\n\nlibrary(reticulate)\n\n## Import the iBioGen package and give it an alias so we can refer to it as MESS\nMESS <- import(\"iBioGen\", as=\"MESS\")\n\n\n\n\n\n\nResolving “non-system installation of Python” warning.\n\n\n\n\n\nInfrequently, if you try to import the iBioGen module you will see the following warning message:\n> MESS <- import(\"iBioGen\", as=\"MESS\")\nNo non-system installation of Python could be found.\nWould you like to download and install Miniconda?\nMiniconda is an open source environment management system for Python.\nSee https://docs.conda.io/en/latest/miniconda.html for more details.\n\nWould you like to install Miniconda? [Y/n]: n\nInstallation aborted.\nThe resolution is to choose ‘n’, then go to Session->Restart R, and then try again, remembering to reload reticulate:\n> library(reticulate)\n> MESS <- import(\"iBioGen\", as=\"MESS\")\n>\n\n\n\nThe first step in API mode is to create a Core object. The Core object is the fundamental unit of the simulation, encompassing a standard Island Biogeography configuration with a very large Metacommunity and a much smaller LocalCommunity which is connected to it by colonization. In creating a Core object the only thing you’re required to pass in is a name, so lets go with “api-simdata”, to differentiate from the earlier simdata in CLI mode.\ncore = MESS$Core(\"api-simdata\")\nWe can view the parameters of this new core object with the get_params() method:\n## verbose=TRUE will format the parameters in a nice way when printed\ncore$get_params(verbose=TRUE)\n------- iBioGen params file (v.0.0.9)-------------------------------------------\napi-simdata          ## [0] [simulation_name]: The name of this simulation scenario\n./default_iBioGen    ## [1] [project_dir]: Where to save files\n1                    ## [2] [birth_rate]: Speciation rate\ntaxa                 ## [3] [meta_stop_criterion]: Whether to stop metacommunity on ntaxa or time\n20                   ## [4] [ntaxa]: Number of taxa to simulate if stop is `ntaxa`\n4                    ## [5] [time]: Amount of time to simulate if stop is `time`\nabundance            ## [6] [process]: Whether to evolve `abundance` or growth `rate` via BM\nTrue                 ## [7] [ClaDS]: Whether to allow speciation rate change a la ClaDS\n50000                ## [8] [abundance_mean]: Ancestral abundance at time 0\n0.1                  ## [9] [abundance_sigma]: Rate at which abundance changes if process is `abundance`\n0                    ## [10] [growth_rate_mean]: Ancestral population growth rate at time 0.\n0.01                 ## [11] [growth_rate_sigma]: Rate at which growth rate changes if process is `rate`\n0.1                  ## [12] [ClaDS_sigma]: Rate at which speciation rate changes if ClaDS is True\n0.9                  ## [13] [ClaDS_alpha]: Rate shift if ClaDS is True\n500                  ## [14] [sequence_length]: Length of the genomic region simulated, in base pairs\n1e-08                ## [15] [mutation_rate]: Mutation rate per base per generation\n10                   ## [16] [sample_size]: Number of samples to draw for calculating genetic diversity\nNone                 ## [17] [abundance_scaling]: Scaling abundance to Ne (None, log, ln or a ratio)\n1000                 ## [18] [J]: Number of individuals in the local community\n0.005                ## [19] [colrate]: Rate of colonization into the local community (% / birth event)\nneutral              ## [20] [assembly_model]: Selecting neutral or non-neutral assembly processes\n2                    ## [21] [ecological_strength]: Impact of competition or filtering on fitness\n1                    ## [22] [generation_time]: Generation time of local community taxa (in years)\ntime                 ## [23] [local_stop_criterion]: Stop local community on time or equilibrium\n100                  ## [24] [local_stop_time]: Local community simulation duration (in generations)\n100                  ## [25] [tau_max]: Duration of anagenetic speciation (in generations)\n10                   ## [26] [gene_flow_effect]: Damping effect of gene flow (in generations) on tau_max\n1000                 ## [27] [Ne_scaling]: Scaling individuals to demes in local community\nFalse                ## [28] [rm_duplicates]: Deduplicate seqs before calculating local community pi\nThese should all look somewhat familiar by now.\nChanging parameters of Core objects can be accomplished with the set_param() method:\n# Set a parameter to a fixed value\ncore$set_param(\"J\", 5000)\n# Set a parameter to a prior range to be sampled uniformly\ncore$set_param(\"colrate\", \"0.01-0.05\")\nFinally, we can generate simulations by calling the simulate() method and specifying the number of simulations to perform:\n## Lets do 100 simulations so there's a _chance_ we'll see some pattern in\n## in the data\ncore$simulate(nsims=100)\n    Generating 100.0 simulation(s).\n  [####################] 100%  Finished 100 simulations in   0:00:03| \nIf you are still in the default_iBioGen directory in your files pane, then you will see the new api-simdata-SIMOUT.csv file pop up there:\n\n\n\nNew api-simdata-SIMOUT.csv in the files pane\n\n\nNow you might see a glimmer of how the API mode might allow you much more power in manipulating parameters and running simulations, but there is more!\n\n\n10.3.2 Load, inspect, and plot simulation results\nRecall that the output of MESS simulations is a file which contains parameters and data generated with one simulation per line. In the raw form, the simulated data are bundled up in a somewhat obscure format, with reasonable looking information in most columns, but obscure information in three columns: metadata, metatree, and localdata. The MESS API mode provides a method to unpack and summarize this obscure data into a way that is more meaningful and human readable (and also useful for plotting), and this is the load_sims() method.\n# Load simulation results\nresults = core$load_sims()\n# Results is a 3 element list, each element is a data.frame that contains\n# one row per simulation\n# results[[1]] - params and sumstats\n# results[[2]] - Raw data per simulation (abunds, pis, traits, etc)\n# results[[3]] - global phylogeny per simulation\nLets focus for the moment on results[[1]], the data.frame of params and sumstats.\nresults[[1]]\n   birth_rate meta_stop_criterion ntaxa time   process ClaDS abundance_mean abundance_sigma\n1           1                taxa    20    4 abundance  TRUE          50000             0.1\n2           1                taxa    20    4 abundance  TRUE          50000             0.1\n3           1                taxa    20    4 abundance  TRUE          50000             0.1\n4           1                taxa    20    4 abundance  TRUE          50000             0.1\n5           1                taxa    20    4 abundance  TRUE          50000             0.1\n6           1                taxa    20    4 abundance  TRUE          50000             0.1\n7           1                taxa    20    4 abundance  TRUE          50000             0.1\n8           1                taxa    20    4 abundance  TRUE          50000             0.1\n9           1                taxa    20    4 abundance  TRUE          50000             0.1\n10          1                taxa    20    4 abundance  TRUE          50000             0.1\n   growth_rate_mean growth_rate_sigma ClaDS_sigma ClaDS_alpha sequence_length mutation_rate sample_size\n1                 0              0.01         0.1         0.9             500         1e-08          10\n2                 0              0.01         0.1         0.9             500         1e-08          10\n3                 0              0.01         0.1         0.9             500         1e-08          10\n4                 0              0.01         0.1         0.9             500         1e-08          10\n5                 0              0.01         0.1         0.9             500         1e-08          10\n6                 0              0.01         0.1         0.9             500         1e-08          10\n7                 0              0.01         0.1         0.9             500         1e-08          10\n8                 0              0.01         0.1         0.9             500         1e-08          10\n9                 0              0.01         0.1         0.9             500         1e-08          10\n10                0              0.01         0.1         0.9             500         1e-08          10\n   abundance_scaling    J    colrate assembly_model ecological_strength generation_time\n1               None 5000 0.02816428        neutral                   2               1\n2               None 5000 0.04468862        neutral                   2               1\n3               None 5000 0.01764501        neutral                   2               1\n4               None 5000 0.01780714        neutral                   2               1\n5               None 5000 0.04374193        neutral                   2               1\n6               None 5000 0.03645229        neutral                   2               1\n7               None 5000 0.01470986        neutral                   2               1\n8               None 5000 0.01802192        neutral                   2               1\n9               None 5000 0.01327007        neutral                   2               1\n10              None 5000 0.03985648        neutral                   2               1\n   local_stop_criterion local_stop_time tau_max gene_flow_effect Ne_scaling rm_duplicates\n1                  time             100     100               10       1000         FALSE\n2                  time             100     100               10       1000         FALSE\n3                  time             100     100               10       1000         FALSE\n4                  time             100     100               10       1000         FALSE\n5                  time             100     100               10       1000         FALSE\n6                  time             100     100               10       1000         FALSE\n7                  time             100     100               10       1000         FALSE\n8                  time             100     100               10       1000         FALSE\n9                  time             100     100               10       1000         FALSE\n10                 time             100     100               10       1000         FALSE\n   meta_obs_ntaxa meta_obs_time local_obs_time(gen) local_obs_time(eq) meta_turnover_rate   ClaDS_m\n1              20      7.238911                 100             0.3724         0.20000000 0.9045113\n2              20      3.304861                 100             0.6238         0.00000000 0.9045113\n3              20      3.618550                 100             0.7324         0.04761905 0.9045113\n4              20      2.464254                 100             0.4310         0.00000000 0.9045113\n5              20      4.223441                 100             0.8360         0.00000000 0.9045113\n6              20      2.421120                 100             0.8176         0.00000000 0.9045113\n7              20      5.467468                 100             0.6822         0.00000000 0.9045113\n8              20      2.940106                 100             0.4306         0.00000000 0.9045113\n9              20      7.681525                 100             0.7238         0.00000000 0.9045113\n10             20      5.754293                 100             0.9566         0.00000000 0.9045113\n   local_S abund_h1 abund_h2 abund_h3    pi_h1    pi_h2    pi_h3  trait_h1 trait_h2 trait_h3\n1       15 3.920161 2.333055 1.990338 5.600035 3.921122 3.358520  6.739850 4.842076 4.062254\n2       16 7.128577 5.040473 4.096290 8.526984 7.054373 6.291502  7.802848 6.135959 5.476494\n3       16 9.350507 7.267024 6.180439 7.649085 5.653503 4.763161 10.192978 8.298181 7.381126\n4       20 4.587044 2.794328 2.309903 6.338057 5.330343 4.886343  6.694479 5.049368 4.660101\n5       20 9.058059 6.625556 5.695691 7.261966 4.943116 4.148738 13.298536 9.102956 7.439101\n6       18 4.943500 3.162048 2.632319 8.009983 6.529084 5.872421  8.282393 5.839957 4.894036\n7       19 9.566252 6.654908 5.250049 9.211804 7.333543 6.414324 10.890139 8.424614 7.399630\n8       15 4.710206 2.813355 2.309257 8.199976 6.782519 6.118322  9.273341 6.583071 5.696876\n9       18 7.161965 5.070151 4.344310 5.869568 4.004179 3.236785 10.296570 8.314281 6.957238\n10      17 6.138167 4.205529 3.658744 8.177872 6.203475 5.101742 11.851781 7.690572 5.713765\nMost of these things should look familiar (if not understandable), but there are several new columns which are very important. You might also notice at first that the colrate parameters per simulation do indeed vary, while all others are fixed, as we specified a prior range on this paramter.\nThe new thing to notice is that all the data (previously encapsulate in a human unreadable format) has been transformed into a much more user friendly summarization including local_S (indicating local species richness), and 9 columns of the form *_h*, which represent the first 3 Hill numbers on abundance (abund), nucleotide diversity (pi), and trait value distributions (trait).\nSince we manipulated the colrate parameter in these simulations we can visualize the relationship between colrate and a few of the resulting summary statistics, like local_S, abund_h1, and pi_h1 using the built-in R plotting function plot().\n## For convenience, lets create a new variable for the results we're interested in\nsims <- results[[1]]\n\n## Now plot local_S, abund_h1, and pi_h1 as a function of colrate\nplot(sims$colrate, sims$local_S)\nplot(sims$colrate, sims$abund_h1)\nplot(sims$colrate, sims$pi_h1)\n  \nHere we can see a weak positive trend in all three summary statistics, with increasing colrate correlating with an increase in each of these data axes.\n\n\n\n\n\n\nDiscussion topic: Why would colrate impact these summary statistics in this way?\n\n\n\nThink for a moment about why increasing the colonization rate would increase local species richness, and the first Hill number of abundance and nucleotide diversity (on average). Let’s take a moment to reflect on these results.\n\n\nIt seems that local species richness is saturating, with uniformly high S which increasingly approaches 20 as colrate increases. What this means is that as colrate increases, the local community increasingly contains a total sample of the metacommunity, and there are no more unique lineages to colonize. Think back to what we learned yesterday about the neutral theory parameters. What parameter might we manipulate and how in order to relax this constraint and prevent the local community from saturating in this way?\n\n\n\n\n\n\nHow can we allow for more diversity in the local community?\n\n\n\n\n\nRemember the Jm parameter from neutral theory, which increases the # of species in the metacommunity. In MESS we call this ntaxa, but it represents exactly the same parameter. What would happen if we increase the size of the metacommunity species pool (ntaxa) and also ran many more simulations?\nTry increasing the value of ntaxa in your simulations, running it again, and plotting the results. How did this change affect S?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ncore$set_param(\"ntaxa\", 100)\ncore$simulate(nsims=500)\n\nresults = core$load_sims()\nsims <- results[[1]]\n\nplot(sims$colrate, sims$local_S)\nplot(sims$colrate, sims$abund_h1)\nplot(sims$colrate, sims$pi_h1)\nThere we go! More species richness in the metacommunity allows for more species richness in the local community with increasing colrate.\n  \n\n\n\n\n\n10.3.3 Group brainstorming of params to tweak\nSo far we have seen just a few MESS model parameters in action: J, colrate, and ntaxa. Looking back at the params there are many other parameters that will change the behavior of the model. For the purpose of this workshop we will only consider parameters 14-27, which focus on the LocalCommunity processes.\n500                  ## [14] [sequence_length]: Length of the genomic region simulated, in base pairs\n1e-08                ## [15] [mutation_rate]: Mutation rate per base per generation\n10                   ## [16] [sample_size]: Number of samples to draw for calculating genetic diversity\nNone                 ## [17] [abundance_scaling]: Scaling abundance to Ne (None, log, ln or a ratio)\n1000                 ## [18] [J]: Number of individuals in the local community\n0.005                ## [19] [colrate]: Rate of colonization into the local community (% / birth event)\nneutral              ## [20] [assembly_model]: Selecting neutral or non-neutral assembly processes\n2                    ## [21] [ecological_strength]: Impact of competition or filtering on fitness\n1                    ## [22] [generation_time]: Generation time of local community taxa (in years)\ntime                 ## [23] [local_stop_criterion]: Stop local community on time or equilibrium\n100                  ## [24] [local_stop_time]: Local community simulation duration (in generations)\n100                  ## [25] [tau_max]: Duration of anagenetic speciation (in generations)\n10                   ## [26] [gene_flow_effect]: Damping effect of gene flow (in generations) on tau_max\n1000                 ## [27] [Ne_scaling]: Scaling individuals to demes in local community\nFalse                ## [28] [rm_duplicates]: Deduplicate seqs before calculating local community pi\n\n\n\n\n\n\nDetailed Metacommunity parameters\n\n\n\nThe other parameters of this model all relate to the metacommunity speciation process, and can be used for many cool things which change features of the metacommunity like phylogenetic branch lengths, tree imbalance, trait distributions, and metacommunity abundance distributions.\n\n\n\n\n\n\n\n\nGroup discussion of model parameters and their behaviors\n\n\n\nLet’s take a moment to look at the focal parameters and the (brief) descrptions given in the params file. Think about what each parameter says that it does and which axis of biodiversity data this may affect. Will some parameters impact more than one axis of data? Discuss as a group, highlighting some specific parameters of interest.\nWe will keep track of parameters and their hypothesized impacts on the whiteboard.\n\n\nOf particular interest are the assembly_model and ecological_strength parameters. assembly_model can take three values: neutral (default), competition, and filtering, which specify the neutral and non-neutral community assembly processes within MESS. ecological_strength determines the ‘strength’ of the non-neutral assembly dynamics, and can take any value > 0. ecological_strength (sometimes es for short) values have the following effects:\n# es ~ 0 = More neutral\n# es ~ 1 = Noticeable non-neutrality\n# es >> 1 = Strong non-neutrality\n\n\n10.3.4 Breakout to test hypothesized model behavior given different parameter values\nTake some time (perhaps in small groups) to study the behavior of a few model parameters. It would probably benefit to make use of the power of API mode to create new Core objects with sensible names that indicate the parameters you wish to manipulate. This will allow you to retain and compare simulation results across runs with different parameter combinations. For example:\n## Here we give meaningful variable names to each Core object, so we can keep\n## track of them\nstr_comp = MESS$Core(\"strong_competition\")\nstr_comp$set_param(\"assembly_model\", \"competition\")\nstr_comp$set_param(\"ecological_strength\", 10)\nstr_comp$simulate(nsims=10)\nstr_comp_results = str_comp$load_sims()\n## Now you can summarize or visualize the results for these simulations\nTake the time remaining in this lesson to construct one or two models to examine some of the hypothesized model behaviors we discussed earlier. Be prepared to discuss the results of your simulation experiments with the group.\nIf you have some interesting results, you can add a slide or two to the MESS-SimulationExperiments-Results google slides presentation for us to discuss.\n\n\n\n\n\n\nAn easy way to get the raw data for one simulation\n\n\n\n\n\nYou can access the raw data for the most recent simulation by looking at the community property of the simulated LocalCommunity which is stored in the Core object as l (for “local community”). The community property returns a data.frame, as follows:\nstr_comp$l$community\n      coltime local_abund migrants      trait tau          pi is_founder\nm6-2        0         533      195  8.4844489  99 0.015466667          1\nm18-1      13         464        2 -4.1551176  33 0.007955556          0\nm4-1       98           1        0  4.4826415  98 0.000000000          0\nm15-1      99           1        0  3.1975592  99 0.000000000          0\nm12-1      99           1        0 -0.1183388  99 0.000000000          0\n\n\n\n\n\n10.3.5 Come together to debrief/synthesize results\nOk! Let’s wrap up this lesson and discuss some of the results from the simulation experiments. What did you discover?"
  },
  {
    "objectID": "mess_simulations.html#key-points",
    "href": "mess_simulations.html#key-points",
    "title": "10  Using MESS API mode in RStudio to explore hypotheses",
    "section": "10.4 Key points",
    "text": "10.4 Key points\n\nThe iBioGen package can run MESS models in either CLI or API mode, which provides a rich and flexible programatic method for specifying and generating simulations.\nChanging MESS parameters changes the predicted multi-dimensional biodiversity pattterns.\nSome MESS model parameters have predictable effects on patterns of data in the LocalCommunity and some have more diffuse or multi-dimensional effects."
  },
  {
    "objectID": "ml_inference_classification.html",
    "href": "ml_inference_classification.html",
    "title": "11  ML model selection with MESS simulations",
    "section": "",
    "text": "What do I do with all these MESS simulations?\nHow do I perform ML inference using MESS simulations in R?"
  },
  {
    "objectID": "ml_inference_classification.html#lesson-objectives",
    "href": "ml_inference_classification.html#lesson-objectives",
    "title": "11  ML model selection with MESS simulations",
    "section": "11.2 Lesson objectives",
    "text": "11.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nUse MESS simulations and RandomForest ML to perform community assembly model selection (classification)\nEvaluate uncertainty in classification accuracy using confusion matrices\nApply ML Classification to empirical data and interpret results in terms of story\nBrainstorm applications to other systems/empirical datasets"
  },
  {
    "objectID": "ml_inference_classification.html#planned-exercises",
    "href": "ml_inference_classification.html#planned-exercises",
    "title": "11  ML model selection with MESS simulations",
    "section": "11.3 Planned exercises",
    "text": "11.3 Planned exercises\n\nLecture: How does ML work with MESS simulations?\nMotivating community assembly model classification\nGet the MESS simulations and do some set-up\nOverview of major steps of machine learning process\nImplement ML assembly model classification\nHands-on Exercise: Predicting assembly_model of mystery simulations\n\n\n11.3.1 Lecture: How does ML work with MESS simulations?\nLecture: How does machine learning work with MESS simulations?\n\n\n11.3.2 Motivating community assembly model classification\nThe first step is now to assess the model of community assembly that best fits the data. The three models are neutral, in which all individuals are ecologically equivalent; competition, in which species have traits, and differential survival probability is weighted by distance of traits from the trait mean in the local community (closer to the trait mean == higher probability of death); and filtering, in which there is an environmental optimum, and proximity of trait values to this optimum is positively correlated with survival probability.\nBasically we want to know, are individuals in the local community ecologically equivalent, and if not are they interacting more with each other or more with the local environment. To accomplish this, we are going to use simulation-based supervised machine learning, which is what the rest of this lesson is all about.\n\n\n11.3.3 Run MESS simulations\nOk, so now we know what we want to know: Are the “data” (the Hill # summary statistics for abundance, traits, and genetics) sufficient to distinguish among community assembly models? How do we go about evaluating this question? Well, the first step is to generate a large number simuations under each of the three assembly models, so we have a representative sample of patterns in the data characteristic of each assembly model.\nCoding exercise: Create a new MESS$Core object and use a for loop to run 2 simulations for each of the three community assembly models: neutral, competition, and filtering.\n\n\n\n\n\n\nSolution: Running a loop over assembly models to generate simulations\n\n\n\n\n\nMESS <- import(\"iBioGen\", as=\"MESS\")\ncore = MESS$Core(\"watdo\")\n\n## Set all the parameters common to all simulations\ncore$set_param(\"ntaxa\", \"300\")\ncore$set_param(\"J\", \"40000-150000\")\ncore$set_param(\"colrate\", \"0.0001-0.005\")\ncore$set_param(\"ecological_strength\", \"0.1-10.0\")\ncore$set_param(\"local_stop_criterion\", \"equilibrium\")\ncore$set_param(\"local_stop_time\", \"0.5-1.0\")\n\nfor (model in c(\"neutral\", \"competition\", \"filtering\")){\n  core$set_param(\"assembly_model\", model)\n  core$simulate(nsims=2)\n}\n\n\n\n\n11.3.3.1 Download the pre-baked simulations\nSince it can take quite some time to run a number of simulations sufficient for model selection and parameter estimation we will use a suite of pre-baked simulations generated ahead of time. We want to practice good organizational habits for our simulations and .R scripts, so as a challenge, before downloading the simulations file, **make a new directory in your home directory called “MESS-inference” and change directory into it.*\n\n\n\n\n\n\nSolution: Make a ‘MESS-inference’ directory in your home directory\n\n\n\n\n\n## Make sure you are in your home directory\ncd ~\n## Make the 'MESS-inference' directory\nmkdir MESS-inference\n## change into the new directory\ncd MESS-inference\n## print working directory to verify you are where you should be\npwd\n/home/rstudio/MESS-inference\n\n\n\nOnce you are in the /home/rstudio/MESS-inference directory, you may fetch the simulations from the workshop site using wget:\nwget https://github.com/role-model/process-models-workshop/raw/main/data/MESS-SIMOUT.csv.gz\ngunzip MESS-SIMOUT.csv.gz\nwc -l MESS-SIMOUT.txt\nMESS-SIMOUT.csv.gz   100%[====================>]  35.24M  1.12MB/s    in 32s\n2023-06-15 14:31:54 (1.12 MB/s) - ‘MESS-SIMOUT.csv.gz’ saved [36954693/36954693]\n3000 SIMOUT.txt\n\n\n\n\n\n\nNote\n\n\n\nThe wc command counts the number of lines if you pass it the -l flag. You can see this series of 3000 simulations is about 35MB gzipped.\n\n\n\n\n\n11.3.4 Implement ML assembly model classification\n\n11.3.4.1 Overview of major steps of simulation-based machine learning process\nBefore we dive in, it’s maybe useful to have an idea of the roadmap of what we’ll be doing. The major steps of implementing simulation-based machine learning are:\n\nLoad the simulations and do some set-up\nTransform the simulations\nTrain the ML model\nEvaluate how well it trained\nApply it to real data\nClean up and you’re done\n\n\n\n11.3.4.2 Do some set-up\nLooking in the “Files” tab, double click on the new “MESS-inference” folder and you should see your new “MESS-SIMOUT.csv.gz” file. Now you can create a “New Blank File->Rscript” in this directory by clicking on the small green plus. When prompted to enter a file name, use “MESS-classification.R”.\n\n\n\nMake a new blank file for the MESS inference R code\n\n\nNow we will install a couple necessary packages: randomForest and caret, two machine learning packages for R.\n## install.packages only has to happen ONCE within your R environment, you don't\n## need to install.packages if you create a new file, you can skip straight\n## to loading the libraries.\ninstall.packages(\"randomForest\")\ninstall.packages(pkgs = \"caret\", dependencies = c(\"Depends\", \"Imports\"))\nlibrary(randomForest)\nlibrary(caret)\n\n## Make sure you're _in_ the MESS-inference working directory\nsetwd(\"/home/rstudio/MESS-inference\")\n\n\n11.3.4.3 Load the pre-baked simulations\nSimulation results are composed of ‘parameters’ and ‘data’. Parameters are what goes in to the simulation model and data is what comes out.\nBefore, we had used core$load_sims() to load the simulations generated by a specific MESS Core object, but here we have a file that we got from the internet. There was a Core objec that created this somewhere, but we don’t have access to it. Luckily, there is a utility function (MESS$load_local_sims()) which allows us to load data from an external file. Let’s use it now:\nsimdata = MESS$load_local_sims(\"MESS-SIMOUT.csv\")[[1]]\nThe simulated data is in the form that we have already looked at previously, so we don’t need to look at it again here now, there’s just a lot more of it.\n\n\n11.3.4.4 Transform the simulations\nIn some cases parameters/data must be reshaped in some way to make it more appropriate for the ML approach.\nOne constraint of the randomForest classification process is that the “target” variable (or “response” variable) must be an R factor, which is essentially a categorical variable. We can convert the assembly_model column in our simdata data.frame to a factor, and then count the number of elements in each level with table().\nsimdata$assembly_model <- as.factor(simdata$assembly_model)\ntable(simdata$assembly_model)\ncompetition     filtering   neutral\n       1000          1000       999\n\n\n\n\n\n\nA perfectly reasonable explanation\n\n\n\n\n\nIf anyone can guess why this is, I will buy you dinner tonight.\n\n\n\n\n\n11.3.4.5 Split the data into training/testing sets\nIf you train a machine learning model with all of your data, then you won’t have any way to really evaluate its performance. It’s very common to split data into ‘training’ and ‘testing’ sets, where the training data used to train the model and the testing data is used to evaluate how well it performs.\n## 70/30 test/train split\ntmp <- sample(2, nrow(simdata), replace = TRUE, prob = c(0.7, 0.3))\ntrain <- simdata[tmp==1,]\ntest <- simdata[tmp==2,]\n\n\n11.3.4.6 Train the model on the training set\nLearn a mapping between patterns in the data and community assembly model.\nThe randomForest() function takes two important arguments:\n\nThe data upon which to operate, in this case the train set of training data\nA formula describing the desired model of the relationship between dependent variables (on the left of the ~) and independent variables (on the right of the ~).\n\nThe formula below indicates that we wish to express assembly_model as a function of local_S, pi_h1, abund_h1, and trait_h1. We chose these 4 predictor variables to demonstrate an ML model which considers all the axes of biodiversity, without being overly complex and including all the Hill numbers for each axis of data. There’s no reason to believe this is the best model, and you could certainly experiment with manipulating the independent variables in this formula.\n## Experiment with results for different axes of data!\nrf <- randomForest(assembly_model ~ local_S + pi_h1 + abund_h1 + trait_h1, data=train, proximity=TRUE)\nprint(rf)\nCall:\n randomForest(formula = assembly_model ~ local_S + pi_h1 + abund_h1 + trait_h1, data = train, proximity = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 19.41%\nConfusion matrix:\n            competition filtering neutral class.error\ncompetition         538        33     124   0.2258993\nfiltering            24       571      89   0.1652047\nneutral              61        71     560   0.1907514\n\n\n\n\n\n\nTuning model parameters, feature selection, iterating performance evaluation\n\n\n\n\n\nThere is a LOT of fancy stuff that we are NOT going to do in this workshop, so please don’t imagine we are giving you the full picture. We are doing enough to give a flavor of how ML inference works, and to show that even when done ‘quick and dirty’ it still works pretty okay. To do this properly would require an entire workshop in itself, which we would encourage you to look into in the future!\n\n\n\n\n\n11.3.4.7 Test the model on the testing set\nHow well does the trained model recover the ‘known’ assembly model in simulations that it hasn’t seen yet.\nNow we have trained the rf model and it’s time to evaluate how well it can classify simulations that it hasn’t previously seen. This is exactly what the testing dataset is for! We can use the testing data, which the rf model has not encountered, to determine accuracy of classification on new data.\nWe use the built-in R function predict() to feed the test data to the trained rf model, and it will record classification predictions for each simulation in the training set.\ntest_predictions <- predict(rf, test)\ncm <- confusionMatrix(test_predictions, test$assembly_model)\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    competition filtering neutral\n  competition         233        13      32\n  filtering            23       270      28\n  neutral              49        33     247\n\nOverall Statistics\n                                         \n               Accuracy : 0.8082         \n                 95% CI : (0.7814, 0.833)\n    No Information Rate : 0.3405         \n    P-Value [Acc > NIR] : < 2e-16        \n                                         \n                  Kappa : 0.7122         \n                                         \n Mcnemar's Test P-Value : 0.08011        \n\nStatistics by Class:\n\n                     Class: competition Class: filtering Class: neutral\nSensitivity                      0.7639           0.8544         0.8046\nSpecificity                      0.9278           0.9167         0.8680\nPos Pred Value                   0.8381           0.8411         0.7508\nNeg Pred Value                   0.8892           0.9242         0.8998\nPrevalence                       0.3287           0.3405         0.3308\nDetection Rate                   0.2511           0.2909         0.2662\nDetection Prevalence             0.2996           0.3459         0.3545\nBalanced Accuracy                0.8459           0.8855         0.8363\nThe simplest way to evaluate classification accuracy is to look at the “Confusion matrix”. In the testing set we know the true assembly model the data was generated under, and we want to know how often the trained RF model is able to recover the known assembly model from the held out testing data. The confusion matrix reports the contingency table of reference vs predicted assembly model outcomes. With perfect classification you would expect 100% agreement between the ‘Reference’ model and the ‘Prediction’ model, with all values in the confusion matrix falling along the diagonal. Off-diagonal elements represent the frequency of mis-classification and the category of the mis-classified prediction (e.g. predictions of ‘neutral’ when the known assembly model was ‘competition’).\nAlready we can see that classification accuracy is relatively high, with the majority of testing simulations being accurately classified, and few (~15%) being mis-classified in one way or the other.\nIt can sometimes be useful to visualize confusion matrices using the heatmap function in base R, which produces a plot representing exactly the data in the contingency table above.\nheatmap(cm$table, Rowv=NA, Colv=NA, margins=c(12, 12), xlab=\"Prediction\", ylab=\"Reference\")\n\n\n\nVisualization of the confusion matrix\n\n\n\n\n11.3.4.8 Getting prediction probabilities for one individual simulation\nWhen we call predict(rf, test) we are using the classifier to make predictions for all the simulations in the testing set. This takes the single assembly model with the highest prediction probability as the predicted value (essentially it is a point estimate). Often it is useful (particularly with empirical data) to have a more comprehensive view of the prediction probabilities across classes. We can pass in just one simulated dataset and ask predict to give us the type=\"prob\", which will return exactly these.\n# Take the first row of testing data as just one simulation to predict\n# `type=\"prob\" indicates that we wish to return the percent probability of\n# each assembly_model, rather than the point estimate.\np_emp <- predict(rf, test[1, ], type=\"prob\")\np_emp\n  competition filtering neutral\n1       0.016      0.04   0.944\nActivity: Spend a few moments making predictions for several other of the simulated datasets by changing the 1 above to 2 or 3 or some other integer values, just to get the hang of it.\n\n\n\n11.3.5 Predict assembly model of empirical data\nAnd “Embrace the uncertainty in the model classification.\nNow we will do a brief analysis of some empirical data, so you can see that the classification prediction process for empirical data is identical to that of simulated data: training the model and calling predict.\nFor this exercsie we will use genetic data from a soil arthropod metabarcoding study published by Noguerales et al (2021). The data is comprised of soil micro-arthropod COI sequences sampled from four different habitat types in the montane forests of Cyprus. Fetch the empirical data from Noguerales et al 2021 (which we have again preprocessed and stored on github):\nIn the terminal wget the data for one habitat type from the workshop repository:\n## Be sure you are in the right working directory inside the terminal\ncd /home/rstudio/MESS-Inference\nwget https://github.com/role-model/process-models-workshop/raw/main/empirical_data/Qa_hills.txt\nNow go back into the R console and load the data for this sample. It might also help to take a quick look at what the data for this sample looks like, so also print it to the screen.\nQa_emp <- read.csv(\"/home/rstudio/MESS-inference/Qa_hills.txt\")\nQa_emp\n  local_S   pi_h1    pi_h2    pi_h3\n1     156 48.5767 41.83594 37.69666\nThe empirical data was generated from a metabarcoding dataset, and for this exercise we provide the data already collapsed into summary statistics: the number of species in the local community (local_S) and the first three Hill numbers of genetic diversity (pi_h1, pi_h2, pi_h3). Earlier when we fit the randomForest model we used the first Hill numbers of abundance, traits, and genetic data, so before we can predict() community assembly model with the real data we need to re-fit the rf classifier so it’s predictor variables match those of the empirical data.\n# Fit the model including only genetic Hill numbers to the training set\nrf <- randomForest(assembly_model ~ local_S + pi_h1 + pi_h2 + pi_h3, data=train, proximity=TRUE)\npredict(rf, Qa_emp, type=\"prob\")\n  competition filtering neutral\n1       0.172     0.742   0.086\n\n\n\n\n\n\nOther empirical data from Noguerales et al 2021\n\n\n\n\n\nwget https://github.com/role-model/process-models-workshop/raw/main/empirical_data/Cb_hills.txt\nwget https://github.com/role-model/process-models-workshop/raw/main/empirical_data/Pb_hills.txt\nwget https://github.com/role-model/process-models-workshop/raw/main/empirical_data/Pn_hills.txt\n\n\n\n\n\n11.3.6 Hands-on Exercise: Predicting assembly_model of mystery simulations\nTime allowing, you may wish to try to classify some simulated multi-dimensional biodiversity data with unknown (to you) assembly_model values. There exists a new simulated data file which contains 20 mystery simulations with random assembly_model values. Choose one of these and try to re-run the classification procedure we just completed.\nFirst go into your terminal and download the mystery simulations:\ncd /home/rstudio/MESS-inference\nwget https://github.com/role-model/process-models-workshop/raw/main/data/Mystery-data.csv\nNow see if you can load the mystery simulations and do the classification inference on one or a couple of them. Try to do this on your own, but if you get stuck you can check the hint here:\n\n\n\n\n\n\nHints: Loading the mystery sims and making predictions\n\n\n\n\n\n## Importing the mystery data\nmystery_sims = read.csv(\"/home/rstudio/MESS-inference/Mystery-data.csv\")\n\n## Predict for mystery sim 1\nmyst_sim = 1\nmystery_pred = predict(rf, mystery_sims[myst_sim, ], type=\"prob\")\nprint(mystery_pred)\n\n\n\nA link to the key containing the true assembly_model values is hidden below. Don’t peek until you have a guess for your simulated data! How close did you get?\n\n\n\n\n\n\nThe Key is here\n\n\n\n\n\nAssembly models per mystery simulation are in this file\n\n\n\n\n\n11.3.7 More to explore with R randomForest package\nThis is just a tiny taste of the randomForest R package, and a tiny molecule of everything possible with machine learning, but we hope it gives the flavor still.\n\n\n\n\n\n\nMore to explore if you are ahead of the group\n\n\n\n\n\nEvaluate variable importances\nWhich axes of data contribute most to distinguishing assembly models? Details of how and why this works are left as an exercise to the enthusiastic reader.\nvarImpPlot(rf,\n           sort = T,\n           n.var = 4,\n           main = \"Variable Importance\")\n\n\n\nVariable importance in RF classification\n\n\nMultidimensional scaling\nMDSplot() is another way to inspect how the simulations are being clustered by the RF model. It’s a little too much ‘inside baseball’ for this workshop, but if you get ahead of the group you should try it out.\nMDSplot(rf, train$assembly_model)\n\n\n\nMDSplot depicting the clustering of simulations by assembly model"
  },
  {
    "objectID": "ml_inference_classification.html#key-points",
    "href": "ml_inference_classification.html#key-points",
    "title": "11  ML model selection with MESS simulations",
    "section": "11.4 Key points",
    "text": "11.4 Key points\n\nMachine learning models can be used to “classify” categorical response variables given multi-dimensional input data.\nMajor components of machine learning inference include gathering and transforming data, splitting data into training and testing sets, training the ml model, and evaluating model performance.\nML inference is inherently uncertain, so it is important to evaluate uncertainty in ml prediction accuracy and propagate this into any downstream interpretation when applying ml to empirical data."
  },
  {
    "objectID": "ml_inference_regression.html",
    "href": "ml_inference_regression.html",
    "title": "12  ML parameter estimation with MESS simulations",
    "section": "",
    "text": "How do I estimate parameter values for continuous MESS model parameters?\nHow do I evaluate the uncertainty of ML parameter estimates?"
  },
  {
    "objectID": "ml_inference_regression.html#lesson-objectives",
    "href": "ml_inference_regression.html#lesson-objectives",
    "title": "12  ML parameter estimation with MESS simulations",
    "section": "12.2 Lesson objectives",
    "text": "12.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nUse simulations and randomForest ML to perform parameter estimation for key MESS model parameters.\nEvaluate uncertainty in parameter estimation accuracy using cross-validation simulations.\nApply ML parameter estimation to empirical data and interpret results in terms of story\nBrainstorm applications to real data"
  },
  {
    "objectID": "ml_inference_regression.html#planned-exercises",
    "href": "ml_inference_regression.html#planned-exercises",
    "title": "12  ML parameter estimation with MESS simulations",
    "section": "12.3 Planned exercises",
    "text": "12.3 Planned exercises\n\nMotivating MESS process-model parameter estimation\nImplement ML parameter estimation\nHands-on Exercise: Predicting MESS parameters of mystery simulations\nPosterior predictive simulations (if time)\n\n\n12.3.1 Motivating MESS process-model parameter estimation\nNow that we have identified the neutral model as the most probable, we can estimate parameters of the emipirical data given this model. Essentially, we are asking the question “What are the parameters of the model that generate summary statistics most similar to those of the empirical data?”\n\n\n12.3.2 Implement ML parameter estimation\nLet’s start with a new clean Rscript, so create a new file and save it as “MESS-Regression.R”, then load the necessary libraries and simulated data.\n\n12.3.2.1 Load libraries, setwd, and load the simulated data\nlibrary(randomForest)\nlibrary(caret)\nlibrary(reticulate)\n\nsetwd(\"/home/rstudio/MESS-inference\")\nsimdata = MESS$load_local_sims(\"MESS-SIMOUT.csv\")[[1]]\n\n\n12.3.2.2 Extract sumulations generated under the ‘filtering’ assembly model\nLet’s pretend that the most probable model from the classification procedure was ‘environmental filtering’. If our goal is to estimate parameters under this model then we want to exclude simulations from the ml inference that do not fit our most probable model. This can be achieved by selecting rows in the simdata data.frame that have “filtering” as the assembly_model.\nsimdata = simdata[simdata$assembly_model == \"filtering\", ]\nsimdata\n\n\n12.3.2.3 Split train and test data as normal\nAgain, we’ll split the data into training and testing sets.\ntmp <- sample(2, nrow(simdata), replace = TRUE, prob = c(0.7, 0.3))\ntrain <- simdata[tmp==1,]\ntest <- simdata[tmp==2,]\n\n\n12.3.2.4 Train the ML regressor as normal\nTrain the ml model to perform regression, as our focal dependent variable takes continuous values. The randomForest package auto-detects if the dependent variable is continuous or categorical, so there’s nothing more to do there. We will use the same formula as before, specifying local_S and the first Hill number on each axis of biodiversity as the predictor variables.\nrf <- randomForest(colrate ~ local_S + pi_h1 + abund_h1 + trait_h1, data=train, proximity=TRUE)\nrf\n\n\n12.3.2.5 Plot results of predictions for colrate of held-out training set\nFor a regression analysis we can’t use a confusion matrix because the response is continuous, so instead we evaluate prediction accuracy by making a scatter plot showing predictions as a function of known simulated values. With perfect prediction accuracy the points in the scatter plot would fall along the identity line.\npreds <- predict(rf, test)\nplot(preds, test$colrate)\n\n\n\nSimulated and predicted colonization rates\n\n\n\n\n\n\n\n\nWhat can we say about the accuracy of ml prediction of colonization rate?\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction accuracy should be quantified with a formal method.\n\n\n\n\n\nWhat we are doing here is a very qualitative, ad hoc evaluation of prediction accuracy. This is another case where the full details of performing ml inference are slightly out of scope for this workshop, given the amount of time we have. In a real analysis one should more formally quantify this with mean squared error, or mean absolute error, or pseudo-R^2, some of which are provided in the randomForest package, e.g.:\nrf <- randomForest(colrate ~ local_S + pi_h1 + abund_h1 + trait_h1, data=train, proximity=TRUE)\nmean(rf$rsq)\n0.6219401\n\n\n\n\n\n12.3.2.6 Predict colrate of test simulation and plot distribution of predictions\nNow we can practice making predictions for a single simulation and looking at uncertainty in the prediction. Like we did before we can select one simulation by using the test [#, ] row selection strategy on our test data.frame.\nWhen we ask for predict.all=TRUE this will return the prediction value for each tree in the rf, and we can plot the aggregation of these predictions using hist to show a histogram.\n\n\n\n\n\n\nReturn values of predict() for regression\n\n\n\n\n\nWhen predict.all=TRUE the predict() function returns a list with 2 elements:\n\nThe predicted value (point estimate)\nA vector of predicted values for each tree in the forest\n\n\n\n\npreds <- predict(rf, test[2, ], predict.all=TRUE)\n# The predicted value is element [[1]]\nprint(preds[[1]])\n# A vector of predictions for each tree in the forest [[2]]\nhist(preds[[2]])\n0.001191852\n\n\n\nDistribution of colonization rate predictions for a single simulation\n\n\nWhat can we say about the uncertainty on parameter estimation for this one simulation?\n\n\n\n12.3.3 Hands-on Exercise: Predicting MESS parameters of mystery simulations\nNow see if you can load the mystery simulations and do the regression inference on one or a couple of them. Try to do this on your own, but if you get stuck you can check the hint here:\n\n\n\n\n\n\nHints: Loading the mystery sims and making predictions\n\n\n\n\n\n## Importing the mystery data\nmystery_sims = read.csv(\"/home/rstudio/MESS-inference/Mystery-data.csv\")\n\n## Predict for mystery sim 1\nmyst_sim = 1\nmystery_pred = predict(rf, mystery_sims[myst_sim, ])\nprint(mystery_pred)\n\n\n\nA link to the key containing the true colrate values is hidden below. Don’t peek until you have a guess for your simulated data! How close did you get?\n\n\n\n\n\n\nThe Key is here\n\n\n\n\n\nAssembly models per mystery simulation are in this file\n\n\n\n\n\n\n\n\n\nMore to explore if you are ahead of the group\n\n\n\n\n\nIf you are ahead of the group you will see that the “key” also contains the true values of J and ecological_strength so you might try performing the regression inference on these as well. Are either of these parameters harder or easier to estimate than colrate? Why do you think that is?\nYou might also try manipulating the predictor variables before training the ml model and evaluating the impact on performance. For example, try only using abundance Hill numbers, or only genetics Hill numbers. Try using all the Hill numbers for all the data axes. How does the composition of the predictor variables change the ml parameter estimation accuracy?\n\n12.3.4 Posterior predictive simulations (if time)\nPosterior predictive simulations are a good practice to assess the goodness of fit of the model to the data. Essentially this involves running new simulations setting MESS model parameters to the most probable estimates from the ML inference, and then evaluating the fit of the new simulations to the empirical data, typically by projecting summary statistics of the simulated and observed data and checking that they all fall within a point cloud.\nWe probably won’t have time to get to this, but it’s a good practice in fitting ml models to real data."
  },
  {
    "objectID": "ml_inference_regression.html#key-points",
    "href": "ml_inference_regression.html#key-points",
    "title": "12  ML parameter estimation with MESS simulations",
    "section": "12.4 Key points",
    "text": "12.4 Key points\n\nMachine learning models can be used to “estimate parameters” of continuous response variables given multi-dimensional input data.\nMajor components of machine learning inference include gathering and transforming data, splitting data into training and testing sets, training the ml model, and evaluating model performance.\nML inference is inherently uncertain, so it is important to evaluate uncertainty in ml prediction accuracy and propagate this into any downstream interpretation when applying ml to empirical data."
  },
  {
    "objectID": "wrapup.html",
    "href": "wrapup.html",
    "title": "13  Wrap-up",
    "section": "",
    "text": "What have we covered?\nWhere can I find information and support post-workshop?\nHow can I submit feedback on this workshop?"
  },
  {
    "objectID": "wrapup.html#lesson-objectives",
    "href": "wrapup.html#lesson-objectives",
    "title": "13  Wrap-up",
    "section": "13.2 Lesson objectives",
    "text": "13.2 Lesson objectives\nAfter this lesson, learners should be able to…\n\nIdentify (personal) takeaways from the workshop\nKnow where to look for help from the instructors and the internet post-workshop\nSend us feedback!"
  },
  {
    "objectID": "wrapup.html#key-points",
    "href": "wrapup.html#key-points",
    "title": "13  Wrap-up",
    "section": "13.3 Key points",
    "text": "13.3 Key points\n\nwat\ndo\nThank you for joining us!!!!!!!!!!!"
  }
]