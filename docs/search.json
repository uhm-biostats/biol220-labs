[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Labs for Intro Biostatistics",
    "section": "",
    "text": "Setup"
  },
  {
    "objectID": "index.html#software-and-cloud-services",
    "href": "index.html#software-and-cloud-services",
    "title": "Labs for Intro Biostatistics",
    "section": "Software and Cloud Services",
    "text": "Software and Cloud Services\nAll labs will use R, RStudio, and Posit Cloud.\nR is a computer program that allows an extraordinary range of statistical calculations. It is a free program, mainly written by volunteer contributors from around the world.\nRStudio is a separate program, also free, that allows you to easily organize separate tabs for R code files, graphic, help docs, and more.\nFor this course, you won’t need R and RStudio installed on your own device because we will work in Posit Cloud. However, if you wish to install them on your device, go to https://rstudio.com/products/rstudio/download/ for instructions about getting set up."
  },
  {
    "objectID": "index.html#using-posit-cloud",
    "href": "index.html#using-posit-cloud",
    "title": "Labs for Intro Biostatistics",
    "section": "Using Posit Cloud",
    "text": "Using Posit Cloud\nWe have setup Workspaces for each lab assignment and each lab section. For example, the workspace for lab01 materials for section 001 is lab01-001. Join the Workspace for each lab by clicking on the appropriate link in Google Classroom (e.g. “Lab 01-005 Posit Cloud Link” for the first week’s materials for section 005)\nImportant: Login with your userid@hawaii.edu account, not a personal gmail account. If you accidentally use your personal account, let me know. I will delete it while you register with the university account.\nNote: Wherever images say “lab02”, replace that with the actual lab number you will be completing (e.g. “lab01” or “lab11”).\n\n\nOnce you’re logged in, the lab01 Workspace will looking something like this (note: where it says “-monday” in the picture should be your section number):\n\n\nEach item listed under “All Projects” is a “Project” within the lab01-section Workspace. Each week I will create a Project template that you will copy and work from (you cannot edit the original).\n\nTo create your own, click “START” to the left of “lab01”.\n\n\n\nOnce the Project is copied and deployed, rename the Project from “lab01” to include your name like this: lab01-muir-chris\n\n\nDisclaimer: These labs rely heavily on those developed by Mike Whitlock for his BIOL 300 course at UBC https://www.zoology.ubc.ca/~whitlock/bio300/. In some cases the materials have been used verbatim, in other cases Chris Muir, and/or Andy Rominger have added new material, or heavily modified the original material by Prof. Whitlock."
  },
  {
    "objectID": "lab01.html#goals",
    "href": "lab01.html#goals",
    "title": "1  Getting started",
    "section": "1.1 Goals",
    "text": "1.1 Goals\n\nLearning how to start with Posit Cloud\nUse the command line\nUse functions in R\nUse vectors\nUse data frames"
  },
  {
    "objectID": "lab01.html#r-rstudio-and-posit-cloud",
    "href": "lab01.html#r-rstudio-and-posit-cloud",
    "title": "1  Getting started",
    "section": "1.2 R, RStudio, and Posit Cloud",
    "text": "1.2 R, RStudio, and Posit Cloud\n\n1.2.1 What is R?\nR is a computer program that allows an extraordinary range of statistical calculations. It is a free program, mainly written by voluntary contributions from statisticians around the world. R is available on most operating systems, including Windows, Mac OS, and Linux.\nR can make graphics and do statistical calculations. It is also a full-fledged computing language. In this manual, we will only scratch the surface of what R can do.\n\n\n1.2.2 What is RStudio?\nRStudio is a separate program, also free, that provides a more elegant front end for R. RStudio allows you to easily organize separate windows for R commands, graphic, help, etc. in one place.\nFor this course, you won’t need R and RStudio installed on your own device because we will work in Posit Cloud. However, if you wish to install them on your device, go to https://rstudio.com/products/rstudio/download/ for instructions about getting set up.\n\n\n1.2.3 What is Posit Cloud?\nPosit Cloud allows you to run a hosted version of RStudio in the cloud that allows you to run R without having to download anything on your personal computer or confirgure your personal computer. You access Posit Cloud from your browser, and conveniently you can access from any computer, not just your personal device.\n\n\n1.2.4 Resources\n\nPosit Cloud provides many learning materials: interactive tutorials covering the basics of data science, cheatsheets, and a guide to using Posit Cloud.\n\n\n\n1.2.5 Getting started with Posit Cloud\nFollow the instructions on the “Setup” page for setting yourself up with Posit Cloud.\nOnce done, proceed with the rest of the lab below."
  },
  {
    "objectID": "lab01.html#learning-the-tools",
    "href": "lab01.html#learning-the-tools",
    "title": "1  Getting started",
    "section": "1.3 Learning the Tools",
    "text": "1.3 Learning the Tools\nWhen you start Posit Cloud, it will automatically start RStudio as well. You run R inside RStudio, itself inside Posit Cloud.\nAfter you have started Posit Cloud, you should see a new window with a menu bar at the top and three main sections. One of the sections is called the “Console” – this is where you type commands to give instructions to R and typically where you see R’s answers to you.\nAnother important corner of this window can show a variety of information. Most importantly to us, this is where graphics will appear, under the tab marked “Plots”.\n\n1.3.1 The command line\nWhen you start Posit Cloud, you’ll see a corner of the window called the “Console.” By the default the console window is in the bottom left of the RStudio screen.\nYou can type commands in this window where there is a prompt (which will look like a &gt; sign at the bottom of the window). The Console has to be the selected window. (Clicking anywhere in the Console selects it.)\nThe &gt; prompt is R’s way of inviting you to give it instructions. You communicate with R by typing commands after the &gt; prompt.\nType “2+2” at the &gt; prompt, and hit return. You’ll see that R can work like a calculator (among its many other powers). It will give you the answer, 4, and it will label that answer with [1] to indicate that it is the first element in the answer. (This is sort of annoying when the answers are simple like this, but can be very valuable when the answers become more complex.)\nRemember, you don’t type the &gt; sign. The &gt; is the prompt that R gives saying it is ready for input. We reproduce it here so you can see which is input (in blue) and which is output (in black or red).\n\n2 + 2\n\n[1] 4\n\n\nYou can use a wide variety of math functions to make calculations here, e.g., log() calculates the log of a number:\n\nlog(42)\n\n[1] 3.73767\n\n\n(By default, this gives the natural log with base \\(e\\).)\nParentheses are used both as a way to group elements of the calculation and also as a way to denote the arguments of functions. (The “arguments” of a function are the set of values given to it as input.) For example, log(3) is applying the function log() to the argument 3.\nAnother mathematical function that often comes in handy is the square root function, sqrt(). For example, the square root of 4 is:\n\nsqrt(42)\n\n[1] 6.480741\n\n\nTo calculate a value with an exponent, used the ^ symbol. For example \\(4^3\\) is written as:\n\n4 ^ 3\n\n[1] 64\n\n\nNote how R ignores white space when it’s not in quotes (we’ll come back to quotes later):\n\n4^3\n\n[1] 64\n\n4  ^  3\n\n[1] 64\n\n\nOf course, many math functions can be combined to give an almost infinite possibility of mathematical expressions. For example,\n\\[\\frac{1}{\\sqrt{2 \\pi (3.1)^2}} e ^ {-\\frac{(12 - 10.7) ^ 2}{2 \\times 3.1}}\\]\ncan be calculated with\n\n(1 / (sqrt(2 * pi * (3.1) ^ 2))) * exp(-(12 - 10.7) ^ 2 / (2 * 3.1))\n\n[1] 0.09798692\n\n\n\n\n1.3.2 Saving your code\nWhen you analyze your own data, we strongly recommend that you keep a record of all commands used, along with copious notes, so that weeks or years later you can retrace the steps of your earlier analysis.\nIn Posit Cloud, you can create a plain text file (sometimes called a script), which contains R commands that can be reloaded and used at a later date. We have created a scratch file where you can enter and save your commands while you’re learning the tools. Click on “scratch.R” in the lower-right:\n\nThat will open a mostly blank text file above the Console that looks like this:\n\nYou can copy and paste any commands that you want from the Console, or type directly here. (When you copy and paste, do not include the &gt; prompt in the script.) Save this script for later reference by hitting “Save” under the “File” menu. In the future you can open this file to have those commands available for use again.\nIt is a good habit to type all your commands in the script window and run them from there, rather than typing directly into the console. This lets you save a record of your session so that you can more easily re-create what you have done later.\nFYI, if you want to create a new, blank R script, here’s how: under the menu at the top, choose “File”, then “New File”, and then “R Script”. Follow the prompts to save the new file.\n\n\n1.3.3 Comments\nIn scripts, it can be very useful to save a bit of text which is not to be evaluated by R. You can leave a note to yourself (or a collaborator) about what the next line is supposed to do, what its strengths and limitations are, or anything else you want to remember later. To leave a note, we use “comments”, which are a line of text that starts with the hash symbol #. Anything on a line after a # will be ignored by R.\n\n# This is a comment. Running this in R will \n# have no effect.\n\n\n\n1.3.4 Functions\nMost of the work in R is done by functions. A function has a name and one or more arguments. For example, log(4) is a function that calculates the log in base \\(e\\) for the value 4 given as input.\nSometimes functions have optional input arguments. For the function log(), for example, we can specify the optional input argument base to tell the function what base to use for the logarithm. If we don’t specify the base variable, it has a default value of base = e. To get a log in base 10, for example, we would use:\n\nlog(4, base = 10)\n\n[1] 0.60206\n\n\n\n\n1.3.5 Defining variables\nIn R, we can store information of various sorts by assigning them to variables. For example, if we want to create a variable called x and give it a value of 4, we would write\n\nx &lt;- 4\n\nThe middle bit of this—a less than sign and a hyphen typed together to make something that looks a little like a left-facing arrow – tells R to assign the value on the right to the variable on the left. After running the command above, whenever we use x in a command it would be replaced by its value 4. For example, if we add 3 to x, we would expect to get 7.\n\nx + 3\n\n[1] 7\n\n\nVariables in R can store more than just simple numbers. They can store lists of numbers, functions, graphics, etc., depending on what values get assigned to the variable.\nWe can always reassign a new value to a variable. If we now tell R that x is equal to 32\n\nx &lt;- 32\n\nthen x takes its new value:\n\nx\n\n[1] 32\n\n\n\n\n1.3.6 Names\nNaming variables and functions in R is pretty flexible.\nA name has to start with a letter, but that can be followed by any combination of letters, numbers, and underscores (_). Names cannot have spaces or any character other than letters, numbers, and underscores, for example $, -, and % are not allowed. Technically, periods (.) are allowed in names, but not reccomended excpet for specific uses outside the scope of this course.\nNames in R are case-sensitive, which means that Weights and weights are completely different things to R. This is a common and incredibly frustrating source of errors in R.\nIt’s a good idea to have your names be as descriptive as possible, so that you will know what you meant later on when looking at it. (However, if they get too long, it becomes painful and error prone to type them each time we use them, so this, as with all things, requires moderation.)\nSometimes clear naming means that it is best to have multiple words in the name, but we can’t have spaces. Therefore a common approach is like we saw in the previous section, to chain the words with underscores (not hyphens!), as in weights_before_hospital. (Another solution to make separate words stand out in a variable name is to vary the case: weightsBeforeHospital. This is called “Camel Case” because the capital letters are like camel humps.)\n\n\n1.3.7 Vectors\nOne useful feature of R is the ability to apply functions to an entire collection of numbers. The technical term for a set of numbers is “vector”. For example, the following code will create a vector of six numbers:\n\n c(78, 85, 64, 54, 102, 98.6)\n\n[1]  78.0  85.0  64.0  54.0 102.0  98.6\n\n\nc() is a function that creates a vector, containing the items given in its arguments. To help you remember, you could think of the function c() meaning to “combine” some elements into a vector.\nLet’s add a little extra here to make the computer remember this vector. Let’s assign it to a variable, called temperatureF (because these numbers are actually a set of temperatures in degrees Fahrenheit):\n\ntemperatureF &lt;- c(78, 85, 64, 54, 102, 98.6)\n\nThe combination of the less than sign and the hyphen makes an arrow pointing from right to left—this tells R to assign the stuff on the right to the name on the left. In this case we are assigning a vector to the variable temperatureF.\nInputting this into R causes no obvious output, but R will now remember this vector of temperatures under the name temperatureF. We can view the contents of the vector temperatureF by simply typing its name:\n\ntemperatureF\n\n[1]  78.0  85.0  64.0  54.0 102.0  98.6\n\n\nThe power of vectors is that R can do the same calculation on all elements of a vector with one command. For example, to convert a temperature in Fahrenheit to Celsius, we would want to subtract 32 and multiply times 5/9. We can do that for all the numbers in this vector at once:\n\ntemperatureC &lt;- (temperatureF - 32) * 5 / 9\ntemperatureC\n\n[1] 25.55556 29.44444 17.77778 12.22222 38.88889 37.00000\n\n\nTo pull out one of the numbers in this vector, we add square brackets after the vector name, and inside those brackets put the index of the element we want. (The “index” is just a number giving the location in the vector of the item we want. The first item has index 1, etc.) For example, the second element of the vector temperatureC is\n\ntemperatureC[2]\n\n[1] 29.44444\n\n\nOne of the ways to slip up in R is to confuse the [square brackets] which pull out an element of a vector, with the (parentheses), which is used to enclose the arguments of a function.\nVectors can also operate mathematically with other vectors. For example, imagine you have a vector of the body weights of patients before entering hospital (weight_before_hospital) and another vector with the same patient’s weights after leaving hospital (weight_after_hospital). You can calculate the change in weight for all these patients in one command, using vector subtraction:\n\nweight_before_hospital &lt;- c(100, 102)\nweight_after_hospital &lt;- c(98, 99)\n\nweight_change_during_hospital &lt;- weight_before_hospital - weight_after_hospital\n\nThe result will be a vector that has each patient’s change in weight.\n\n\n1.3.8 Basic calculation examples\nIn this course, we’ll learn how to use a few dozen functions, but let’s start with a couple of basic ones.\nThe function mean() does just what it sounds like: it calculates the sample mean (that is, the average) of the vector given to it as input. For example, the mean of the vector of the temperatures in degrees Celsius from above is 26.81481:\n\nmean(temperatureC)\n\n[1] 26.81481\n\n\nAnother simple (and simply named) function calculates the sum of all numbers in a vector: sum().\n\nsum(temperatureC)\n\n[1] 160.8889\n\n\nTo count the number of elements in a vector, use length().\n\nlength(temperatureC)\n\n[1] 6\n\n\nThis shows that there are 6 temperature values in the vector that make up the vector temperatureC.\n\n\n1.3.9 Reading a data file\nIn this course, we have saved the data in a “comma-separated variable” format. All files in this format ought to have “.csv” as the end of their file name. A CSV file is a plain text file, easily read by a wide variety of programs. Each row in the file (besides the first row) is the data for a given individual, and for each individual each variable is listed in the same order, separated by commas. It’s important to note that you can’t have commas anywhere else in the file, besides the separators.\nThe first row of a CSV file should be a “header” row, which gives the names of each variable, again separated by commas.\nFor examples in this tutorial, let’s use a data set about the passengers of the RMS Titanic. One of the data sets in the folder of data attached to this lab is called “titanic.csv”. This is a data set of 1313 passengers from the voyage of this ship, which contains information about some personal info about each passenger as well as whether they survived the accident or not.\nTo import a CSV file into R, use the read.csv() function as in the following command. (This assumes that you have set the working directory to the labs folder, as we described above.)\n\ntitanic_data &lt;- read.csv(\"data/titanic.csv\")\n\nThis looks for the file called titanic.csv in the folder called data. Here we have given the name titanic_data to the object in R that contains all this passenger data. Of course, if you wanted to load a different data set, you would be better off giving it a more apt name than “titanic_data”.\nTo see if the data loads appropriately, you might want to run the command\n\nsummary(titanic_data)\n\n passenger_class        name                age            embarked        \n Length:1313        Length:1313        Min.   : 0.1667   Length:1313       \n Class :character   Class :character   1st Qu.:21.0000   Class :character  \n Mode  :character   Mode  :character   Median :30.0000   Mode  :character  \n                                       Mean   :31.1942                     \n                                       3rd Qu.:41.0000                     \n                                       Max.   :71.0000                     \n                                       NA's   :680                         \n home_destination       sex              survive         \n Length:1313        Length:1313        Length:1313       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n\n\nwhich will list all the variables and some summary statistics for each variable.\n\n\n1.3.10 Intro to data frames\nA data frame is a way that R can store a data set on a number of individuals. A data frame is a collection of columns; each column contains the values of a single variable for all individuals. The values of each individual occur in the same order in all the columns, so the first value for one variable represents the same individual as the first value in the lists of all other variables.\nThe function read.csv() loads the data it reads into a data frame.\nThe data frame is usually given a name, which is used to tell R’s functions which data set to use. For example, in the previous section we read in a data set to a data frame that we called titanic_data. This data frame now contains information about each of the passengers on the Titanic. This data frame has seven variables, so it has seven columns (passenger_class, name, age, embarked, home_destination, sex, and survive).\nVery importantly, we can grab one of the columns from a data frame by itself. We write the name of the data frame, followed by a $, and then the name of the variable.\nFor example, to show a list of the age of all the passengers on the Titanic, use\n\ntitanic_data$age\n\nThis will show a vector that has the values for this variable age, one for each individual in the data set.\nNote, when looking at long vectors or data frames, it’s convenient to use the head function, which only shows the first 6 elements, not the whole huge vector or data frame.\n\nhead(titanic_data$age)\n\n[1] 29.0000  2.0000 30.0000 25.0000  0.9167 47.0000\n\n\n\n\n1.3.11 Adding a new column\nSometimes we would like to add a new column to a data frame. The easiest way to do this is to simply assign a new vector to a new column name, using the $.\nFor example, to add the log of age as a column in the titanic_data data frame, we can write\n\ntitanic_data$log_age &lt;- log(titanic_data$age)\n\nYou can run the command head(titanic_data) to see that log_age is now a column in titanic_data.\n\n\n1.3.12 Choosing subsets of data\nSometimes we want to do an analysis only on some of the data that fit certain criteria. For example, we might want to analyze the data from the Titanic using only the information from females. The easiest way to do this is to use the subset function.\nIn the titanic data set there is a variable named sex. We can create a new data frame that includes only the data from passengers recorded as female with the following command:\n\ntitanic_female_data &lt;- subset(titanic_data, sex == \"female\")\nhead(titanic_female_data)\n\n   passenger_class                                       name age    embarked\n1              1st                  Allen,MissElisabethWalton  29 Southampton\n2              1st                   Allison,MissHelenLoraine   2 Southampton\n4              1st  Allison,MrsHudsonJ.C.(BessieWaldoDaniels)  25 Southampton\n7              1st              Andrews,MissKorneliaTheodosia  63 Southampton\n9              1st    Appleton,MrsEdwardDale(CharlotteLamson)  58 Southampton\n12             1st Astor,MrsJohnJacob(MadeleineTalmadgeForce)  19   Cherbourg\n              home_destination    sex survive   log_age\n1                   StLouis,MO female     yes 3.3672958\n2  Montreal,PQ/Chesterville,ON female      no 0.6931472\n4  Montreal,PQ/Chesterville,ON female      no 3.2188758\n7                    Hudson,NY female     yes 4.1431347\n9            Bayside,Queens,NY female     yes 4.0604430\n12                  NewYork,NY female     yes 2.9444390\n\n\nThis new data fame will include all the same columns as the original titanic_data, but it will only include the rows for which the sex was “female”.\nNote that the syntax here requires a double == sign. In R (and many other computer languages), the double equal sign creates a statement that can be evaluated as TRUE or FALSE. Here we are asking, for each individual, whether sex is “female”."
  },
  {
    "objectID": "lab01.html#questions-for-lab-report",
    "href": "lab01.html#questions-for-lab-report",
    "title": "1  Getting started",
    "section": "1.4 Questions for Lab Report",
    "text": "1.4 Questions for Lab Report\nYour lab report is due before the start of next week’s lab. When you’re finished, save it, and your TA can access it on the Cloud.\n\nFor each week, use the R script called “report.R” to save the commands that you use to answer the questions, as well as the answers themselves.\n\nOpen the report.R file. Start by editing comments with your name at the top. \nSave the script regularly as you work! To save, go to “File &gt; Save” or use command-S shortcut. \nFor each of the questions below, write the question number as a comment, followed by any R code you use to do the question, and give the answers as comments. It might look something like this:\n\n\n\n# Student's name\n# BIOL 220 Lab 11\n# 2023-01-12\n\n# Questions\n\n# 1. I followed directions to set up my lab report\n\n# 2. Yes, I got the same answers!\n\n# 3. Only text answers, no code\n# Answer part 1\n# Answer part 2\n\n# 4. Text and code\n# a.\nx &lt;- c(1, 2, 3) # you can also comment like this\n\n# b.\nmean(x)\n\n# The mean of c(1, 2, 3) is 2. Here's what that means...\n\n\nRun the Learning the tools commands in R from your “scratch.R” script. Did you get the same answers as shown in the text? (Answer “yes”, “no”, or a more detailed explanation. You don’t need to re-run all the code and output here.)\nFor each of the following, come up with a variable name that would be appropriate to use in R for the listed variable:\n\n\n\n\nVariable\nName in R\n\n\n\n\nBody temperature in Celsius\n\n\n\nHow much aspirin is given per dose for a patient\n\n\n\nNumber of televisions per person\n\n\n\nHeight (including neck and extended legs) of giraffes\n\n\n\n\n\nUse R to calculate:\n\n\\(15 \\times 17\\)\n\\(13^3\\)\n\\(\\text{log}_e(14)\\) (natural log)\n\\(\\text{log}_{10}(100)\\) (base 10 log)\n\\(\\sqrt{81}\\)\n\nWeddell seals live in Antarctic waters and take long strenuous dives in order to find fish to feed upon. Researchers (Williams et al. 2004) wanted to know whether these feeding dives were more energetically expensive than regular dives (perhaps because they are deeper, or the seal has to swim further or faster). They measured the metabolic costs of dives using the oxygen consumption of 10 animals (in ml O\\(_2\\) / kg) during a feeding dive. Here are the data:\n\n71.0, 77.3, 82.6, 96.1, 106.6, 112.8, 121.2, 126.4, 127.5, 143.1\n\nFor the same 10 animals, they also measured the oxygen consumption in non-feeding dives. With the 10 animals in the same order as before, here are those data:\n\n42.2, 51.7, 59.8, 66.5, 81.9, 82.0, 81.3, 81.3, 96.0, 104.1\n\n\nMake a vector for each of these lists, and give them appropriate names.\nConfirm (using R) that both of your vectors have the same number of individuals in them.\nCreate a vector called metabolism_difference by calculating the difference in oxygen consumption between feeding dives and nonfeeding dives for each animal.\nWhat is the average difference between feeding dives and nonfeeding dives in oxygen consumption?\nThe arithmetic mean is calculated by adding up all the numbers and dividing by how many numbers there are. Calculate the mean of these numbers using sum() and length(). Did you get the same answer as with using mean()?\nAnother appropriate way to represent the relationship between these two numbers would be to take the ratio of O\\(_2\\) consumption for feeding dives over the O\\(_2\\) consumption of nonfeeding dives. Make a vector which gives this ratio for each seal.\nSometimes ratios are easier to analyze when we look at the log of the ratio. Create a vector which gives the log of the ratios from the previous step. (Use the natural log.) What is the mean of this log-ratio?\n\nThe data file called “countries.csv” in the data folder contains information about all the countries on Earth1. Each row is a country, and each column contains a variable.\n\nUse read.csv() to read the data from this file into a data frame called countries.\nUse summary() to get a quick description of this data set. What are the first three variables?\nUsing the output of summary(), how many countries are from Africa in this data set?\nWhat kinds of variables (i.e., categorical or numerical) are continents, cell_phone_subscriptions_per_100_people_2012, total_population_in_thousands_2015, and fines_for_tobacco_advertising_2014? (Don’t go by their variable names – look at the data in the summary results to decide.)\nAdd a new column to your countries data frame that has the difference in ecological footprint between 2012 and 2000. What is the mean of this difference? (Note: this variable will have “missing data”, which means that some of the countries do not have data in this file for one or the other of the years of ecological footprint. By default, R doesn’t calculate a mean unless all the data are present. To tell R to ignore the missing data, add an option to the mean() command that says na.rm=TRUE. We’ll learn more about this later.)\n\nUsing the countries data again, create a new data frame called africa_data, that only includes data for countries in Africa. What is the sum of the total_population_in_thousands_2015 for this new data frame?"
  },
  {
    "objectID": "lab01.html#footnotes",
    "href": "lab01.html#footnotes",
    "title": "1  Getting started",
    "section": "",
    "text": "These data mainly come from the World Health Organization, but the Continent list comes from https://datahub.io/ and the ecological footprint and cell phone data come from http://www.nationmaster.com.↩︎"
  },
  {
    "objectID": "lab02.html#goals",
    "href": "lab02.html#goals",
    "title": "2  Graphics in R",
    "section": "2.1 Goals",
    "text": "2.1 Goals\n\nKnow how to load packages to expand the capabilities of R\nKnow some basic graphical formats and when they are useful.\nMake graphs in R, such as histograms, bar charts, box plots, and scatter plots.\nBe able to suggest improvements to basic graphs to improve readability and accurate communication\n\nAs will be the case with all labs, this lab will be completed using Posit Cloud. If needed, refer back to “Setup” and “Lab 1” for instructions on access and setting up your workspace for this lab. Here is the direct link to the Posit Cloud shared workspace for Lab 2: xyz."
  },
  {
    "objectID": "lab02.html#learning-the-tools",
    "href": "lab02.html#learning-the-tools",
    "title": "2  Graphics in R",
    "section": "2.2 Learning the Tools",
    "text": "2.2 Learning the Tools\n\n2.2.1 Extending R’s capabilities with packages\nR has a lot of power in its basic form, but one of the most important parts about R is that it is expandable by the work of other people. These expansions are usually released in “packages”.\nEach package needs to be installed on your computer only once, but to be used it has to be loaded into R during each session.\nTo install a package in RStudio, click on the packages tab from the sub-window with tables for Files, Plots, Packages, Help, and Viewer. Immediately below that will be a button labeled “Install” – click that and a window will open.\n\nIn the second row (labeled “Packages”), type ggplot2. Make sure the box for “Install dependencies” near the bottom is clicked, and then click the “Install” button at bottom right. This will install the graphics package ggplot2.\n\nAlternatively, you can also use a function to install packages:\n\ninstall.packages(\"ggplot2\")\n\nInstalling a package only needs to be done once on a given computer or a given Posit Cloud Workspace, and that package is permanently available.\n\n\n2.2.2 Loading a package\nOnce a package is installed, it needs to be loaded into R during a session if you want to use it. You do this with the function called library().\n\nlibrary(ggplot2)\n\nNow we can start making graphics with the ggplot2 package.\n\n\n2.2.3 ggplot\nWhile base R has ample graphic capabilities, functions from the ggplot2 package are becoming the de facto standard for scientific graphics because they allow more easy customization of plots.\nTo make a graph with ggplot2, you need to specify at least two elements in your command. The first uses the function ggplot itself, to specify which data frame you want to visualize and also which variables are to be plotted. The second part tells R what kind of graph to make, using a geom function. The odd part is that these two parts are put together with a + sign. It’s simplest to see this with an example. We’ll draw a histogram with ggplot in the next section.\n\n\n2.2.4 Histograms\nUseful when:\n\nResponse variable is numerical\n\nA histogram represents the frequency distribution of a numerical variable in a sample.\nLet’s see how to make a basic histogram using the age data from the Titanic data set. Make sure you have loaded the data (using read.csv()) into a data frame called titanic_data.\n\ntitanic_data &lt;- read.csv(\"data/titanic.csv\")\n\nHere’s the ggplot2 code to make a simple histogram of age:\n\nggplot(titanic_data, aes(x = age)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nNotice that there are two functions called here, put together in a single command with a + sign. You don’t have to put a line break after the + (R ignores it), but it makes the code more readable. The first function is ggplot, and it has two input arguments. The first is titanic_data, this is the name of the data frame containing the variables that we want to graph. The second input to ggplot is an aes function. In this case, the aes function tells R that we want Age to be the \\(x\\)-variable (i.e. the variable that is displayed along the \\(x\\)-axis). The “aes” stands for “aesthetics”.\nThe second function in this command is geom_histogram(). This is the part that tells R that the “geometry” of our plot should be a histogram.\nRunning this should give a plot that look something like this:\n\n\n\n\n\nThis is not the most beautiful graph in the world, but it conveys the information. At the end of this tutorial we’ll see a couple of options that can make a ggplot graph look a little better.\n\n\n2.2.5 Bar graphs\nUseful when:\n\nResponse variable is categorical\n\nA bar graph plots the frequency distribution of a categorical variable. With ggplot, the syntax for a bar graph is very similar to that for a histogram. For example, here is a bar graph for the categorical variable sex in the titanic data set:\n\nggplot(titanic_data, aes(x = sex)) + \n  geom_bar(stat = \"count\")\n\nAside from specifying a different variable for \\(x\\) in the aes function, we use a different geom function here, geom_bar, and specify the statistic we want to draw, which is the count (or frequency) of the different categories. The result should look like this:\n\n\n\n\n\n\n\n2.2.6 Boxplots\nUseful when:\n\nExplanatory variable is categorical\nResponse variable is numerical\n\nA boxplot is a convenient way of showing the distribution of a numerical variable in multiple groups. Here’s the code to draw a boxplot for age in the titanic data set, separately for each recorded sex:\n\nggplot(titanic_data, aes(x = sex, y = age)) + \n  geom_boxplot()\n\nNotice that the \\(y\\) variable here is age, and \\(x\\) is the categorical variable sex that goes on the \\(x\\)-axis. The other new feature here is the new geom function, geom_boxplot().\n\n\n\n\n\nHere the thick bar in the middle of each boxplot is the median of that group. The upper and lower bounds of the box extend from the first to the third quartile. The vertical lines are called whiskers, and they cover most of the range of the data (except when data points are pretty far from the median, then they are plotted as individual dots, as on the male boxplot).\n\n\n2.2.7 Scatterplots\nUseful when:\n\nExplanatory variable is numerical\nResponse variable is numerical\n\nScatterplots shows the relationship between two numerical variables.\nThe titanic data set does not have two numerical variables, so let’s use a different data set. We will plot the relationship between sea surface temperature and species richness of reef fishes as compiled by Barneche et. al (2019). These data come from many different published fish surveys conducted by many different researchers all around the world. Barneche and colleagues compiled those data to try to understand what environmental variables predict the species richness of reef fish. Let’s find out!\nYou can load the data with:\n\nreef_fish &lt;- read.csv(\"data/global-reef-fish.csv\")\n\nTo make a scatter plot of the variables temp_C and spp_richness with ggplot, you need to specify the \\(x\\) and \\(y\\) variables, and use geom_point():\n\n# Side note:  I've added a line break between arguments in ggplot()\n# This has no effect on the code, but makes it easier to read IMO\nggplot(reef_fish, \n       aes(x = temp_C, y = spp_richness)) +\n  geom_point()\n\nThe result look like this:\n\n\n\n\n\n\n\n2.2.8 Better looking graphics with options\nThe code we have listed here for graphics barely scratches the surface of what ggplot2, and R as a whole, are capable of. Not only are there far more choices about the kinds of plots available, but there are many, many options for customizing the look and feel of each graph. You can choose the font, the font size, the colors, the style of the axes labels, etc., and you can customize the legends and axes legends nearly as much as you want.\nLet’s dig a little deeper into just a couple of options that you can add to any of the forgoing graphs to make them look a little better. For example, you can change the text of the \\(x\\)-axis label or the \\(y\\)-axis label by using xlab() or ylab(). Let’s do that for the scatterplot, to make the labels a little nicer to read for humans.\n\nggplot(reef_fish, \n       aes(x = temp_C, y = spp_richness)) +\n  geom_point() +\n  xlab(\"Temperature (degrees C)\") +\n  ylab(\"Species richness\")\n\nThe labels that we want to add are included in quotes inside the xlab() and ylab() functions. Here is what appears:\n\n\n\n\n\nIt can also be nice to remove the default gray background, to make what some feel is a cleaner graph. Try adding\n\n+ theme_minimal()\n\nto the end of one of your lines of code making a graph, to see whether you prefer the result to the default design.\n\n\n2.2.9 Color palettes\nIt is important to use a palette that will be clear to color blind individuals and, in some cases, to those who view a printed version in greyscale. There are bewildering array of options, but the viridis palettes accomplish these goals well (read more here). We’ll revisit the histogram example above and view the age distribution on the Titanic by sex (multiple histogram). This is a bit more advanced than what we’ve covered so far, but hang in there. We’ll go step-by-step.\nThe cool thing about ggplot2 is we can assign a large number of graphical features (size, color, fill, shape, line type, etc.) to variables on our data. We’ll do that using the fill = ... argument in the aes() function to make the fill of the bars dependent on sex.\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThat works, but it’s pretty ugly. For one thing, the bars are stacked on top of one another, so it’s hard to see the separate histograms for males and females. We’ll fix that by using the position = ... argument in the geom_histogram() function like this:\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  # I will interleave comments to explain what's going on\n  # position = position_identity() stops the bars from stacking\n  geom_histogram(position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWell, that’s worse! Now the male bars are blocking the female bars. Let’s add a couple more arguments, making the color 50% transparent using the alpha = ... argument. I’ll also make the color around the bars black so we can see them better.\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  # alpha = 0.5 makes bars transparent\n  # color = \"black\" adds black lines around bars\n  geom_histogram(alpha = 0.5, color = \"black\", position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nBetter, but not great. Let’s use the facet_grid() function to put the histograms on separate panels. For this, we have to put sex in quotes (learning when things need to be quoted or not is frustrating).\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  # facet_grid() makes separate panels for each sex\n  facet_grid(rows = \"sex\") +\n  geom_histogram(alpha = 0.5, color = \"black\", position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nPretty good. Now, let’s finally add the viridis color palette. ggplot2 has some built in functions that you can just add using the + operator to change the color, like this:\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n  facet_grid(rows = \"sex\") +\n  geom_histogram(alpha = 0.5, color = \"black\", position = position_identity()) +\n  # This function changes the color palette\n  scale_fill_viridis_d()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n2.2.10 Getting help\nThe help pages in R are the main source of help, but the amount of detail might be off-putting for beginners. For example, to explore the options for ggplot(), enter the following into the R Console.\n\nhelp(ggplot)\n\n# you can also use\n?ggplot\n\nThis will cause the contents of the manual page for this function to appear in the Help window in RStudio Cloud. These manual pages are often frustratingly technical. What many of us do instead is simply google the name of the function—there are a great number of resources online about R."
  },
  {
    "objectID": "lab02.html#questions",
    "href": "lab02.html#questions",
    "title": "2  Graphics in R",
    "section": "2.3 Questions",
    "text": "2.3 Questions\n\nUse the data from “countries.csv” to practice making some graphs.\n\nRead the data from the file “countries.csv” in the “data” folder. (Hint: we did this in the last lab - you need to use read.csv(), use the correct path, and give the object a name.)\nMake sure that you have run library(ggplot2). Why is this necessary for the remainder of this question?\nMake a histogram to show the frequency distribution of values for measles_immunization_oneyearolds, a numerical variable. (This variable gives the percentage of 1-year-olds that have been vaccinated against measles.) Describe the pattern that you see.\nMake a bar graph to show the numbers of countries in each of the continents. (The categorical variable continent indicates the continent to which countries belong.)\nDraw a scatterplot that shows the relationship between the two numerical variables life_expectancy_at_birth_male and life_expectancy_at_birth_female.\n\nThe ecological footprint is a widely-used measure of the impact a person has on the planet. It measures the area of land (in hectares) required to generate the food, shelter, and other resources used by a typical person and required to dispose of that person’s wastes. Larger values of the ecological footprint indicate that the typical person from that country uses more resources.\nThe countries data set has two variables showing the ecological footprint of an average person in each country. ecological_footprint_2000 and ecological_footprint_2012 show the ecological footprints for the years 2000 and 2012, respectively.\n\nPlot the relationship between the ecological footprint of 2000 and of 2012.\nDescribe the relationship between the footprints for the two years. Does the value of ecological footprint of 2000 seem to predict anything about its value in 2012?\nFrom this graph, does the ecological footprint tend to go up or down in the years between 2000 and 2012? Did the countries with high or low ecological footprint change the most over this time? (Hint: you can add a one-to-one line to your graph by adding + geom_abline(intercept = 0, slope = 1) to your ggplot() command. This will make it easier to see when your points are above or below the line of equivalence.)\n\nPlotting categorical and numerical variables: use the countries data again. Plot the relationship between continent and female life expectancy at birth. Describe the patterns that you see.\nMuchala (2006) measured the length of the tongues of eleven different species of South American bats, as well as the length of their palates (to get an indication of the size of their mouths). All of these bats use their tongues to feed on nectar from flowers. Data from the article are given in the file “BatTongues.csv”. In this file, both Tongue Length and Palette Length are given in millimeters. Each value for tongue length and palate length is a species mean, calculated from a sample of individuals per species.\n\nImport the data and inspect it using summary(). You can call the data set whatever you like, but in one of the later steps we’ll assume it is called bat_tongues.\nDraw a scatter plot to show the association between palate length and tongue length, with tongue length as the response variable. Describe the association: is it positive or negative? Is it strong or weak?\nAll of the data points that went into this graph have been double checked and verified. With that in mind, what conclusion can you draw from the outlier on the scatterplot?\nLet’s figure out which species is the outlier. To do this, we’ll use the subset function from Lab 1. Remember, the function subset gives us the row (or rows) of a data frame that has a certain property. Looking at the graph, we can tell that the point we are interested in has a very long tongue_length, at least over 80 mm long! Use subset to figure out the species name of this unusually long-tongued bat.\nThe unusual species is Anoura fistulata (See a photo here). This species has an outrageously long tongue, which it uses to collect nectar from a particular flower (can you guess what feature of the flower has led to the evolution of such a long tongue?). See the article by Muchala (2006) to learn more about the biology of this strange bat.\n\nImprove your figure! Pick one of the plots you made using R today. What could be improved about this graph to make it a more effective presentation of the data?"
  },
  {
    "objectID": "lab03.html#goals",
    "href": "lab03.html#goals",
    "title": "3  Workikng with data",
    "section": "3.1 Goals",
    "text": "3.1 Goals\n\nLearn to create data files\nExplore the importance of random sampling\nUnderstand variance and standard deviation\nUnderstand mean and median\n\nAs will be the case with all labs, this lab will be completed using Posit Cloud. If needed, refer back to “Setup” and “Lab 1” for instructions on access and setting up your workspace for this lab. Here is the direct link to the Posit Cloud shared workspace for Lab 2: xyz."
  },
  {
    "objectID": "lab03.html#learning-the-tools",
    "href": "lab03.html#learning-the-tools",
    "title": "3  Workikng with data",
    "section": "3.2 Learning the Tools",
    "text": "3.2 Learning the Tools\n\n3.2.1 Structure of a good data file\nData files appear in many formats, and different formats are sometimes preferable for different tasks. But there is one way to structure data — called “long” format — that is extremely useful for most things that you will want to do in statistics and R.\nLong format is actually very simple. Every row in the data set is a unique individual. Every column is a variable being measured on those individuals.\nFor example, last week in Question 4 we looked at some data about the tongue and palate lengths of several species of bats. There were three variables in that data set, the species name, tongue length, and palate length. Here each “individual” is a species. Here is that data in long format—each row is an individual. There are three columns, one for each variable:\n\n\n\n\n\nspecies\npalate_length\ntongue_length\n\n\n\n\nLichonycteris obscura\n10.0\n36.1\n\n\nGlossophaga comissarisi\n10.7\n26.6\n\n\nGlossophaga soricina\n11.4\n30.2\n\n\nAnoura caudifer\n11.6\n36.7\n\n\nHylonycteris underwoodi\n13.4\n36.7\n\n\nAnoura geoffroyi\n13.8\n39.6\n\n\nLonchophylla robusta\n14.3\n42.6\n\n\nAnoura fistulata\n12.4\n85.2\n\n\nAnoura cultrata\n14.3\n34.3\n\n\nLeptonycteris curosoae\n16.0\n40.2\n\n\nChoeronycteris mexicana\n18.0\n52.1\n\n\n\n\n\n\n\n3.2.2 Creating a data file\nWhen you have new data that you want to get into the computer in a format that R can read, it is often easiest to do this outside of R. A spreadsheet program like Excel (or a freely available program like Google Sheets) is a straightforward way to create a .csv file that R can read. For the lab today, we’ll use Google Sheets.\n\nLog into Google Drive using your userid@hawaii.edu credentials.\nCreate a new Google Sheet\n\n\n\nName the new sheet “BatTongues2”\n\n\n\nEnter data\n\nIn the first row of your new spreadsheet, write your variable names, one for each column. (Be sure to give them good names that will work in R following principles of spreadsheet organized in Broman & Woo (2018). Mainly, don’t have any spaces in a variable name and make sure that it doesn’t start with a number or contain punctuation marks. See Lab 01 for more about naming variables.)\nOn the rows immediately below that first row, enter the data for each individual, in the correct column. Here’s what the spreadsheet would look like for the bat data after they are entered:\n\n\nDownload as a .csv file. Go to “File &gt; Download &gt; Comma-spearate values (.csv, current sheet)”\n\n Save to the Desktop (or elsewhere that is convenient) as “BatTongues2.csv”\n\nSaving a spreadsheet in a format that R can read is very straightforward. In these tutorials, we are using .csv files (which stands for comma separated values).\n\nUpload file to RStudio Cloud\n\nIn your lab03 project, click on the “data” directory under the Files tab:\n\nThen click the Upload option:\n\nYou should see the following dialog box:\n\nClick “Choose File” and navigate to where you saved “BatTongues2.csv” and select it.\n\nClick OK. Then you should see “BatTongues2.csv” in your data directory.\n\nLook at “BatTongues2.csv” in the RStudio Cloud viewer by clicking the file name and selecting “View File”\n\nIt should something like this:"
  },
  {
    "objectID": "lab03.html#questions",
    "href": "lab03.html#questions",
    "title": "3  Workikng with data",
    "section": "3.3 Questions",
    "text": "3.3 Questions\n\nReview: Import “BatTongues2.csv” using the read.csv(), name the data.frame bat_tongues, and make a scatterplot using the ggplot2 package. It should look the same as last week unless you made a typo.\nCalculate the sample variance and standard deviation.\nThe variance and standard deviation are common descriptions of the variability in a population. If we’re plotting the data in a histogram, these are associated with the spread of histogram. By convention the population variance and standard deviations are often denoted \\(\\sigma ^ 2\\) and \\(\\sigma\\), respectively (\\(\\sigma\\), pronounced “sig-ma”, is a lowercase Greek letter). The sample variance and standard deviations are often denoted \\(s^2\\) and \\(s\\), respectively. In both cases, the standard deviation is simply the square root of the variance.\nThe equations for the sample variance and standard deviation are:\n\\[ s ^ 2 = \\frac{\\Sigma^{n}_{i=1} (Y_i - \\bar{Y}) ^ 2}{n - 1} \\] \\[ s = \\sqrt{\\frac{\\Sigma^{n}_{i=1} (Y_i - \\bar{Y}) ^ 2}{n - 1}} \\]\n\nCalculate the sample variance of the palate_length variable. Note that if you named the column something other than palate_length, you will have to use your name. Here’s a couple hints to get started:\n# Create a vector and call it Y\nY &lt;- bat_tongues$palate_length\n\n# Calculate the mean of Y\nY_bar &lt;- mean(Y)\n\n# Calculate the sample size, n\nn &lt;- length(Y)\n\n# Calculate the squared deviations (Y_i - Y_bar) ^ 2\nsquared_devs &lt;- (Y - Y_bar) ^ 2\nOnce you’re done, you can check your answer using the var() function in R\nvar(bat_tongues$palate_length)\nNow calculate the standard deviation and check your result using the sd() function.\n\n\n\nLearning when to use mean versus median. For this exercise, we’ll use a data set on leaf area from Wright et al. 2017.\n\nImport the file “wright_etal_2017.csv” from the data directory using the read.csv() function. Call the data.frame leafsize.\nMake a histogram of leaf size (units of cm\\(^2\\)) using ggplot2. The variable name is leafsize_cm2. It should look something like this: \nDescribe the distribution of leaf size. Do you expect the mean or median to be larger? Check your answer using the mean() and median() functions. Make sure to use the na.rm = TRUE argument to ignore missing data. Which value would you use to describe the location (aka central tendency) of leaf size in this data set?\nNow let’s look at the power of log-transformation. First, use the + scale_x_log10() function in ggplot2 to plot leaf size on a log\\(_{10}\\)-transformed scale. Modify your previous histogram using this function. The output should look something like this: \nNow describe the shape of this distribution. Would you expect the mean or median to be higher? Check your guesses by first creating a new column called log10_leafsize_cm2. Hint: you’ll need to use the $ operator and the log(..., base = 10) or log10(...) functions. Then use mean() and median() on the new column.\nNow, would you still use the same function (mean() or median()) you selected in part c. to describe the location of the log-transformed leaf size? Explain your answer.\n\n\n\nMake a random sample.\nLet’s assume that the data set represents the entire population of leaf sizes in the world (it doesn’t, but let’s just assume). Now we’ll look at the property of random samples from this population.\n\nTo make our lives simpler, let’s first filter out the missing values using the subset() function.\nleafsize1 &lt;- subset(leafsize, !is.na(leafsize_cm2))\nNote how we now have two data frames, leafsize and leafsize1. leafsize1 has all the same columns as the original, but without rows containing missing values. The command !is.na() (pronounced “bang! is-dot-na”) tells subset which values are NOT NA. In R, ! (“bang!”) means NOT.\nWhat is the sample size now? You can use the nrow() function to figure this out. Assign the output of nrow to a variable named n_leaves.\nTo take a random sample, all members of the population must have the same chance of being chosen for our sample. In R, the function sample() can randomly choose integers from a given range. For example, to randomly sample 5 individuals from 25 possibilities, we can use:\nsample(25, size = 5)\nNow randomly sample 5 leaf sizes and assign the output to a vector called i:\ni &lt;- sample(n_leaves, size = 5)\nThe square brackets [ ] in R let you extract portions of a vector. Extract the values of the random leaves you sampled using the square brackets and assign the output as leafsize1_sample; you can run the following code:\nleafsize1_sample &lt;- leafsize1$leafsize_cm2[i]\nUse R to calculate the mean of the leaf sizes for these 5 leaves in your random sample. The result is an estimate of the mean leaf size in your population.\nMake another random sample of 5 leaves, and calculate the mean of this sample. Did you get a different number from the mean of the first sample? Why do you think the second sample mean is different from the first?"
  },
  {
    "objectID": "lab04.html#goals",
    "href": "lab04.html#goals",
    "title": "4  The Sampling Distribution",
    "section": "4.1 Goals",
    "text": "4.1 Goals\n\nUnderstand the sampling distribution of an estimate\nInvestigate sampling error\nCalculate standard error of the mean\nCalculate confidence intervals"
  },
  {
    "objectID": "lab04.html#learning-the-tools",
    "href": "lab04.html#learning-the-tools",
    "title": "4  The Sampling Distribution",
    "section": "4.2 Learning the Tools",
    "text": "4.2 Learning the Tools\n\n4.2.1 Simulating your own sampling distributions\nJust like we did in lecture, we will simulate our own sampling distribution of the mean. For this lab, we will use the iris dataset to demonstrate how. You’ll take many random samples from the iris dataset, calculate the sample mean \\(\\bar{Y}\\) for each, and plot the distribution.\n\n4.2.1.1 Randomly sampling rows\nWe used the replicate function in lecture, but the dplyr package offers tools that are perhaps even more intuitive. You can take a random sample of rows in your data using the dplyr function slice_sample(). For example, to sample 5 rows at random from the iris data set, you would do the following:\n\nlibrary(dplyr)\n\nsamp &lt;- slice_sample(iris, n = 5)\nsamp\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          6.0         3.0          4.8         1.8  virginica\n2          4.9         2.4          3.3         1.0 versicolor\n3          7.2         3.2          6.0         1.8  virginica\n4          6.7         2.5          5.8         1.8  virginica\n5          5.1         2.5          3.0         1.1 versicolor\n\n\nNow try on your to increase the sample size 10, how would you do this?\n\n\n4.2.1.2 Repeated sampling\nTo simulate a sampling distribution, we need to repeatedly randomly sample the “population” (in this case, we’re pretending the iris data set is the entire population). The infer package has a convenient function rep_slice_sample() that will repeat slice_sample() many times. It creates a new column called replicate to index each replicate sample. To randomly sample 5 rows 4 times from Iris setosa, we do:\n\n# first load the infer pacakge\nlibrary(infer)\n\n# let's look at just the species setosa, so we'll need to subset our data\njust_setosa &lt;- subset(iris, Species == \"setosa\")\n\nmany_setosa_samp &lt;- rep_slice_sample(just_setosa, n = 5, reps = 4)\nhead(many_setosa_samp)\n\n# A tibble: 6 × 6\n# Groups:   replicate [2]\n  replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n      &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         1          4.8         3            1.4         0.3 setosa \n2         1          5           3.2          1.2         0.2 setosa \n3         1          5           3.6          1.4         0.2 setosa \n4         1          4.4         3.2          1.3         0.2 setosa \n5         1          5.1         3.8          1.6         0.2 setosa \n6         2          5.1         3.8          1.5         0.3 setosa \n\n\nNote: the infer package converted the iris data.frame to something called a tibble, for our purposes, think of a data.frame and a tibble as equivalent.\nNow, let’s take 1000 samples of 10 from each species. I’ll show you the code for Iris setosa, then fill in the ___ sections below to do it for it the other species. After, you’ll need to combine the results before summarizing and plotting.\n\n# let's use `set.seed` so we can compare answers\nset.seed(123)\n\njust_setosa &lt;- subset(iris, Species == \"setosa\")\nmany_setosa_samp10 &lt;- rep_slice_sample(just_setosa, n = 10, reps = 1000)\n\njust_versicolor &lt;- subset(iris, Species == ___)\nmany_versicolor_samp10 &lt;- rep_slice_sample(___, n = 10, reps = 1000)\n\njust_virginica &lt;- subset(iris, Species == ___)\nmany_virginica_samp10 &lt;- rep_slice_sample(___, n = 10, reps = 1000)\n\nIf you used the same seed (123) in the above code then you should get these same answers:\n\nmany_setosa_samp10\n\n# A tibble: 10,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1         1          4.8         3.1          1.6         0.2 setosa \n 2         1          5.8         4            1.2         0.2 setosa \n 3         1          4.3         3            1.1         0.1 setosa \n 4         1          4.7         3.2          1.3         0.2 setosa \n 5         1          4.5         2.3          1.3         0.3 setosa \n 6         1          4.4         3.2          1.3         0.2 setosa \n 7         1          5.5         3.5          1.3         0.2 setosa \n 8         1          4.6         3.2          1.4         0.2 setosa \n 9         1          4.8         3.4          1.9         0.2 setosa \n10         1          5           3            1.6         0.2 setosa \n# ℹ 9,990 more rows\n\nmany_versicolor_samp10\n\n# A tibble: 10,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1         1          6.6         3            4.4         1.4 versicolor\n 2         1          5.7         3            4.2         1.2 versicolor\n 3         1          5.6         2.9          3.6         1.3 versicolor\n 4         1          5.5         2.6          4.4         1.2 versicolor\n 5         1          6.1         2.9          4.7         1.4 versicolor\n 6         1          4.9         2.4          3.3         1   versicolor\n 7         1          5.6         2.7          4.2         1.3 versicolor\n 8         1          5.5         2.3          4           1.3 versicolor\n 9         1          6.9         3.1          4.9         1.5 versicolor\n10         1          5.1         2.5          3           1.1 versicolor\n# ℹ 9,990 more rows\n\nmany_virginica_samp10\n\n# A tibble: 10,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    \n 1         1          5.9         3            5.1         1.8 virginica\n 2         1          6.3         2.8          5.1         1.5 virginica\n 3         1          7.1         3            5.9         2.1 virginica\n 4         1          6.4         3.1          5.5         1.8 virginica\n 5         1          6.9         3.1          5.1         2.3 virginica\n 6         1          7.2         3            5.8         1.6 virginica\n 7         1          6.3         2.9          5.6         1.8 virginica\n 8         1          6.7         3.3          5.7         2.1 virginica\n 9         1          5.8         2.7          5.1         1.9 virginica\n10         1          6.3         2.7          4.9         1.8 virginica\n# ℹ 9,990 more rows\n\n\nRemember, we only need to use set.seed in situations where we’re trying to compare output from random sampling. You can delete set.seed after you compared your output to mine.\nYou can use the rbind function to combine all three sets of sampling distributions into a single tibble.\n\niris_sample_dists &lt;- rbind(many_setosa_samp10, \n                           many_versicolor_samp10,\n                           many_virginica_samp10)\n\niris_sample_dists\n\n# A tibble: 30,000 × 6\n# Groups:   replicate [1,000]\n   replicate Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n       &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1         1          4.8         3.1          1.6         0.2 setosa \n 2         1          5.8         4            1.2         0.2 setosa \n 3         1          4.3         3            1.1         0.1 setosa \n 4         1          4.7         3.2          1.3         0.2 setosa \n 5         1          4.5         2.3          1.3         0.3 setosa \n 6         1          4.4         3.2          1.3         0.2 setosa \n 7         1          5.5         3.5          1.3         0.2 setosa \n 8         1          4.6         3.2          1.4         0.2 setosa \n 9         1          4.8         3.4          1.9         0.2 setosa \n10         1          5           3            1.6         0.2 setosa \n# ℹ 29,990 more rows\n\n\nNow we have a very large set of samples to examine.\n\n\n\n4.2.2 Sample mean \\(\\bar{Y}\\)\nTo look at the distribution of the sample mean, we first need to calculate the sample mean for all 1000 replicates per species. We will use two helpful functions from dplyr to first group the iris_sample_dists data.frame by species and replicate, and then calculate the mean for each species with the summarize function. Let’s first look at the sampling distribution of the mean of sepal length:\n\nsepal_length_sample_dists &lt;-  group_by(iris_sample_dists, Species, replicate)\nsepal_length_sample_dists &lt;- summarize(sepal_length_sample_dists, \n                                       Y_bar = mean(Sepal.Length))\n\nsepal_length_sample_dists\n\n# A tibble: 3,000 × 3\n# Groups:   Species [3]\n   Species replicate Y_bar\n   &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;\n 1 setosa          1  4.84\n 2 setosa          2  4.93\n 3 setosa          3  4.87\n 4 setosa          4  4.86\n 5 setosa          5  5.04\n 6 setosa          6  5.1 \n 7 setosa          7  5   \n 8 setosa          8  5.05\n 9 setosa          9  4.87\n10 setosa         10  5.05\n# ℹ 2,990 more rows\n\n\nModify the code above to calculate sample means for Petal.Length.\n\n\n4.2.3 Plot the sampling distribution \\(\\bar{Y}\\)\nWe can apply the ggplot() tools we’ve already learned to plot a multiple histogram to compare the sampling distributions in each species.\n\nggplot(sepal_length_sample_dists, aes(Y_bar, fill = Species)) +\n    geom_histogram(alpha = 0.5, position = \"identity\", bins = 30) +\n    scale_fill_viridis_d()\n\n\n\n\nSee if you remember how to use facet_grid() to put each Specie in it’s own panel like this:\n\n\n\n\n\n\n\n4.2.4 Standard error of the mean\nThe standard error of the mean helps us quantify our uncertainty about our estimate of the population mean given our sample size. We can calculate a hypothetical standard error for the perfect random sample of size \\(n\\) by dividing the population standard deviation by \\(\\sqrt{n}\\): \\(\\sigma / \\sqrt{n}\\). Let’s pretend that the iris data set is the entire “population”, the population standard deviation for Sepal.Length is:\n\n\n\n\n\nSpecies\n\\(\\sigma\\)\n\n\n\n\nsetosa\n0.35\n\n\nversicolor\n0.51\n\n\nvirginica\n0.62\n\n\n\n\n\nNow calculate the hypothetical standard error of the mean for a sample size of 10. You should get:\n\n\n\n\n\nSpecies\n\\(\\sigma\\)\n\\(\\sigma/\\sqrt{n}\\)\n\n\n\n\nsetosa\n0.35\n0.1106797\n\n\nversicolor\n0.51\n0.1612762\n\n\nvirginica\n0.62\n0.1960612\n\n\n\n\n\nLet’s compare this hypothetical standard error of the mean to what we obtain from our simulations. Remember that the standard error of the mean is simply the standard deviation of the sampling distribution. That means we can get the answer by using the sd() function on our simulated sampling distribution.\n\nsepal_length_se &lt;- group_by(sepal_length_sample_dists, Species)\nsepal_length_se &lt;- summarize(sepal_length_se, SE_Ybar = sd(Y_bar))\nsepal_length_se\n\n# A tibble: 3 × 2\n  Species    SE_Ybar\n  &lt;fct&gt;        &lt;dbl&gt;\n1 setosa       0.101\n2 versicolor   0.147\n3 virginica    0.182\n\n\nNotice that we had to first group by Species, then summarize by taking the standard deviation of all of our sample means.\nAre the population standard errors of the mean close to what you calculated from the simulations? Are the standard errors what you expected given the multiple histogram figure above?\n\n\n4.2.5 Sample standard error of the mean\nThe sample standard error (\\(\\mathrm{SE}_{\\bar{Y}}\\)) quantifies our uncertainty in our estimate of the population mean, \\(\\bar{Y}\\). Specifically, \\(\\mathrm{SE}_{\\bar{Y}}\\) is the standard deviation of sampling distribution for \\(\\bar{Y}\\). The equation for the \\(\\mathrm{SE}_{\\bar{Y}}\\) is the sample standard deviation divided by the square-root of the sample size:\n\\[ \\mathrm{SE}_{\\bar{Y}} = \\frac{s}{\\sqrt{n}} \\]\nThere’s no function in R to calculate \\(\\mathrm{SE}_{\\bar{Y}}\\), but you know the functions for sample standard deviation and square-root. Use R to calculate the sample standard error of the mean for the following numbers:\n\n2.16 -0.79 -0.18 1.62 -0.98 -1.15 -0.15 1.34 1.96 1.74\n\nYou should get \\(\\mathrm{SE}_{\\bar{Y}}\\) = 0.419.\n\n\n4.2.6 95% confidence intervals\nConfidence intervals are a way to show the plausible range of parameter values given the data. 95% confidence intervals will include the true population parameter 95% of the time. We’ll learn ways to calculate confidence intervals for different parameters throughout the class. Today, we’ll use the “2 SE” rule to approximate 95% confidence intervals for the sample mean \\(\\bar{Y}\\). The lower bound and upper bounds of the approximate 95% confidence interval using the 2 SE rule are:\n\\[ \\text{lower CI}: \\bar{Y} - 2 \\times \\mathrm{SE}_{\\bar{Y}} \\] \\[ \\text{upper CI}: \\bar{Y} + 2 \\times \\mathrm{SE}_{\\bar{Y}} \\] Use the mean() functions and standard error of the mean to calculate the confidence interval for the data you used in the last section. You should get:\n\n\n\n\n\n\\(\\bar{Y}\\)\nLower CI\nUpper CI\n\n\n\n\n0.557\n-0.2813638\n1.395364"
  },
  {
    "objectID": "lab04.html#questions",
    "href": "lab04.html#questions",
    "title": "4  The Sampling Distribution",
    "section": "4.3 Questions",
    "text": "4.3 Questions\nAll questions are about the sampling distribution of the sample mean, \\(\\bar{Y}\\)\n\nImport data\nWe’ll use a dataset about leaf sizes from Wright et al. (2017). We’ll pretend that this is population of all leaf sizes in the world and look at the properties of random samples from the population.\nUse the read.csv(), $, [, and/or dplyr functions to\n\nread-in the dataset\nmake a data.frame with only the latitude and leafsize_cm2 columns\nremove all rows with missing values from leafsize_cm2\nsubset the data to only tropical latitudes between -23.43655° and 23.43655°\nassign this data.frame to the name leafsize\n\nHints:\n\nto get ONE column you can use ...$column_name, to get multiple columns, you can use ...[, c(\"column_name1\", \"column_name2\")]\nremember the select function\nyou can figure out if a value is missing with the is.na function\nlatitudes between -23.43655° and 23.43655° is the same as abs(latitude) &lt; 23.43655\n\nIf you’ve done everything correctly, you should get the same values for the population mean seen below:\nmean(leafsize$leafsize_cm2)\n[1] 65.97642\nCreate 1000 replicates each of sample sizes of 64, 256, and 1024 from the leafize data.frame you generated in a. I’ll show you the code for \\(n = 64\\), then you should copy and modify it to make similar objects called sample_dist256 and sample_dist1024. Then use rbind() to combine them into an object called sample_dists.\n# create replicate samples\nsample_dist64 &lt;- rep_slice_sample(leafsize, n = 64, reps = 1e3)\n\n# add a column recording the sample size\nsample_dist64$sample_size &lt;- 64\n\n# create replicate samples\nsample_dist256 &lt;- ___\n\n# add a column recording the sample size\n___ &lt;- 256\n\n# create replicate samples\nsample_dist1024 &lt;- ___\n\n# add a column recording the sample size\n___ &lt;- 1024\n\n# combine using `rbind`\nsample_dists &lt;- ___\nUse the group_by() and summarize() functions to calculate the sample mean for each level of sample size (64, 256, or 1024) and replicate. Make sure you assign the output a name so you can use it to make a plot in the next part.\nMake a multi-panel histogram with separate panels for each sample size. It should look something like this, but will not be exactly the same because the simulations are random.\n\nHow does the location and width of the sampling distribution for \\(\\bar{Y}\\) change as \\(n\\) increases?"
  },
  {
    "objectID": "lab05.html#goals",
    "href": "lab05.html#goals",
    "title": "5  🎉 Review 🎉 coding, data, graphics",
    "section": "5.1 Goals",
    "text": "5.1 Goals\nBuild more confidence with:\n\nCoding in R, with special attention to writing useful scripts and different types (aka classes) of objects\nManipulating data\nMaking graphics"
  },
  {
    "objectID": "lab05.html#learning-the-tools",
    "href": "lab05.html#learning-the-tools",
    "title": "5  🎉 Review 🎉 coding, data, graphics",
    "section": "5.2 Learning the Tools",
    "text": "5.2 Learning the Tools\n\n5.2.1 Writing useful scripts\nOne key to writing useful scripts is making effective use of comments. Recall: comments are the text that is only for us humans to read. Comments always start with a # symbol, like this:\n\n# this is a comment, I could use it to explain the code below\n# I am making a numeric vector x\n\nx &lt;- c(1, 2, 3)\n\nIn the above code, the computer doesn’t bother trying to understand what you wrote in the comment, it only cares about the code x &lt;- c(1, 2, 3).\nTry doing the opposite:\n\nthis is a comment, I could use it to explain the code below\nI am making a numeric vector x\n\n# x &lt;- c(1, 2, 3)\n\nWhat do you think will happen? Try running it and see.\nImportant We ask you to complete your lab reports by writing a script (saved in the file report.R), running the code in that script, and then pasting the answers back into the script as comments.\nIf a lab report question was\n\n\nMake a vector containing these numeric values and then calculate its median 1.0, 1.2, 2.3, 4.0, 5.1\n\n\nYour answer should look like this:\n\n# 2. Make a vector containing these numeric values and then calculate its median `1.0, 1.2, 2.3, 4.0, 5.1`\n\nx &lt;- c(1.0, 1.2, 2.3, 4.0, 5.1)\n\nmedian(x)\n\n## 2.3\n\nYour answer should not look like this:\n\n# 2. Make a vector containing these numeric values and then calculate its median `1.0, 1.2, 2.3, 4.0, 5.1`\n\n# x &lt;- c(1.0, 1.2, 2.3, 4.0, 5.1)\n# \n# median(x)\n\n2.3\n\nIn the incorrect example, the code is commented out (incorrect because you want to run the code) and the answer is not commented out—incorrect because we explicitly ask you to report the answer as a comment…also imagine if the answer needed to be a sentence, if you reported that answer without making it a comment, R would throw an error.\n\n\n5.2.2 Manipulating data\nLet’s use the data on global reef fish species richness to help understand how to subset data. First read-in the data and have a look:\n\nreef_fish &lt;- read.csv(\"data/global-reef-fish.csv\")\n\nhead(reef_fish)\n\n           site   temp_C spp_richness\n1    abrolhos_1 25.91224           59\n2        aceh_1 28.91176          268\n3        aceh_2 28.93666          141\n4        aceh_4 28.95750          135\n5        aceh_5 28.96406          135\n6 ailuk_atoll_1 27.91439          132\n\n\nThe columns are site, temp, and spp_richness. If I was curious about the mean species richness for the whole dataset, how could I figure that out?\nFirst, I could recall that to access the one column containing species richness I can use the $:\n\nreef_fish$spp_richness\n\nTry that out.\nThen if I want to know the mean, I can just put that above command inside the function mean:\n\nmean(reef_fish$spp_richness)\n\n[1] 121.1167\n\n\nNow suppose I want to know the specific values of temp and spp_richness for one specific site. Imagine I already know the name of the site I’m interested in: hawaii_11. How do I extract the rows associated with that site? I use the subset function:\n\nsubset(reef_fish, reef_fish$site == \"hawaii_11\")\n\n         site   temp_C spp_richness\n157 hawaii_11 24.73345          117\n\n\nThe function subset takes two arguments: the data.frame we want to take a subset of (in this case reef_fish) and a vector of TRUEs and FALSEs that tells R which rows we want. As we covered in lecture, a vector composed of TRUE and FALSE is a logical vector, i.e. it is of class logical.\nLet’s look at how we made a logical vector with the above code reef_fish$site == \"hawaii_11.\nFirst we asked for just the site column using the $ symbol\n\nreef_fish$site\n\nThen we used the double equal sign == to ask R a question: “hey R, which of the values in the site column are equal to \"hawaii_11\"?” And R answered our question by reporting a FALSE for every value in the site column that does not equal \"hawaii_11\" and reporting a TRUE for every value in the site column that does equal hawaii_11. Here’s that full command:\n\nreef_fish$site == \"hawaii_11\"\n\nWe have to put “hawaii_11” in quotes for two reasons:\n\nthe site column is of class character, so to ask R a yes/no question with the == symbol, we need to use the same class\nif we did not put \"hawaii_11\" in quotes, R would think we were trying to refer to an object named hawaii_11; chances are there is no object named hawaii_11, so R would return an error\n\nThere are other types of “yes/no” questions we can ask R. For example, we can ask if a numerical value/vector is greater than or less than something. Here is how we would subset the reef_fish data.frame to look at only the rows with temperature less than or equal to 22C:\n\nsubset(reef_fish, reef_fish$temp_C &lt;= 22)\n\n                  site   temp_C spp_richness\n183 lord_howe_island_1 21.38052          337\n184 lord_howe_island_2 21.30229          119\n234   norfolk_island_1 21.33907           79\n235   norfolk_island_2 21.39242           46\n361   santa_catarina_4 21.95693           46\n\n\nWe used &lt;= for “less than or equal to”. We can use just &lt; for stricktly less than. The same goes for &gt;= and &gt;.\nWe can also combine “yes/no” questions. There are two ways to combine them: with “AND” or with “OR”. Let’s see an “AND” example, we use the symbol & for “AND”:\n\nsubset(reef_fish, reef_fish$temp_C &lt;= 22 & reef_fish$temp_C &gt; 21.5)\n\n                site   temp_C spp_richness\n361 santa_catarina_4 21.95693           46\n\n\nWe are asking R to make a subset for cases where the temperature is \\(\\le 22\\) AND \\(&lt; 21.5\\). There is only one row that meets those criteria.\nR might not be able to find any cases that match our criteria, for example if we ask for temperature less than 21.5 AND site equal to \"santa_catarina_4\":\n\nsubset(reef_fish, reef_fish$temp_C &lt; 21.5 & \n         reef_fish$site == \"santa_catarina_4\")\n\n[1] site         temp_C       spp_richness\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nWe get 0 rows back.\nLet’s contrast that with what we get replacing AND with OR. The symbol for “OR” is |:\n\nsubset(reef_fish, reef_fish$temp_C &lt; 21.5 | \n         reef_fish$site == \"santa_catarina_4\")\n\n                  site   temp_C spp_richness\n183 lord_howe_island_1 21.38052          337\n184 lord_howe_island_2 21.30229          119\n234   norfolk_island_1 21.33907           79\n235   norfolk_island_2 21.39242           46\n361   santa_catarina_4 21.95693           46\n\n\nHere we get all the rows that meet either the criterion that temperature is less than 21.5 or the criterion that site is equal to \"santa_catarina_4\".\nThere is one last super useful “yes/no” question having to do with missing data. The reef_fish data do not have missing values so let’s look at the Palmer penguins data provided by the palmerpenguins package\n\nlibrary(palmerpenguins)\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nAlready we can see some missing data in the form of NA values. If we would like to remove rows that have an NA in a certain column, we can use the function is.na to help us do that.\nThe function is.na looks different from the other “yes/no” questions like ==, but it serves the same purpose, it lets us ask a “yes/no” question, is this case, “are the values NA or not?”\nSo if we want to remove rows for which bill_length_mm is NA, we would do this:\n\nsubset(penguins, !is.na(penguins$bill_length_mm))\n\n# A tibble: 342 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           34.1          18.1               193        3475\n 9 Adelie  Torgersen           42            20.2               190        4250\n10 Adelie  Torgersen           37.8          17.1               186        3300\n# ℹ 332 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNotice the ! in that code. The ! symbol (pronounced “bang!”) changes a TRUE to a FALSE and a FALSE to a TRUE. It reverses the answer. So we asked is.na which tells us TRUE when the value is NA. But we want the opposite, we want to know which values are not NA, so we put “bang” in front: !is.na(penguins$bill_length_mm).\n\n\n5.2.3 Making graphs\nLet’s keep using the penguins data to make some plots. Let’s look at how the different species compare in terms of their body mass. Here our explanatory variable is penguin species, and the response variable is body mass. So we have…\n\n5.2.3.1 Categorical explanatory variable, numerical response variable\nFor this type of situation we often want a boxplot:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n    geom_boxplot()\n\n\n\n\nCool! This is a perfectly good exploratory plot. But how could we improve it if we wanted it to be more of a finished product? The y-axis label could be nicer looking:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n    geom_boxplot() +\n    ylab(\"body mass (g)\")\n\n\n\n\nAnd finally, we might debate about whether we need the gridlines in the background (e.g. if they are over-plotting), let’s try adding a different theme to remove those:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n    geom_boxplot() +\n    ylab(\"body mass (g)\") + \n    theme_classic()\n\n\n\n\nSome will prefer the grid, others will not. Make your choice based on what you want the viewers of your graphic to understand about your data.\nWe might also be curious about if the different species have different sizes on the different islands. First let’s use subset to look at one species (Adelie) across the islands:\n\nggplot(subset(penguins, penguins$species == \"Adelie\"), \n       aes(x = island, y = body_mass_g)) +\n    geom_boxplot() +\n    ylab(\"body mass (g)\") + \n    theme_classic()\n\n\n\n\nPretty similar. But what about the other species? We can look at all species at once by using facetting\n\nggplot(penguins, aes(x = island, y = body_mass_g)) +\n    geom_boxplot() +\n    facet_grid(rows = vars(species)) + \n    ylab(\"body mass (g)\") + \n    theme_classic()\n\n\n\n\nAhhh interesting, Chinstrap and Gentoo are not found on all the islands like Adalie is.\nLet’s try a different kind of plot, what if we want to look in more detail at how the body size data are distributed in only one species, that brings us to…\n\n\n5.2.3.2 Numerical response variable, no explanatory variable\nFor this situation a histogram is the best graphic. Let’s look at just the Gentoo penguin (again, we use subset to achieve this)\n\nggplot(subset(penguins, penguins$species == \"Gentoo\"), \n       aes(x = body_mass_g)) +\n    geom_histogram()\n\n\n\n\nLooks like a nice symetric distribution without any major outliers. Here’s a case where I would say the grid lines in the background are definitely not neccesary, and thus overplotting:\n\nggplot(subset(penguins, penguins$species == \"Gentoo\"), \n       aes(x = body_mass_g)) +\n    geom_histogram() +\n    theme_classic()\n\n\n\n\nNow let’s look at how body mass and bill length are related, this brings us to…\n\n\n5.2.3.3 Numerical reponse and numerical explanatory variable\nFor this situation a scatter plot is best:\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) +\n    geom_point()\n\n\n\n\nThe bigger the penguin, the bigger the bill. Makes sense! But do all the penguins have the same exact relationship? Let’s use different colors for the different species to find out:\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm, \n                     color = species)) +\n    geom_point() \n\n\n\n\nThe legend might make it hard to see the actual data! Let’s move the legend to a better place\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm, \n                     color = species)) +\n    geom_point() + \n    theme(legend.position = \"top\", \n          legend.title = element_blank())\n\n\n\n\nWe also used legend.title = element_blank() to remove the title of the legend.\nNow we can see that Adelie and Gentoo follow the same kind of patter, but Chinstrap penguins have comparatively longer bills for their mass. But for folks with red-green color blindness, this would not be clear. Let’s choose different colors\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm, \n                     color = species)) +\n    geom_point() + \n    theme(legend.position = \"top\", \n          legend.title = element_blank()) +\n    scale_color_viridis_d()"
  },
  {
    "objectID": "lab05.html#questions",
    "href": "lab05.html#questions",
    "title": "5  🎉 Review 🎉 coding, data, graphics",
    "section": "5.3 Questions",
    "text": "5.3 Questions\n\nFix the snippet of a script below:\n\n\nFirst I read-in my data\n\n# read.csv(\"data/BatTongues.csv\")\n\n# then I plot a histogram of tongue lengths\nggplot(bats, aes(x = tongue_length)) +\n    \"geom_boxplot()\" +\n    xlab(Tongue length (mm))\n\n\nUse the reef_fish to do the following:\n\ncopy the line of code reef_fish &lt;- read.csv(\"data/global-reef-fish.csv\") over to your report.R script\nuse subset and $ to calculate the mean species richness across sites with a temperature greater than or equal to 25 C\nuse subset and $ to calculate the mean species richness across sites with a temperature less than 25 C\n\nFix this dataset using the principles we’ve discussed in lecture and lab. Your final dataset should have columns called site, date_visited, impact, species, length_cm. Hint: pay special attention to all the inconsistent ways missing data have been recorded.\nIt should be noted that these are made up data (made up by Andy). They come from hypothetical examples we’ve been talking about in class including our three favorite fish species uhu, lauʻipala, and pākuʻikuʻi. And they come from a hypothetical study we have talked about looking at how the level of human impact can affect ecosystems. These data imagine the impact of humans on the body sizes of these three fish species.\nReport your answer to this problem by copying the google sheet, fixing it, and pasting a URL link to the corrected google sheet in the report.R script. Make sure to report the URL as a comment, and make sure to allow sharing (via google sheets) to anyone with the link\nSave the dataset you just cleaned-up as a .csv file and upload it to Posit Cloud so we can work with it for the following questions.\nRemember these are made up data!\n\nRead in the cleaned-up data and use subset to make a new data.frame that has no missing species names\nThe column for impact represents an ordinal variable, but right now R doesn’t know that; write code to change the impact column into a properly ordered ordinal variable column\nMake a plot showing how the sizes of the three different species change across the levels of human impact. Hint: you will need to use colors and facets\nin a few sentences, describe what you conclude about the effect of human impact on fish body size…in these made up data"
  },
  {
    "objectID": "lab06.html#goals",
    "href": "lab06.html#goals",
    "title": "6  🎉 Review 🎉 describing data, standard error, confidence intervals",
    "section": "6.1 Goals",
    "text": "6.1 Goals\nBuild more confidence with:\n\nCalculating descriptions of the location and spread of data\nCalculate standard error of the mean\nCalculate confidence intervals"
  },
  {
    "objectID": "lab06.html#learning-the-tools",
    "href": "lab06.html#learning-the-tools",
    "title": "6  🎉 Review 🎉 describing data, standard error, confidence intervals",
    "section": "6.2 Learning the Tools",
    "text": "6.2 Learning the Tools\n\n6.2.1 Describing the location of data\nFor location we use either the mean or the median. The mean is ideal for data that are symmetrically distributed, the median is more ideal for skewed data or data with outliers because it will be less sensitive to extreme values.\nHere are some examples of simulated datasets with different distributions, organized by which description of location (mean or median) would work best:\n\n\nUse the mean\n\n\n\n\n\n\nUse the median\n\n\n\n\n\n\n\nTry calculating the mean and median for the following vectors:\n\n3.68, 3.51, 4.78, 5.90, 6.67, 6.51, 2.31, 5.72, 5.84, 3.61, 4.78, 4.73, 4.02, 5.28, 5.66, 5.77, 6.86, 5.62, 6.86, 5.86\n\nHere’s how we could do that in R\n\n# copy the numbers in and assign them to an object\ny1 &lt;- c(3.68, 3.51, 4.78, 5.90, 6.67, 6.51, 2.31, \n        5.72, 5.84, 3.61, 4.78, 4.73, 4.02, 5.28, \n        5.66, 5.77, 6.86, 5.62, 6.86, 5.86)\n\nmean(y1)\n\n[1] 5.1985\n\nmedian(y1)\n\n[1] 5.64\n\n\nThe mean and median are relatively close to each other\nNow let’s try the mean and median for these numbers:\n\n0.42, 3.46, 0.82, 1.36, 1.34, 1.61, 1.18, 0.73, 0.63, 1.40, 1.54, 1.64, 0.87, 6.65, 0.28, 7.05, 6.67, 2.45, 4.06, 5.68\n\nAgain in R:\nHere’s how we could do that in R\n\n# copy the numbers in and assign them to an object\ny2 &lt;- c(0.42, 3.46, 0.82, 1.36, 1.34, 1.61, 1.18, \n        0.73, 0.63, 1.40, 1.54, 1.64, 0.87, 6.65, \n        0.28, 7.05, 6.67, 2.45, 4.06, 5.68)\n\ny2 &lt;- round(rexp(20, 0.5), 2)\n\nmean(y2)\n\n[1] 2.2125\n\nmedian(y2)\n\n[1] 1.78\n\n\nThe values are more distant. Let’s figure out why by making a histogram to look at the shape of the distribution of these data:\n\n# first make a data.frame out of each of y1 and y2 \ny1df &lt;- data.frame(group = \"y1\", y = y1)\ny2df &lt;- data.frame(group = \"y2\", y = y2)\n\n# now combine the two data.frames so we can plot\nall_y_df &lt;- rbind(y1df, y2df)\n\nggplot(all_y_df, aes(x = y)) +\n    geom_histogram() +\n    facet_grid(rows = vars(group))\n\n\n\n\nFor y1 the data are more symmetric, so the mean and median are closer to the same value. For y2 the data are not symmetric at all, so the mean and median are more different. In the case of y2, the outliers are to the right, so the mean gets pulled to the right and is bigger than the median. In the case of y1, even though the data are closer to symmetric, there is still some outlier-ish points to the left so the mean gets pulled to the left and is smaller than the median\nRemember that the median is the middle value of the data. We can calculate this by hand like this:\n\n# median by hand for y1\n\n# first we need to sort y1 from smallest to biggest\ny1 &lt;- sort(y1)\n\n# the length of y1 is 20, which is even, so the median is the average\n# of the middle two values\n\n# these are the middle two values\ny1[c(10, 11)]\n\n[1] 5.62 5.66\n\n# here's their average\nmean(y1[c(10, 11)])\n\n[1] 5.64\n\n# confirm we get the same answer\nmedian(y1)\n\n[1] 5.64\n\n\n\n\n6.2.2 Describing the spread of data\nWe have learned two types of calculations to describe the spread of data: 1. the standard deviation (and variance); and 2. the interquartile range.\nWhenever we would use the mean to describe the location, we can also feel confident using the standard deviation to describe the spread. Whenever we would use the median to describe the location, we should use the interquartile range for the spread.\n\n\nUse the standard deviation\n\n\n\n\n\n\nUse the interquartile range\n\n\n\n\n\n\n\nLet’s use our made up y1 and y2 data to calculate all the metrics of spread.\nWe start with variance and standard deviation, which are appropriate for y1. Let’s look at the equation:\n\\[\ns^2 = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\bar{y})^2\n\\]\nNow translate that to code:\n\ny1_sumsqdiff &lt;- sum((y1 - mean(y1))^2)\ny1_s2 &lt;- 1 / (length(y1) - 1) * y1_sumsqdiff\ny1_s2\n\n[1] 1.564519\n\n# confirm the by-hand calculation with the built-in function\nvar(y1)\n\n[1] 1.564519\n\n\nNow for standard deviation:\n\\[\ns = \\sqrt(s^2)\n\\]\nAnd translating that into code:\n\ny1_s &lt;- sqrt(y1_s2)\ny1_s\n\n[1] 1.250807\n\n# confirm with the built-in function\nsd(y1)\n\n[1] 1.250807\n\n\nNow let’s look at the interquartile range for y2. The goal of the interquartile range is to find the middle region that contains 50% of the data. That means 25% of the data (or one quarter of the data) will be smaller, and 25% of the data will be larger. Here are the steps:\n\n# first we need to sort the data from smallest to largest\ny2sorted &lt;- sort(y2)\ny2sorted\n\n [1] 0.09 0.16 0.37 0.55 0.61 0.95 1.03 1.11 1.46 1.75 1.81 2.03 2.03 2.43 2.60\n[16] 2.98 3.73 4.58 4.88 9.10\n\n# now we figure out how many data points we're dealing with\nlength(y2sorted)\n\n[1] 20\n\n# now we find the \"quarters\"\nlength(y2sorted) / 4\n\n[1] 5\n\n# because this is an even number we need to average the 5th and 6th values\n# to find the lower bound of the interquartile region\nq1 &lt;- mean(y2sorted[c(5, 6)])\n\n# now we do the same for finding the upper bound\nq3 &lt;- mean(y2sorted[c(15, 16)])\n\n# now put them together\niqr &lt;- c(q1, q3)\niqr\n\n[1] 0.78 2.79\n\n# to calculate a single value describing the spread we can just calcualte\n# the difference between those two values\nq3 - q1\n\n[1] 2.01\n\n# or...\ndiff(iqr)\n\n[1] 2.01\n\n\nNotice that I called these values q1 and q3 that is because these are the first and third quartiles. The first quartile has 25% of the data below it, the third quartile has 25% of the data above it. Put another way, the third quartile has 75% of the data below it!\nThat description can help us use a built-in function to verify our by-hand calculation. We use the quantile function with probs equal to 0.25 and 0.75 (for 1st and 3rd quartiles):\n\nquantile(y2, type = 2, probs = c(0.25, 0.75))\n\n 25%  75% \n0.78 2.79 \n\n\nWe have to specify type = 2 so that R takes the average of adjacent values when the sample size is even.\nNote: if we were trying to compare y1 and y2 we would need to use the SAME metrics, we can’t compare the mean of one to the median of another, or the standard deviation of one to the interquartile range of another. The median and interquartile range work for any data, so in this case we should use the median and interquartile range for both y1 and y2 if we wanted to compare them.\n\n\n6.2.3 Describing our confidence in estimates\nWe calculated the mean of y1 before:\n\ny1bar &lt;- mean(y1)\ny1bar\n\n[1] 5.1985\n\n\nHow precise do we think this estiamte is? This is the job for the standrad error and confidence interval.\nThe standard error is defined as\n\\[\nSE = \\frac{s}{\\sqrt n}\n\\]\nLet’s turn that into code:\n\ny1_se &lt;- y1_s / sqrt(length(y1))\ny1_se\n\n[1] 0.279689\n\n\nThe 95% confidence interval is very close to the mean \\(\\pm 2 \\times SE\\). We can calculate that with code as well:\n\ny1_ci95 &lt;- c(y1bar - 2 * y1_se, y1bar + 2 * y1_se)\ny1_ci95\n\n[1] 4.639122 5.757878\n\n\nIn biology, the 95% confidence interval is most commonly used. In other fields different percentages (such as 90% or 99%) are also used.\nThe interpretation of the 95% confidence interval is that 95% of the middle of the sampling distribution is contained within the upper and lower bounds of the 95% confidence interval. Have a look at the interactive website from our book for more explanation: https://www.zoology.ubc.ca/~whitlock/Kingfisher/CIMean.htm"
  },
  {
    "objectID": "lab06.html#questions",
    "href": "lab06.html#questions",
    "title": "6  🎉 Review 🎉 describing data, standard error, confidence intervals",
    "section": "6.3 Questions",
    "text": "6.3 Questions\n\nFix the snippet of a script below:\n\n\n# calculate the mean and median of flipper length of \n# chinstrap penguins by hand\n\nlibrary(palmerpenguins)\n\nchinstrap &lt;- subset(penguins, \n                    !is.na(penguins$species) & \n                        !is.na(penguins$flipper_length_mm))\n\n# the data we'll use\ny &lt;- chinstrap$flipper_length_mm\n\n# number of observations\nn &lt;- nrow(y)\n\n# sum of values\nysum &lt;- sum(y)\n\n# mean\nybar &lt;- mean(ysum)\n\n\n# median, n = 68 so middle value is at the index 34\nysorted &lt;- sort(y)\n\nymedian &lt;- ysorted[34]\n\n# compare\nybar\nymedian\n\n\nMake a graph (which kind of graph should you use??) to help evaluate whether you should use the mean or median to describe the location of the chinstrap penguin flipper length data. Pick which metric you should use (mean or median) and justify your choice using both the ouput of the corrected code from (1.) and from the graph of the data\n\n\nWrite the code to calculate (by hand) the variance and standard deviation of the chinstrap flipper length data. Use var and sd to check your by-hand calculations\n\n\nMake a boxplot of the chinstrap flipper length data. Then calculate the values of the first and third quartiles. Can you locate the first and third quartiles on the boxplot?\n\n\nCalculate the standard error of the mean of chinstrap flipper length. From the SE, calculate the 95% confidence interval of the mean using the 2 * SE rule\n\n\nSuppose that instead of the 500 data points we have for chinstraps we actually had 250 data points. Write some code to calculate how we might expect the SE and 95% CI to change. Hint: in this thought experiment, what value in the calculation of SE can you assume stays approximately constant, what value must change? Now do the same kind of calculation, but for the hypothetical case where we have twice as much data (so n = 1000). How do the smaller and larger sample size data sets compare the data we actually have?"
  },
  {
    "objectID": "lab07.html#goals",
    "href": "lab07.html#goals",
    "title": "7  Probability",
    "section": "7.1 Goals",
    "text": "7.1 Goals\n\nExchange contact info with your groups\nBe able to estimate probabilities from data\nUse sample to simulate events with different probabilities\nUse rules of probability to evaluate if different events in a dataset are likely to be independent or not"
  },
  {
    "objectID": "lab07.html#exchange-contact-info",
    "href": "lab07.html#exchange-contact-info",
    "title": "7  Probability",
    "section": "7.2 Exchange contact info!",
    "text": "7.2 Exchange contact info!\nBefore starting the main content of this lab, take a few minutes to introduce yourselves to your group members and exchange email addresses so you can communicate about your group projects."
  },
  {
    "objectID": "lab07.html#learning-the-tools",
    "href": "lab07.html#learning-the-tools",
    "title": "7  Probability",
    "section": "7.3 Learning the Tools",
    "text": "7.3 Learning the Tools\n\n7.3.1 More fun with penguins!\nWe’ll use our old friend the Palmer penguins data set. Let’s first look at some simple probabilities:\n\nlibrary(palmerpenguins)\n\n# probability species is \"Adelie\"\nPr_adelie &lt;- sum(penguins$species == \"Adelie\") / nrow(penguins)\nPr_adelie\n\n[1] 0.4418605\n\n\nSo there is a 0.44 probability that if we grabbed a penguin out of this dataset it would belong to the Adelie species. The random trial here is “grabbing” a penguin, and the event of interest is that its species is Adelie.\nNotice how we’re calculating the probability. First we use penguins$species == \"Adelie\" to ask R to tell us TRUE ever time it finds the species is Adelie and FALSE ever time it finds the spcies to not be Adelie.\nThen we wrap those TRUE and FALSE values in sum:\n\nsum(penguins$species == \"Adelie\")\n\n[1] 152\n\n\nThat sum tells us the total number of TRUE values, aka the total number of penguins of species Adelie in the data.\nFinally we divide by nrow(penguins) to turn the count into a probability. That’s the definition of probability: the proportion of times an event is true.\nWe can also look at probabilities of numerical data, like the bill length\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- sum(penguins$bill_length_mm &lt; 45) / nrow(penguins)\nPr_bill_l_less45\n\n[1] NA\n\n\nWe got NA?! The reason is that the bill length column has some missing data. We’ve dealt with that before by using subset to remove rows with missing data. Here we will learn a new approach: telling sum to ignore NA values:\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- sum(penguins$bill_length_mm &lt; 45, na.rm = TRUE) /\n    nrow(penguins)\nPr_bill_l_less45\n\n[1] 0.5116279\n\n\nSo there is a 0.51 probability of randomly grabbing a penguin from these data and its bill being less than 45 mm long.\nInterestingly, taking the sum of something and then dividing by sample size is also the definition of the mean, so we can take a shortcut and just use the mean function:\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- mean(penguins$bill_length_mm &lt; 45, na.rm = TRUE)\nPr_bill_l_less45\n\n[1] 0.5146199\n\n\nSame answer.\nNow let’s calculate some joint probabilities. What’s the probability that the bill length is less than 45 AND the species is Adelie? To save us some typing, let’s call this probability pAl45\n\npAl45 &lt;- mean(penguins$bill_length_mm &lt; 45 &\n                  penguins$species == \"Adelie\", \n              na.rm = TRUE)\npAl45\n\n[1] 0.4314869\n\n\nInteresting, \\(Pr(\\text{bill length} &lt; 45)\\) and \\(Pr(\\text{bill length} &lt; 45 \\text{ \\& Adelie})\\) are about the same, could that mean that bill length and species are independent? To figure out, we need to calculate \\(Pr(\\text{bill length} &lt; 45)\\) and \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) and see if the two probabilities are roughly the same.\nThe expression \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) tells us that we already know the species is Adelie. That means to caculate the probability bill length &lt; 45 GIVEN Adelie, we need to ignore all the rest of the data\n\n# subest to just Adelie\njustAdelie &lt;- subset(penguins, penguins$species == \"Adelie\")\n\n# now the conditional probability, is calculated just by counting up\n# cases where bill length &lt; 45\npl45GivenA &lt;- mean(justAdelie$bill_length_mm &lt; 45, na.rm = TRUE)\npl45GivenA\n\n[1] 0.9801325\n\n\nRecall that \\(Pr(\\text{bill length} &lt; 45)\\) = 0.51 which is vastly different from \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) = 0.98 so we conclude that no, bill length and species are not independent.\nWe can confirm this by visualizing the histograms: different penguin species have different distributions of bill lengths, again confirming that the two variables are not indipendent.\n\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm, fill = species)) +\n    geom_histogram() + \n    facet_grid(rows = vars(species)) +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d()"
  },
  {
    "objectID": "lab07.html#simulating-random-events-with-sample",
    "href": "lab07.html#simulating-random-events-with-sample",
    "title": "7  Probability",
    "section": "7.4 Simulating random events with sample",
    "text": "7.4 Simulating random events with sample\nYou’ve seen us use the sample function all the time. We can take a random sample of integers:\n\n# randomly sample 3 numbers between 1 and 10\nsample(1:10, 3)\n\n[1] 7 1 5\n\n\nWe can also sample characters:\n\n# sample 5 letters from A, B, C with replacement\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE)\n\n[1] \"C\" \"C\" \"A\" \"A\" \"C\"\n\n\nTry running the above code without setting replace = TRUE\n\nsample(c(\"A\", \"B\", \"C\"), 5)\n\nYou’ll get an error like this:\n\n\nError in sample.int(length(x), size, replace, prob) : \n  cannot take a sample larger than the population when 'replace = FALSE'\n\n\nThat’s because we’re trying to sample 5 random events from only 3 possible outcomes. We have to “replace” each outcome once we sampled it.\nWe can also sample with replacement when our number of random draws is less than the total number of outcomes:\n\n# randomly sample 3 numbers between 1 and 10\nsample(1:10, 3, replace = TRUE)\n\n[1]  3 10 10\n\n\nWe can also change the frequency of different outcomes by changing the probability that R assigns to them!\n\n# randomly sample 3 numbers between 1 and 10\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE, prob = c(0.8, 0.1, 0.1))\n\n[1] \"A\" \"C\" \"C\" \"A\" \"A\"\n\n\nThe above code will yield 80% “A”, and 10% of both “B” and “C” if we run it over and over.\nThe probabilities need to sum to 1 across all the outcomes because that’s one of the rules of probability. R will automatically do that for you, even if you don’t re-scale the probabilities yourself. For example, this code is equivalent:\n\n# randomly sample 3 numbers between 1 and 10\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE, prob = c(8, 1, 1))\n\n[1] \"A\" \"A\" \"A\" \"A\" \"A\"\n\n\nThe exact events will be different because each time you run it is a random trial."
  },
  {
    "objectID": "lab07.html#questions",
    "href": "lab07.html#questions",
    "title": "7  Probability",
    "section": "7.5 Questions",
    "text": "7.5 Questions\nIn questions 1–4 you will simulate data using the sample function and use the resulting data.frame to evaluate probabilities\n\nUse sample to make a data.frame with 40 rows and 2 columns: x and y. Column x should be filled with a random sample of the the integers 1 and 2, each with equal probability; column y should be filled with a random sample of the letters A and B, each with equal probability.\nThe data.frame you made in (1.) should have two independent columns (i.e. you were not instructed to use any code that would cause the events in column y to depend on the events in column x). To confirm their independence calculate \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) and evaluate if they are close to the same value. If at first they are not, try re-running the code a few times (each time will be different, you’re making random data!). In most of the times you re-run the code, the difference between \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) should be between -0.1 and 0.1\nGiven that we simulate y and x as independent, why do we find that \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) and not exactly equal? If we simulated a data.frame with 80 rows, would you expect \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) to be more closely equal or more different? Why?\nDiscuss in 2 to 3 sentence how you could change the code in (2.) to simulate non-independence between the events in column x and column y. You do not need to implement this code, just discuss\n\nQuestions 5–7 will revisit the visualization of probabilities as overlapping circles that we used in class. NOTE: this is the same concept as what we covered in class, but the actual probabilities will be different\n\nFill in the probability figure. Use the made up data below to calculate the probabilities of all the events in the figure (e.g. what is \\(Pr(\\text{length} &gt; 40)\\)? What about \\(Pr(\\text{length} \\leq 40 \\text{ \\& species} \\neq \\text{uhu})\\)?)\n\n\n\n\n\n\n\n\n\n\nBelow, we have highlighted just two parts of the probability figure, use the above made up data to calculate the probabilities in this figure.\n\n\n\nUsing conditional probabilities (e.g. \\(Pr(A | B)\\)), would you say that the event of \\(length &gt; 40\\) and \\(species = uhu\\) are independent or not?"
  },
  {
    "objectID": "lab08.html#goals",
    "href": "lab08.html#goals",
    "title": "8  Statistical Null Hypothesis Testing",
    "section": "8.1 Goals",
    "text": "8.1 Goals\n\nDiscuss ideas for questions and hypotheses with group members\nReview steps of null hypothesis statistical testing\nGenerate null distributions through repeated random sampling\nTest hypotheses with null distributions"
  },
  {
    "objectID": "lab08.html#discuss-ideas-for-questions-and-hypotheses-with-group-members",
    "href": "lab08.html#discuss-ideas-for-questions-and-hypotheses-with-group-members",
    "title": "8  Statistical Null Hypothesis Testing",
    "section": "8.2 Discuss ideas for questions and hypotheses with group members",
    "text": "8.2 Discuss ideas for questions and hypotheses with group members\nBefore starting the main content of this lab, get together with members of your group for the final project. Make sure everybody agrees on which dataset you will use. Discuss some ideas for questions and hypotheses. Your gruop’s write-up of your questions and hypotheses will be due tomorrow! Consult with your TA about any questions you might have. Your TA can either answer those, or pass them along to Andy.\nThis discussion of group projects should be allowed around 20 minutes."
  },
  {
    "objectID": "lab08.html#learning-the-tools",
    "href": "lab08.html#learning-the-tools",
    "title": "8  Statistical Null Hypothesis Testing",
    "section": "8.3 Learning the Tools",
    "text": "8.3 Learning the Tools\n\n8.3.1 Hypothesis testing\nThe remainder of this course works within the null hypothesis statistical testing (NHST) framework that we will introduce this week. As we will discover, there are six core steps in NHST:\n\nState \\(H_0\\) and \\(H_A\\)\nCalculate the test statistic\nGenerate the null distribution\nCalculate the \\(p\\)-value\nDecide:\n\nReject \\(H_0\\) if \\(p \\le \\alpha\\)\nFail to reject \\(H_0\\) if \\(p &gt; \\alpha\\)\n\n\nFor the statistical tests that we encounter during this course, the equations to calculate appropriate test statistics, null distribution of the test statistic, and \\(p\\)-value are well understood. However, the concepts of null hypothesis testing can often be made more obvious by making a computer simulate the null distribution for us. Such “computationally intensive” null distributions also have important applications beyond learning the concepts—there are many real-world situations where no mathematical equation has ever been derived to calculate the necessary test statistic or null distribution for specific data or null hypotheses. In these cases, the only workable solution is to use a computer to simulate a null distribution.\nLet’s have a look at some data and a hypothesis where a “computational null distribution” will both be informative for learning, and also necessary because no mathematical equation exists to define an appropriate null distribution.\n\n\n8.3.2 Coral reef fish across Polynesia\nThe Hawaiian Islands and Rapa Nui are unique in the world for their geographic isolation. We might hypothesize that this isolation makes it difficult for organisms to disperse to these islands. Even fish (turns out they swim) might have a difficult time getting there. As such, we might hypothesize that compared to the rest of Polynesia, Hawaiʻi and Rapa Nui might have fewer species of coral reef-associated fishes. Let’s test that out. We have a dataset from Barneche et. al (2019) that compiles surveys of reef fish from across the globe. The Barneche et. al data report total numbers of species found at different sites. We’ll look at just a subset of those sites to test our scientific hypothesis about species richness of reef fish in Polynesia.\nLet’s go through the steps of null hypothesis testing:\n\n\n1. State \\(H_0\\) and \\(H_A\\)\n\n\\(H_0\\): Between the two groups “Distant Polynesia” (i.e. Hawaiʻi and Rapa Nui) and “Core Polynesia”, there will be no difference in mean species richness\n\\(H_A\\): There will be a difference in mean species richness between “Distant Polynesia” and “Core Polynesia”\n\n\n\n2. Calculate the test statistic\nWhat is the test statistic? The null hypothesis says there will be no difference in the mean richness. So our test statistic will be (mean species richness of Core Polynesia) - (mean species richness of Distant Polynesia).\nTo calculate that, we need to read-in the data,\n\nreef_fish &lt;- read.csv(\"data/reef_fish.csv\")\n\n\n# have a look at the data\nView(reef_fish)\n\nWe can see there are columns for region, polynesia_isolation, site, lon (longitude), lat (latitude), and richness (species richness). The polynesia_isolation column was added special for this lab. It has values core_polynisia, distant_polynisia, and NA. NA is for everything all sites outside of Polynesia. Before we calculate our test statistic, we need to subset our data to just Polynesia. We can do that like this:\n\n# \"pol_tri\" for polynesian triangle \npol_tri_fish &lt;- subset(reef_fish, !is.na(reef_fish$polynesia_isolation))\n\n\n# have a look\nView(pol_tri_fish)\n\nNow we will use our friends group_by and summarize from dplyr to help us calculate the test statistic.\n\ngroups &lt;- group_by(pol_tri_fish, polynesia_isolation)\ngroup_means &lt;- summarize(groups, ybar = mean(richness))\n\n# have a look (this is a small data frame, so we donʻt need\n# to use the `View` function)\ngroup_means\n\n# A tibble: 2 × 2\n  polynesia_isolation  ybar\n  &lt;chr&gt;               &lt;dbl&gt;\n1 core_polynesia      105. \n2 distant_polynesia    82.5\n\n\nNow from group_means we can calculate the test statistic of the difference in the means\n\ntest_stat &lt;- diff(group_means$ybar)\ntest_stat\n\n[1] -22.5\n\n\nNote: test_stat is the mean of core_polynesia minus the mean of distant_polynesia. The fact that test_stat is negative means that on average there is higher fish species richness in core_polynesia, which aligns with out scientific hypothesis, but will we reject the statistical null or not? For that we need to…\n\n\n3. Generate the null distribution\nRemember a null distribution seeks to capture what our test statistic would look like if the null hypothesis were true. If the null hypothesis were true, then it shouldn’t matter for reef fish richness if we did a survey in Core Polynesia or in Distant Polynesia. That means we can simulate the null distribution by repeatedly shuffling the values of the column polynesia_isolation and then following the same steps for calculating calculating the test statistic. Let’s look at how we would shuffle polynesia_isolation and calculate the test statistic.\n\n# make a new copy of the data for purposes of making\n# the null distribution\npol_fish_null &lt;- pol_tri_fish\n\nNow we can re-make the polynesia_isolation column as a random shuffle of itself. First, have a look at the behavior of the sample function\n\n# make a simple vector of letters\nx &lt;- c(\"A\", \"A\", \"B\", \"B\")\nx\n\n[1] \"A\" \"A\" \"B\" \"B\"\n\n\n\nx &lt;- sample(x)\nx\n\n[1] \"A\" \"B\" \"B\" \"A\"\n\n\nWe started with x being a orderly sequence A A B B and then used sample to generate a random sequence that happens to be A B B A. If we ran sample(x) again, we’d likely get a different random reshuffling of the letters.\nNow let’s use that approach to make our null distribution. For the null distribution we need to make many many many random re-shufflings. But first we’ll look at how we do it just once:\n\n# remember we already made `pol_fish_null` as a copy\n# of the real data\n\n# reshuffle the group identities\npol_fish_null$polynesia_isolation &lt;- sample(pol_fish_null$polynesia_isolation)\n\n# follow the same steps as before for calculating the test statistic\ngroups_null &lt;- group_by(pol_fish_null, polynesia_isolation)\ngroup_means_null &lt;- summarize(groups_null, ybar = mean(richness))\n\ntest_stat_null &lt;- diff(group_means_null$ybar)\ntest_stat_null\n\n[1] -9.088235\n\n\nGreat! Now we just need to do that over and over again! Luckily we can ask R to do the work for us. We can use the replicate function to do the same task over and over. Let’s look at a simple example first.\nSuppose we want to simulate the distribution of outcomes of rolling two dice (and adding their values). We can do one roll like this:\n\ndie1 &lt;- sample(6, 1)\ndie2 &lt;- sample(6, 1)\ndie1 + die2\n\n[1] 9\n\n\nDo do that 20 times, we just copy and paste the above code into replicate:\n\ndice_rolls &lt;- replicate(20, {\n    die1 &lt;- sample(6, 1)\n    die2 &lt;- sample(6, 1)\n    die1 + die2\n})\n\ndice_rolls\n\n [1]  2 10 10 10  9  5 10  7  4  8  5  8  7  7  3  6  4  7  9  9\n\n\nCool, we got 20 random outcomes! Notice that we had to paste the three lines of code into “squiggly brackets” {} within the replicate function. That’s just our way of letting R know that those three lines of code all need to be run together, that the whole set of three lines is what we want R to compute 20 times. With this structure, we could ask R to make 20 replicates, or 20,000!\nNow we can use the same approach to make the null distribution. Let’s do 1000 re-shufelings to make our null distribution.\n\nnull_dist &lt;- replicate(1000, {\n    # reshuffle the group identities\n    pol_fish_null$polynesia_isolation &lt;- \n        sample(pol_fish_null$polynesia_isolation)\n    \n    # follow the same steps as before for calculating the test statistic\n    groups_null &lt;- group_by(pol_fish_null, polynesia_isolation)\n    group_means_null &lt;- summarize(groups_null, ybar = mean(richness))\n    \n    test_stat_null &lt;- diff(group_means_null$ybar)\n    test_stat_null\n})\n\n# the output of `replicate` will be a vector with 1000 null test statistics\n# let's just look at the first little bit\nhead(null_dist)\n\n[1]   1.058824  30.352941 -17.647059   4.588235 -18.264706  -1.235294\n\n\nLet’s visualize the null distribution as a histogram\n\nlibrary(ggplot2)\n\n# notice we need to make a data.frame in order for ggplot to work\nnull_dist_df &lt;- data.frame(null_test_stat = null_dist)\n\nggplot(null_dist_df, aes(x = null_test_stat)) +\n    geom_histogram()\n\n\n\n\n\n\n4. Calculate the \\(p\\)-value\nNow we calculate the \\(p\\)-value. Remember, our default assumption is that we are considering a two-tailed alternative hypothesis. So we need to calculate \\(Pr(\\text{lower tail}) + Pr(\\text{upper tail})\\). Our test statistic is -22.5 which is negative, so our lower tail probability will be the proportion of times that null_dist is less than or equal to test_stat, and the upper tail probability will be the proportion of times that null_dist is greater than or equal to -1 * test_stat. Let’s put that in code:\n\nlower_tail &lt;- mean(null_dist &lt;= test_stat)\nupper_tail &lt;- mean(null_dist &gt;= -1 * test_stat)\npval &lt;- lower_tail + upper_tail\npval\n\n[1] 0.043\n\n\nWow cool! our \\(p\\)-value is 0.043. What does that mean?\n\n\n5. Decide: reject \\(H_0\\) or fail to reject \\(H_0\\)\nThat is the question. Since we’re in biostatistics and the convention in our field is to set \\(\\alpha = 0.05\\), then we will reject \\(H_0\\) because \\(p \\le \\alpha\\), i.e., it is true that 0 \\(\\le 0.05\\)."
  },
  {
    "objectID": "lab08.html#questions",
    "href": "lab08.html#questions",
    "title": "8  Statistical Null Hypothesis Testing",
    "section": "8.4 Questions",
    "text": "8.4 Questions\nA common hypothesis in the study of biodiversity is that species richness is higher in the tropics. The tropics are defined as any place between the latitudes of -23.43615° and 23.43615°, i.e. 23.43615° south of the equator and 23.43615° north of the equator. Reef ecosystems largely exist only within the tropics, but there are a few interesting and unique types of reef ecosystems outside the tropic latitudes as well. That means we can use our reef fish dataset to test whether reef species richness is higher in the tropics or not.\nIn questions 1–X you will conduct the steps of testing this scientific hypothesis with statistical null hypothesis testing.\n\nFirst we need to do some data manipulation to add a column to our reef fish data that tells us if the sites are tropical or temperate (temperate refers to areas outside the tropics). To get you started, recall this is how we can add a column:\nreef_fish$trop_or_temp &lt;- \"tropical\"\nView(reef_fish)\nNow we’ve added one column, but all the values in that column say “tropical”. We need to manipulate that new trop_or_temp column to say “temperate” for all latitudes outside the tropics. Here’s the code to do that for the Northern Hemisphere:\nreef_fish$trop_or_temp[reef_fish$lat &gt;= 23.43615] &lt;- \"temperate\"\nNow your job is to do the same for the Southern Hemisphere.\nNow that we have our column trop_or_temp telling us which group each data point belongs to, we need to state our null and alternative hypotheses\nNow calculate our test statistic\nNow we generate the null distribution. Let’s do that in two steps:\n\nRefer to the code where we shuffled the polynesia_isolation of pol_fish_null. Use that example to make one random reshuffling of the trop_or_temp column and caculate one null test statistic\nNow use the code from (4a.) and refer to the use of the replicate function to simulate a null distribution with 1000 replicates\n\nPlot a histogram of the null distribution\nCalculate the \\(p\\)-value for the two-tailed alternative hypothesis\nDecide whether we reject or fail to reject the null hypothesis"
  }
]