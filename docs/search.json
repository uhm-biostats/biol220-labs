[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Labs for Intro Biostatistics",
    "section": "",
    "text": "Setup",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#software-and-cloud-services",
    "href": "index.html#software-and-cloud-services",
    "title": "Labs for Intro Biostatistics",
    "section": "Software and Cloud Services",
    "text": "Software and Cloud Services\nAll labs will use R, RStudio, and the Koa Server operated by UH Mānoa.\nR is a computer program that allows an extraordinary range of statistical calculations. It is a free program, mainly written by volunteer contributors from around the world.\nRStudio is a separate program, also free, that allows you to easily organize separate tabs for R code files, graphics, help docs, and more.\nFor this course, you won’t need R and RStudio installed on your own device because we will work on the Koa Server. However, if you wish to install them on your device, go to https://rstudio.com/products/rstudio/download/ for instructions about getting set up.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "index.html#using-the-koa-server",
    "href": "index.html#using-the-koa-server",
    "title": "Labs for Intro Biostatistics",
    "section": "Using the Koa Server",
    "text": "Using the Koa Server\nThe Koa Server allows us to run R and RStudio remotely on the servers maintained by UH. Each student has a dedicated folder where all labs (and problem sets) will be posted.\nTo log into the Koa Server visit this url: koa.its.hawaii.edu.\nWhen prompted, login with your UH credentials. Once you are logged in, you will see this landing page:\n\n\n\n\n\n\n\n\n\nClick on the “Interactive Apps” drop-down menu and select RStudio Server\n\n\n\n\n\n\n\n\n\nThis will open up a form where you can select different options for your session working on the Koa Server. Leave all the defaults as is except\n\nenter a reasonable number of hours from 1–4 depending on how long you plan to work in one continuous sitting (if you keep working past the number of hours you entered your session will end, but not to worry, you can just restart it and you won’t lose any work)\nenter 8 GB of RAM (after you set this as 8 the first time, you might not need to re-set it)\n\nOnce you have made your selection for hours and RAM, hit “Launch.”\n\n\n\n\n\n\n\n\n\nYou will have to wait a few seconds for your session to become active. Once it’s ready you will see a button “Connect to RStudio Server.” Click that button and you will be taken to your RStudio session.\n\n\n\n\n\n\n\n\n\nRStudio is arranged into several panels, each with its own purpose. You will learn all about these in “Lab 01: Getting started”. For now, we need to know that the bottom right panel is how we access files and folders on the server. Click on the folder named for our class “biol220_class.”\n\n\n\n\n\n\n\n\n\nThen click “students”\n\n\n\n\n\n\n\n\n\nAnd finally click on the folder named the same as your UH username\n\n\n\n\n\n\n\n\n\nNow the bottom right panel will show you all your lab (and problem set) assignments. As assignments are posted they will appear inside your folder. As an example, check out the “lab_01_getting-started.R” assignment. Clicking on that file will open an R script where you will complete the assignment.\n\n\n\n\n\n\n\n\n\nYou’ll see that the file opens in the top left panel.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "lab01.html",
    "href": "lab01.html",
    "title": "1  Getting started",
    "section": "",
    "text": "1.1 Goals\nAs with all labs, please complete this lab using RStudio running on the UH Koa Server. Access the Koa Server at koa.its.hawaii.edu and use your UH credentials to log in.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "lab01.html#goals",
    "href": "lab01.html#goals",
    "title": "1  Getting started",
    "section": "",
    "text": "Learning how to start with Koa Server\nUse the command line\nUse functions in R\nUse vectors\nUse data frames",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "lab01.html#r-rstudio-and-koa-server",
    "href": "lab01.html#r-rstudio-and-koa-server",
    "title": "1  Getting started",
    "section": "1.2 R, RStudio, and Koa Server",
    "text": "1.2 R, RStudio, and Koa Server\n\n1.2.1 What is R?\nR is a computer program that allows an extraordinary range of statistical calculations. It is a free program, mainly written by voluntary contributions from statisticians around the world. R is available on most operating systems, including Windows, Mac OS, and Linux.\nR can make graphics and do statistical calculations. It is also a full-fledged computing language. In this manual, we will only scratch the surface of what R can do.\n\n\n1.2.2 What is RStudio?\nRStudio is a separate program, also free, that provides a more elegant front end for R. RStudio allows you to easily organize separate windows for R commands, graphic, help, etc. in one place.\nFor this course, you won’t need R and RStudio installed on your own device because we will work on the Koa Server. However, if you wish to install them on your device, go to https://rstudio.com/products/rstudio/download/ for instructions about getting set up.\n\n\n1.2.3 What is the Koa Server?\nThe Koa Server allows you to run a hosted version of RStudio in the cloud that allows you to run R without having to download anything on your personal computer or confirgure your personal computer. You access the Koa Server from your browser, and conveniently you can access it from any computer or tablet.\n\n\n1.2.4 Getting started with on the Koa Server\nFollow the instructions on the “Setup” page for setting yourself up on the Koa Server.\nOnce done, proceed with the rest of the lab below.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "lab01.html#learning-the-tools",
    "href": "lab01.html#learning-the-tools",
    "title": "1  Getting started",
    "section": "1.3 Learning the Tools",
    "text": "1.3 Learning the Tools\nOnce you have launched your RStudio session on the Koa Server you should see a new window with a menu bar at the top and three main panels.\n\n\n\n\n\n\n\n\n\nThe left panel is called the “Console”—this is where you type commands to give instructions to R and typically where you see R’s answers to you.\nThe top right panel can show various different pieces of information. We will not make extensive use of this panel in our class, but you can note that the “Environment” tab in this panel shows all the objects (more on that below) that you have created in your current R session.\nThe bottom left panel, as you already know, shows the folders and files you have access to. You should always make sure you are working in the folder named after your UH username.\nThis bottom left panel is also where help documents will be rendered (in the tab called “Help”) and data visualizations will be shown (in the tab called “Plots”).\nOnce you click on an R script to open it, you Console gets pushed down to the bottom left panel and the script appears in the top left panel.\n\n\n\n\n\n\n\n\n\n\n1.3.1 The Console\nYou can type commands into the Console where there is a prompt (which will look like a &gt; sign at the bottom of the window). The Console has to be the selected window. (Clicking anywhere in the Console selects it.)\nThe &gt; prompt is R’s way of inviting you to give it instructions. You communicate with R by typing commands after the &gt; prompt.\nType “2 + 2” at the &gt; prompt, and hit return. You’ll see that R can work as a calculator (among its many other uses). It will give you the answer 4 and it will label that answer with [1] to indicate that it is the first element in the answer. (This is sort of annoying when the answers are simple like this, but can be very valuable when the answers become more complex and have multiple elements.)\nRemember, you don’t type the &gt; sign. The &gt; is the prompt that R gives saying it is ready for input. We reproduce it here so you can see which is input (in blue) and which is output (in black or red).\n\n2 + 2\n\n[1] 4\n\n\nYou can use a wide variety of math functions to make calculations here, e.g., log() calculates the log of a number:\n\nlog(42)\n\n[1] 3.73767\n\n\n(By default, this gives the natural log with base \\(e\\).)\nParentheses are used both as a way to group elements of the calculation and also as a way to denote the arguments of functions. (The “arguments” of a function are the set of values given to it as input.) For example, log(3) is applying the function log() to the argument 3.\nAnother mathematical function that often comes in handy is the square root function, sqrt(). For example, the square root of 4 is:\n\nsqrt(4)\n\n[1] 2\n\n\nTo calculate a value with an exponent, used the ^ symbol. For example \\(4^3\\) is written as:\n\n4^3\n\n[1] 64\n\n\nNote how R ignores white space when it’s not in quotes (we’ll come back to quotes later):\n\n4^3\n\n[1] 64\n\n4  ^  3\n\n[1] 64\n\n\nOf course, math functions can be combined to give an infinite possibility of mathematical expressions. For example,\n\\[\\frac{1}{\\sqrt{2 \\pi \\times 3.1^2}} e^{-\\frac{(12 - 10.7) ^ 2}{2 \\times 3.1}}\\]\ncan be calculated with\n\n1 / sqrt(2 * pi * 3.1^ 2) * exp(-(12 - 10.7)^2 / (2 * 3.1))\n\n[1] 0.09798692\n\n\nRemember parentheses are used both to pass arguments to functions (like exp(-(12 - 10.7)^2 / (2 * 3.1))) but also to group mathematical operations. For example, the argument passed to exp() is -(12 - 10.7)^2 / (2 * 3.1). We need all those parentheses to make sure the fraction is correctly computed as \\(-\\frac{(12 - 10.7) ^ 2}{2 \\times 3.1}\\). If we didn’t have the parentheses around (12 - 10.7)^2 and instead wrote -12 - 10.7^2 / (2 * 3.1), that would be valid code, but it would calculate an answer to the wrong equation:\n\\[-12 - \\frac{10.7^2}{(2 \\times 3.1)}\\]\n\n\n1.3.2 Saving your code\nWhen you analyze your own data, we strongly recommend that you keep a record of all commands used, along with copious notes, so that weeks or years later you can retrace the steps of your earlier analysis.\nIn RStudio you can create a plain text file (sometimes called a script), which contains R commands that can be reloaded and used at a later date. We create scripts for you for every assignment where you can enter and save your commands.\nWhile you’re “learning the tools” you can copy and paste any commands that you want from the the lab instructions, the Console (do not include the &gt; prompt in the script), or type directly into the script. Save this script for later reference by hitting “Save” under the “File” menu. In the future you can open this file to have those commands available for use again.\nIt is a good habit to type all your commands in the script window and run them from there, rather than typing directly into the console. This lets you save a record of your session so that you can more easily re-create what you have done later.\nFYI, if you want to create a new, blank R script, here’s how: under the menu at the top, choose “File”, then “New File”, and then “R Script”. Follow the prompts to save the new file.\n\n\n1.3.3 Comments\nIn scripts, it can be very useful to save a bit of text which is not to be evaluated by R. You can leave a note to yourself (or a collaborator) about what the next line is supposed to do, what its strengths and limitations are, or anything else you want to remember later. To leave a note, we use “comments”, which are a line of text that starts with the hash symbol #. Anything on a line after a # will be ignored by R.\n\n# This is a comment. Running this in R will \n# have no effect.\n\n\n\n1.3.4 Functions\nMost of the work in R is done by functions. A function has a name and one or more arguments. For example, log(4) is a function that calculates the log in base \\(e\\) for the value 4 given as input.\nSometimes functions have optional input arguments. For the function log(), for example, we can specify the optional input argument base to tell the function what base to use for the logarithm. If we don’t specify the base variable, it has a default value of base = e. To get a log in base 10, for example, we would use:\n\nlog(4, base = 10)\n\n[1] 0.60206\n\n\n\n\n1.3.5 Defining variables\nIn R, we can store information of various sorts by assigning them to variables. For example, if we want to create a variable called x and give it a value of 4, we would write\n\nx &lt;- 4\n\nThe middle bit of this—a less than sign and a hyphen typed together to make something that looks a little like a left-facing arrow – tells R to assign the value on the right to the variable on the left. After running the command above, whenever we use x in a command it would be replaced by its value 4. For example, if we add 3 to x, we would expect to get 7.\n\nx + 3\n\n[1] 7\n\n\nVariables in R can store more than just simple numbers. They can store lists of numbers, functions, graphics, etc., depending on what values get assigned to the variable.\nWe can always reassign a new value to a variable. If we now tell R that x is equal to 32\n\nx &lt;- 32\n\nthen x takes its new value:\n\nx\n\n[1] 32\n\n\n\n\n1.3.6 Names\nNaming variables and functions in R is pretty flexible.\nA name has to start with a letter, but that can be followed by any combination of letters, numbers, and underscores (_). Names cannot have spaces or any character other than letters, numbers, and underscores, for example $, -, and % are not allowed. Technically, periods (.) are allowed in names, but not reccomended excpet for specific uses outside the scope of this course.\nNames in R are case-sensitive, which means that Weights and weights are completely different things to R. This is a common and incredibly frustrating source of errors in R.\nIt’s a good idea to have your names be as descriptive as possible, so that you will know what you meant later on when looking at it. (However, if they get too long, it becomes painful and error prone to type them each time we use them, so this, as with all things, requires moderation.)\nSometimes clear naming means that it is best to have multiple words in the name, but we can’t have spaces. Therefore a common approach is like we saw in the previous section, to chain the words with underscores (not hyphens!), as in weights_before_hospital. (Another solution to make separate words stand out in a variable name is to vary the case: weightsBeforeHospital. This is called “Camel Case” because the capital letters are like camel humps.)\n\n\n1.3.7 Vectors\nOne useful feature of R is the ability to apply functions to an entire collection of numbers. The technical term for a set of numbers is “vector”. For example, the following code will create a vector of six numbers:\n\n c(78, 85, 64, 54, 102, 98.6)\n\n[1]  78.0  85.0  64.0  54.0 102.0  98.6\n\n\nc() is a function that creates a vector, containing the items given in its arguments. To help you remember, you could think of the function c() meaning to “combine” some elements into a vector.\nLet’s add a little extra here to make the computer remember this vector. Let’s assign it to a variable, called temperatureF (because these numbers are actually a set of temperatures in degrees Fahrenheit):\n\ntemperatureF &lt;- c(78, 85, 64, 54, 102, 98.6)\n\nThe combination of the less than sign and the hyphen makes an arrow pointing from right to left—this tells R to assign the stuff on the right to the name on the left. In this case we are assigning a vector to the variable temperatureF.\nInputting this into R causes no obvious output, but R will now remember this vector of temperatures under the name temperatureF. We can view the contents of the vector temperatureF by simply typing its name:\n\ntemperatureF\n\n[1]  78.0  85.0  64.0  54.0 102.0  98.6\n\n\nThe power of vectors is that R can do the same calculation on all elements of a vector with one command. For example, to convert a temperature in Fahrenheit to Celsius, we would want to subtract 32 and multiply times 5/9. We can do that for all the numbers in this vector at once:\n\ntemperatureC &lt;- (temperatureF - 32) * 5 / 9\ntemperatureC\n\n[1] 25.55556 29.44444 17.77778 12.22222 38.88889 37.00000\n\n\nTo pull out one of the numbers in this vector, we add square brackets after the vector name, and inside those brackets put the index of the element we want. (The “index” is just a number giving the location in the vector of the item we want. The first item has index 1, etc.) For example, the second element of the vector temperatureC is\n\ntemperatureC[2]\n\n[1] 29.44444\n\n\nOne of the ways to slip up in R is to confuse the [square brackets] which pull out an element of a vector, with the (parentheses), which is used to enclose the arguments of a function.\nVectors can also operate mathematically with other vectors. For example, imagine you have a vector of the body weights of patients before entering hospital (weight_before_hospital) and another vector with the same patient’s weights after leaving hospital (weight_after_hospital). You can calculate the change in weight for all these patients in one command, using vector subtraction:\n\nweight_before_hospital &lt;- c(100, 102)\nweight_after_hospital &lt;- c(98, 99)\n\nweight_change_during_hospital &lt;- weight_before_hospital - weight_after_hospital\n\nThe result will be a vector that has each patient’s change in weight.\n\n\n1.3.8 Basic calculation examples\nIn this course, we’ll learn how to use a few dozen functions, but let’s start with a couple of basic ones.\nThe function mean() does just what it sounds like: it calculates the sample mean (that is, the average) of the vector given to it as input. For example, the mean of the vector of the temperatures in degrees Celsius from above is 26.81481:\n\nmean(temperatureC)\n\n[1] 26.81481\n\n\nAnother simple (and simply named) function calculates the sum of all numbers in a vector: sum().\n\nsum(temperatureC)\n\n[1] 160.8889\n\n\nTo count the number of elements in a vector, use length().\n\nlength(temperatureC)\n\n[1] 6\n\n\nThis shows that there are 6 temperature values in the vector that make up the vector temperatureC.\n\n\n1.3.9 Reading a data file\nIn this course, we have saved the data in a “comma-separated variable” format. All files in this format ought to have “.csv” as the end of their file name. A CSV file is a plain text file, easily read by a wide variety of programs. Each row in the file (besides the first row) is the data for a given individual, and for each individual each variable is listed in the same order, separated by commas. It’s important to note that you can’t have commas anywhere else in the file, besides the separators.\nThe first row of a CSV file should be a “header” row, which gives the names of each variable, again separated by commas.\nFor examples in this tutorial, let’s use a data set about the passengers of the RMS Titanic. One of the data sets in the folder of data attached to this lab is called “titanic.csv”. This is a data set of 1313 passengers from the voyage of this ship, which contains information about some personal info about each passenger as well as whether they survived the accident or not.\nTo import a CSV file into R, use the read.csv() function as in the following command. The long string in the quotes is how we tell R where to find the file we want to read in. Note: you will need to replace &lt;username&gt; with your actual UH username, for example I would replace &lt;username&gt; with rominger. In this case, the file is in the data folder, itself in the student’s &lt;username folder, itself in the biol220_class folder. All data we use will be found in the data folder at this location.\n\ntitanic_data &lt;- read.csv(\"biol220_class/students/&lt;username&gt;/data/titanic.csv\")\n\nHere we have given the name titanic_data to the object in R that contains all this passenger data. Of course, if you wanted to load a different data set, you would be better off giving it a more apt name than “titanic_data”.\nTo see if the data loads appropriately, you might want to run the command\n\nsummary(titanic_data)\n\n passenger_class        name                age            embarked        \n Length:1313        Length:1313        Min.   : 0.1667   Length:1313       \n Class :character   Class :character   1st Qu.:21.0000   Class :character  \n Mode  :character   Mode  :character   Median :30.0000   Mode  :character  \n                                       Mean   :31.1942                     \n                                       3rd Qu.:41.0000                     \n                                       Max.   :71.0000                     \n                                       NA's   :680                         \n home_destination       sex              survive         \n Length:1313        Length:1313        Length:1313       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n\n\nwhich will list all the variables and some summary statistics for each variable.\n\n\n1.3.10 Intro to data frames\nA data frame is a way that R can store a data set on a number of individuals. A data frame is a collection of columns; each column contains the values of a single variable for all individuals. The values of each individual occur in the same order in all the columns, so the first value for one variable represents the same individual as the first value in the lists of all other variables.\nThe function read.csv() loads the data it reads into a data frame.\nThe data frame is usually given a name, which is used to tell R’s functions which data set to use. For example, in the previous section we read in a data set to a data frame that we called titanic_data. This data frame now contains information about each of the passengers on the Titanic. This data frame has seven variables, so it has seven columns (passenger_class, name, age, embarked, home_destination, sex, and survive).\nVery importantly, we can grab one of the columns from a data frame by itself. We write the name of the data frame, followed by a $, and then the name of the variable.\nFor example, to show a list of the age of all the passengers on the Titanic, use\n\ntitanic_data$age\n\nThis will show a vector that has the values for this variable age, one for each individual in the data set.\nNote, when looking at long vectors or data frames, it’s convenient to use the head function, which only shows the first 6 elements, not the whole huge vector or data frame.\n\nhead(titanic_data$age)\n\n[1] 29.0000  2.0000 30.0000 25.0000  0.9167 47.0000\n\n\n\n\n1.3.11 Adding a new column\nSometimes we would like to add a new column to a data frame. The easiest way to do this is to simply assign a new vector to a new column name, using the $.\nFor example, to add the log of age as a column in the titanic_data data frame, we can write\n\ntitanic_data$log_age &lt;- log(titanic_data$age)\n\nYou can run the command head(titanic_data) to see that log_age is now a column in titanic_data.\n\n\n1.3.12 Choosing subsets of data\nSometimes we want to do an analysis only on some of the data that fit certain criteria. For example, we might want to analyze the data from the Titanic using only the information from females. The easiest way to do this is to use the subset function.\nIn the titanic data set there is a variable named sex. We can create a new data frame that includes only the data from passengers recorded as female with the following command:\n\ntitanic_female_data &lt;- subset(titanic_data, sex == \"female\")\nhead(titanic_female_data)\n\n   passenger_class                                       name age    embarked\n1              1st                  Allen,MissElisabethWalton  29 Southampton\n2              1st                   Allison,MissHelenLoraine   2 Southampton\n4              1st  Allison,MrsHudsonJ.C.(BessieWaldoDaniels)  25 Southampton\n7              1st              Andrews,MissKorneliaTheodosia  63 Southampton\n9              1st    Appleton,MrsEdwardDale(CharlotteLamson)  58 Southampton\n12             1st Astor,MrsJohnJacob(MadeleineTalmadgeForce)  19   Cherbourg\n              home_destination    sex survive   log_age\n1                   StLouis,MO female     yes 3.3672958\n2  Montreal,PQ/Chesterville,ON female      no 0.6931472\n4  Montreal,PQ/Chesterville,ON female      no 3.2188758\n7                    Hudson,NY female     yes 4.1431347\n9            Bayside,Queens,NY female     yes 4.0604430\n12                  NewYork,NY female     yes 2.9444390\n\n\nThis new data fame will include all the same columns as the original titanic_data, but it will only include the rows for which the sex was “female”.\nNote that the syntax here requires a double == sign. In R (and many other computer languages), the double equal sign creates a statement that can be evaluated as TRUE or FALSE. Here we are asking, for each individual, whether sex is “female”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "lab01.html#questions-for-lab-report",
    "href": "lab01.html#questions-for-lab-report",
    "title": "1  Getting started",
    "section": "1.4 Questions for Lab Report",
    "text": "1.4 Questions for Lab Report\nYour lab report is due before the start of next week’s lab. When you’re finished, save it, export it, and upload it to Google Classroom to turn in the assignment.\nTo export your script from the RStudio session running on the Koa Server, select the radio button next to the script name.\n\n\n\n\n\n\n\n\n\nThen click the gear icon titled “More” and from the drop-down menu select “Export.”\n\n\n\n\n\n\n\n\n\nWhen prompted click “Download” and then save the script somewhere on your device. Once downloaded on your device, you can upload to Google Classroom.\nTo answer each of the questions below, in your script lab_01-getting-started.R write the question number as a comment, followed by any R code you use to answer the question, and give the answers as comments. It might look something like this:\n\n# Questions\n\n# 1. I followed directions to set up my lab report\n\n# 2. Yes, I got the same answers!\n\n# 3. Only text answers, no code\n# Answer part 1\n# Answer part 2\n\n# 4. Text and code\n# a.\nx &lt;- c(1, 2, 3) # you can also comment like this\n\n# b.\nmean(x)\n\n# The mean of c(1, 2, 3) is 2. Here's what that means...\n\n\nRun the Learning the tools commands in R from your “scratch.R” script. Did you get the same answers as shown in the text? (Answer “yes”, “no”, or a more detailed explanation. You don’t need to re-run all the code and output here.)\nFor each of the following, come up with a variable name that would be appropriate to use in R for the listed variable:\n\n\n\n\nVariable\nName in R\n\n\n\n\nBody temperature in Celsius\n\n\n\nHow much aspirin is given per dose for a patient\n\n\n\nNumber of televisions per person\n\n\n\nHeight (including neck and extended legs) of giraffes\n\n\n\n\n\nUse R to calculate:\n\n\\(15 \\times 17\\)\n\\(13^3\\)\n\\(\\text{log}_e(14)\\) (natural log)\n\\(\\text{log}_{10}(100)\\) (base 10 log)\n\\(\\sqrt{81}\\)\n\nWeddell seals live in Antarctic waters and take long strenuous dives in order to find fish to feed upon. Researchers (Williams et al. 2004) wanted to know whether these feeding dives were more energetically expensive than regular dives (perhaps because they are deeper, or the seal has to swim further or faster). They measured the metabolic costs of dives using the oxygen consumption of 10 animals (in ml O\\(_2\\) / kg) during a feeding dive. Here are the data:\n\n71.0, 77.3, 82.6, 96.1, 106.6, 112.8, 121.2, 126.4, 127.5, 143.1\n\nFor the same 10 animals, they also measured the oxygen consumption in non-feeding dives. With the 10 animals in the same order as before, here are those data:\n\n42.2, 51.7, 59.8, 66.5, 81.9, 82.0, 81.3, 81.3, 96.0, 104.1\n\n\nMake a vector for each of these lists, and give them appropriate names.\nConfirm (using R) that both of your vectors have the same number of individuals in them.\nCreate a vector called metabolism_difference by calculating the difference in oxygen consumption between feeding dives and nonfeeding dives for each animal.\nWhat is the average difference between feeding dives and nonfeeding dives in oxygen consumption?\nThe arithmetic mean is calculated by adding up all the numbers and dividing by how many numbers there are. Calculate the mean of these numbers using sum() and length(). Did you get the same answer as with using mean()?\nAnother appropriate way to represent the relationship between these two numbers would be to take the ratio of O\\(_2\\) consumption for feeding dives over the O\\(_2\\) consumption of nonfeeding dives. Make a vector which gives this ratio for each seal.\nSometimes ratios are easier to analyze when we look at the log of the ratio. Create a vector which gives the log of the ratios from the previous step. (Use the natural log.) What is the mean of this log-ratio?\n\nThe data file called “countries.csv” in the data folder contains information about all the countries on Earth1. Each row is a country, and each column contains a variable.\n\nUse read.csv() to read the data from this file into a data frame called countries.\nUse summary() to get a quick description of this data set. What are the first three variables?\nUsing the output of summary(), how many countries are from Africa in this data set?\nWhat kinds of variables (i.e., categorical or numerical) are continents, cell_phone_subscriptions_per_100_people_2012, total_population_in_thousands_2015, and fines_for_tobacco_advertising_2014? (Don’t go by their variable names – look at the data in the summary results to decide.)\nAdd a new column to your countries data frame that has the difference in ecological footprint between 2012 and 2000. What is the mean of this difference? (Note: this variable will have “missing data”, which means that some of the countries do not have data in this file for one or the other of the years of ecological footprint. By default, R doesn’t calculate a mean unless all the data are present. To tell R to ignore the missing data, add an option to the mean() command that says na.rm=TRUE. We’ll learn more about this later.)\n\nUsing the countries data again, create a new data frame called africa_data, that only includes data for countries in Africa. What is the sum of the total_population_in_thousands_2015 for this new data frame?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "lab01.html#footnotes",
    "href": "lab01.html#footnotes",
    "title": "1  Getting started",
    "section": "",
    "text": "These data mainly come from the World Health Organization, but the Continent list comes from https://datahub.io/ and the ecological footprint and cell phone data come from http://www.nationmaster.com.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "lab02.html",
    "href": "lab02.html",
    "title": "2  Graphics in R",
    "section": "",
    "text": "2.1 Goals\nAs with all labs, please complete this lab using RStudio running on the UH Koa Server. Access the Koa Server at koa.its.hawaii.edu and use your UH credentials to log in. If needed, refer back to “Setup” and “Lab 1” for instructions on access, setting up your account, and how to turn in your lab report.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphics in R</span>"
    ]
  },
  {
    "objectID": "lab02.html#goals",
    "href": "lab02.html#goals",
    "title": "2  Graphics in R",
    "section": "",
    "text": "Know how to load packages to expand the capabilities of R\nKnow some basic graphical formats and when they are useful\nMake graphs in R, such as histograms, bar charts, box plots, and scatter plots\nBe able to suggest improvements to basic graphs to improve readability and accurate communication",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphics in R</span>"
    ]
  },
  {
    "objectID": "lab02.html#learning-the-tools",
    "href": "lab02.html#learning-the-tools",
    "title": "2  Graphics in R",
    "section": "2.2 Learning the Tools",
    "text": "2.2 Learning the Tools\n\n2.2.1 Extending R’s capabilities with packages\nR has a lot of power in its basic form, but one of the most important parts about R is that it is expandable by the work of other people. These expansions are usually released in “packages”.\nEach package needs to be installed on your computer only once, but to be used it has to be loaded into R during each session.\nTo install a package in RStudio, click on the packages tab from the sub-window with tables for Files, Plots, Packages, Help, and Viewer. Immediately below that will be a button labeled “Install” – click that and a window will open.\n\nIn the second row (labeled “Packages”), type ggplot2. Make sure the box for “Install dependencies” near the bottom is clicked, and then click the “Install” button at bottom right. This will install the graphics package ggplot2.\n\nAlternatively, you can also use a function to install packages:\n\ninstall.packages(\"ggplot2\")\n\nInstalled packages will be available for future work sessions on Koa Server without needing to go through the installation process again. This is also true when using your own computer. But you will need to re-install packages after software updates.\n\n\n2.2.2 Loading a package\nOnce a package is installed, it needs to be loaded into R during a session if you want to use it. You do this with the function called library().\n\nlibrary(ggplot2)\n\nNow we can start making graphics with the ggplot2 package.\n\n\n2.2.3 ggplot\nWhile base R has ample graphic capabilities, functions from the ggplot2 package are becoming the de facto standard for scientific graphics because they allow more easy customization of plots.\nTo make a graph with ggplot2, you need to specify at least two elements in your command. The first uses the function ggplot itself, to specify which data frame you want to visualize and also which variables are to be plotted. The second part tells R what kind of graph to make, using a geom function. The odd part is that these two parts are put together with a + sign. It’s simplest to see this with an example. We’ll draw a histogram with ggplot in the next section.\n\n\n2.2.4 Histograms\nUseful when:\n\nResponse variable is numerical\n\nA histogram represents the frequency distribution of a numerical variable in a sample.\nLet’s see how to make a basic histogram using the age data from the Titanic data set. Make sure you have loaded the data (using read.csv()) into a data frame called titanic_data.\n\ntitanic_data &lt;- read.csv(\"biol220_class/students/rominger/data/titanic.csv\")\n\nHere’s the ggplot2 code to make a simple histogram of age:\n\nggplot(titanic_data, aes(x = age)) + \n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNotice that there are two functions called here, put together in a single command with a + sign. You don’t have to put a line break after the + (R ignores it), but it makes the code more readable. The first function is ggplot, and it has two input arguments. The first is titanic_data, this is the name of the data frame containing the variables that we want to graph. The second input to ggplot is an aes function. In this case, the aes function tells R that we want Age to be the \\(x\\)-variable (i.e. the variable that is displayed along the \\(x\\)-axis). The “aes” stands for “aesthetics”.\nThe second function in this command is geom_histogram(). This is the part that tells R that the “geometry” of our plot should be a histogram.\nRunning this should give a plot that look something like this:\n\n\n\n\n\n\n\n\n\nThis is not the most beautiful graph in the world, but it conveys the information. At the end of this tutorial we’ll see a couple of options that can make a ggplot graph look a little better.\n\n\n2.2.5 Bar graphs\nUseful when:\n\nResponse variable is categorical\n\nA bar graph plots the frequency distribution of a categorical variable. With ggplot, the syntax for a bar graph is very similar to that for a histogram. For example, here is a bar graph for the categorical variable sex in the titanic data set:\n\nggplot(titanic_data, aes(x = sex)) + \n  geom_bar(stat = \"count\")\n\nAside from specifying a different variable for \\(x\\) in the aes function, we use a different geom function here, geom_bar, and specify the statistic we want to draw, which is the count (or frequency) of the different categories. The result should look like this:\n\n\n\n\n\n\n\n\n\n\n\n2.2.6 Boxplots\nUseful when:\n\nExplanatory variable is categorical\nResponse variable is numerical\n\nA boxplot is a convenient way of showing the distribution of a numerical variable in multiple groups. Here’s the code to draw a boxplot for age in the titanic data set, separately for each recorded sex:\n\nggplot(titanic_data, aes(x = sex, y = age)) + \n    geom_boxplot()\n\nNotice that the \\(y\\) variable here is age, and \\(x\\) is the categorical variable sex that goes on the \\(x\\)-axis. The other new feature here is the new geom function, geom_boxplot().\n\n\n\n\n\n\n\n\n\nHere the thick bar in the middle of each boxplot is the median of that group. The upper and lower bounds of the box extend from the first to the third quartile. The vertical lines are called whiskers, and they cover most of the range of the data (except when data points are pretty far from the median, then they are plotted as individual dots, as on the male boxplot).\n\n\n2.2.7 Scatterplots\nUseful when:\n\nExplanatory variable is numerical\nResponse variable is numerical\n\nScatterplots shows the relationship between two numerical variables.\nThe titanic data set does not have two numerical variables, so let’s use a different data set. We will plot the relationship between sea surface temperature and species richness of reef fishes as compiled by Barneche et. al (2019). These data come from many different published fish surveys conducted by many different researchers all around the world. Barneche and colleagues compiled those data to try to understand what environmental variables predict the species richness of reef fish. Let’s find out!\nYou can load the data with:\n\nreef_fish &lt;- read.csv(\"data/global-reef-fish.csv\")\n\nTo make a scatter plot of the variables temp_C and spp_richness with ggplot, you need to specify the \\(x\\) and \\(y\\) variables, and use geom_point():\n\n# Side note:  I've added a line break between arguments in ggplot()\n# This has no effect on the code, but makes it easier to read IMO\nggplot(reef_fish, \n       aes(x = temp_C, y = spp_richness)) +\n    geom_point()\n\nThe result look like this:\n\n\n\n\n\n\n\n\n\n\n\n2.2.8 Better looking graphics with options\nThe code we have listed here for graphics barely scratches the surface of what ggplot2, and R as a whole, are capable of. Not only are there far more choices about the kinds of plots available, but there are many, many options for customizing the look and feel of each graph. You can choose the font, the font size, the colors, the style of the axes labels, etc., and you can customize the legends and axes legends nearly as much as you want.\nLet’s dig a little deeper into just a couple of options that you can add to any of the forgoing graphs to make them look a little better. For example, you can change the text of the \\(x\\)-axis label or the \\(y\\)-axis label by using xlab() or ylab(). Let’s do that for the scatterplot, to make the labels a little nicer to read for humans.\n\nggplot(reef_fish, \n       aes(x = temp_C, y = spp_richness)) +\n    geom_point() +\n    xlab(\"Temperature (degrees C)\") +\n    ylab(\"Species richness\")\n\nThe labels that we want to add are included in quotes inside the xlab() and ylab() functions. Here is what appears:\n\n\n\n\n\n\n\n\n\nIt can also be nice to remove the default gray background, to make what some feel is a cleaner graph. Try adding\n\n+ theme_minimal()\n\nto the end of one of your lines of code making a graph, to see whether you prefer the result compared to the default design.\n\n\n2.2.9 Color palettes\nIt is important to use a palette that will be clear to color blind individuals and, in some cases, to those who view a printed version in greyscale. There are bewildering array of options, but the viridis palettes accomplish these goals well (read more here). We’ll revisit the histogram example above and view the age distribution on the Titanic by sex (multiple histogram). This is a bit more advanced than what we’ve covered so far, but hang in there. We’ll go step-by-step.\nThe cool thing about ggplot2 is we can assign a large number of graphical features (size, color, fill, shape, line type, etc.) to variables on our data. We’ll do that using the fill = ... argument in the aes() function to make the fill of the bars dependent on sex.\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThat works, but it’s pretty ugly. For one thing, the bars are stacked on top of one another, so it’s hard to see the separate histograms for males and females. We’ll fix that by using the position = ... argument in the geom_histogram() function like this:\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n    # I will interleave comments to explain what's going on\n    # position = position_identity() stops the bars from stacking\n    geom_histogram(position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWell, that’s worse! Now the male bars are blocking the female bars. Let’s add a couple more arguments, making the color 50% transparent using the alpha = ... argument. I’ll also make the color around the bars black so we can see them better.\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n    # alpha = 0.5 makes bars transparent\n    # color = \"black\" adds black lines around bars\n    geom_histogram(alpha = 0.5, color = \"black\", \n                   position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nBetter, but not great. Let’s use the facet_grid() function to put the histograms on separate panels. For this, we have to put sex in quotes (learning when things need to be quoted or not is frustrating).\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n    # facet_grid() makes separate panels for each sex\n    facet_grid(rows = \"sex\") +\n    geom_histogram(alpha = 0.5, color = \"black\", \n                   position = position_identity())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nPretty good. Now, let’s finally add the viridis color palette. ggplot2 has some built in functions that you can just add using the + operator to change the color, like this:\n\nggplot(titanic_data, aes(x = age, fill = sex)) +\n    facet_grid(rows = \"sex\") +\n    geom_histogram(alpha = 0.5, color = \"black\", \n                   position = position_identity()) +\n    # This function changes the color palette\n    scale_fill_viridis_d()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n2.2.10 Getting help\nThe help pages in R are the main source of help, but the amount of detail might be off-putting for beginners. For example, to explore the options for ggplot(), enter the following into the R Console.\n\nhelp(ggplot)\n\n# you can also use\n?ggplot\n\nThis will cause the contents of the manual page for this function to appear in the Help window in RStudio Cloud. These manual pages are often frustratingly technical. An additional aid is to simply google the name of the function—there are a great number of resources online about R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphics in R</span>"
    ]
  },
  {
    "objectID": "lab02.html#questions",
    "href": "lab02.html#questions",
    "title": "2  Graphics in R",
    "section": "2.3 Questions",
    "text": "2.3 Questions\n\nUse the data from “countries.csv” to practice making some graphs.\n\nRead the data from the file “countries.csv” in the “data” folder. (Hint: we did this in the last lab - you need to use read.csv(), use the correct path, and give the object a name.)\nMake sure that you have run library(ggplot2). Why is this necessary for the remainder of this question?\nMake a histogram to show the frequency distribution of values for measles_immunization_oneyearolds, a numerical variable. (This variable gives the percentage of 1-year-olds that have been vaccinated against measles.) Describe the pattern that you see.\nMake a bar graph to show the numbers of countries in each of the continents. (The categorical variable continent indicates the continent to which countries belong.)\nDraw a scatterplot that shows the relationship between the two numerical variables life_expectancy_at_birth_male and life_expectancy_at_birth_female.\n\nThe ecological footprint is a widely-used measure of the impact a person has on the planet. It measures the area of land (in hectares) required to generate the food, shelter, and other resources used by a typical person and required to dispose of that person’s wastes. Larger values of the ecological footprint indicate that the typical person from that country uses more resources.\nThe countries data set has two variables showing the ecological footprint of an average person in each country. ecological_footprint_2000 and ecological_footprint_2012 show the ecological footprints for the years 2000 and 2012, respectively.\n\nPlot the relationship between the ecological footprint of 2000 and of 2012.\nDescribe the relationship between the footprints for the two years. Does the value of ecological footprint of 2000 seem to predict anything about its value in 2012?\nFrom this graph, does the ecological footprint tend to go up or down in the years between 2000 and 2012? Did the countries with high or low ecological footprint change the most over this time? (Hint: you can add a one-to-one line to your graph by adding + geom_abline(intercept = 0, slope = 1) to your ggplot() command. This will make it easier to see when your points are above or below the line of equivalence.)\n\nPlotting categorical and numerical variables: use the countries data again. Plot the relationship between continent and female life expectancy at birth. Describe the patterns that you see.\nMuchala (2006) measured the length of the tongues of eleven different species of South American bats, as well as the length of their palates (to get an indication of the size of their mouths). All of these bats use their tongues to feed on nectar from flowers. Data from the article are given in the file “BatTongues.csv”. In this file, both Tongue Length and Palette Length are given in millimeters. Each value for tongue length and palate length is a species mean, calculated from a sample of individuals per species.\n\nImport the data and inspect it using summary(). You can call the data set whatever you like, but in one of the later steps we’ll assume it is called bat_tongues.\nDraw a scatter plot to show the association between palate length and tongue length, with tongue length as the response variable. Describe the association: is it positive or negative? Is it strong or weak?\nAll of the data points that went into this graph have been double checked and verified. With that in mind, what conclusion can you draw from the outlier on the scatterplot?\nLet’s figure out which species is the outlier. To do this, we’ll use the subset function from Lab 1. Remember, the function subset gives us the row (or rows) of a data frame that has a certain property. Looking at the graph, we can tell that the point we are interested in has a very long tongue_length, at least over 80 mm long! Use subset to figure out the species name of this unusually long-tongued bat.\nThe unusual species is Anoura fistulata (See a photo here). This species has an outrageously long tongue, which it uses to collect nectar from a particular flower (can you guess what feature of the flower has led to the evolution of such a long tongue?). See the article by Muchala (2006) to learn more about the biology of this strange bat.\n\nImprove your figure! Pick one of the plots you made using R today. What could be improved about this graph to make it a more effective presentation of the data?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphics in R</span>"
    ]
  },
  {
    "objectID": "lab03.html",
    "href": "lab03.html",
    "title": "3  The Sampling Distribution",
    "section": "",
    "text": "3.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "lab03.html#goals",
    "href": "lab03.html#goals",
    "title": "3  The Sampling Distribution",
    "section": "",
    "text": "Understand the sampling distribution of an estimate\nInvestigate sampling error\nCalculate standard error of the mean\nCalculate confidence intervals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "lab03.html#learning-the-tools",
    "href": "lab03.html#learning-the-tools",
    "title": "3  The Sampling Distribution",
    "section": "3.2 Learning the Tools",
    "text": "3.2 Learning the Tools\n\n3.2.1 Simulating your own sampling distributions\nFor this lab we will simulate our own sampling distribution of the mean. Here we will use the penguins dataset from the palmerpenguins package. To simulate the sampling distribution you’ll treat the data in the penguins dataset as the entire population (it isn’t but we can pretend) and take many random samples from it, calculate the sample mean \\(\\bar{Y}\\) for each, and plot the distribution.\n\n3.2.1.1 Randomly sampling rows\nThe dplyr package offers tools that allow us to easily produce a random sample of rows from a dataset using the function slice_sample(). You may need to install the dplyr package, refer back to Lab 02 for how to do that. With the slice_sample() function we can, for example, sample 5 rows at random from the penguins data set by doing the following:\n\nlibrary(dplyr)\n\nsamp &lt;- slice_sample(penguins, n = 5)\nsamp\n\n    species island bill_len bill_dep flipper_len body_mass    sex year\n1    Adelie Biscoe     38.6     17.2         199      3750 female 2009\n2    Adelie  Dream     38.1     18.6         190      3700 female 2008\n3    Gentoo Biscoe     55.9     17.0         228      5600   male 2009\n4    Gentoo Biscoe     46.5     14.8         217      5200 female 2008\n5 Chinstrap  Dream     49.0     19.6         212      4300   male 2009\n\n\nNow try on your to increase the sample size 10, how would you do this?\n\n\n3.2.1.2 Repeated sampling\nTo simulate a sampling distribution, we need to repeatedly randomly sample the “population” (again, in this case, we’re pretending the penguins data set is the entire population). The infer package (again you may need to install this pacakge following direction sin Lab 02) has a convenient function rep_slice_sample() that will repeat slice_sample() many times. It creates a new column called replicate to index each replicate sample. To randomly sample 5 rows 4 times from the species Adelie, we do:\n\n# first load needed packages\nlibrary(palmerpenguins)\nlibrary(infer)\n\n# let's look at just the species setosa, so we'll need to subset our data\njust_adelie &lt;- subset(penguins, species == \"Adelie\")\n\nmany_adelie_samp &lt;- rep_slice_sample(just_adelie, n = 5, reps = 4)\nhead(many_adelie_samp)\n\n# A tibble: 6 × 9\n# Groups:   replicate [2]\n  replicate species island    bill_length_mm bill_depth_mm flipper_length_mm\n      &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1         1 Adelie  Torgersen           42.5          20.7               197\n2         1 Adelie  Biscoe              39.7          17.7               193\n3         1 Adelie  Dream               37.8          18.1               193\n4         1 Adelie  Torgersen           39.6          17.2               196\n5         1 Adelie  Torgersen           39.2          19.6               195\n6         2 Adelie  Biscoe              42            19.5               200\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNote: the infer package converted the penguins data.frame to something called a tibble, for our purposes, think of a data.frame and a tibble as equivalent.\nNow, let’s take 1000 samples of size 10 from each species. I’ll show you the code for Adelie, then fill in the blank (___) sections below to do it for the other species. After, you’ll need to combine the results before summarizing and plotting.\n\n# let's use `set.seed` so we can compare answers\nset.seed(123)\n\njust_adelie &lt;- subset(penguins, species == \"Adelie\")\nmany_adelie_samp10 &lt;- rep_slice_sample(just_adelie, n = 10, reps = 1000)\n\njust_chinstrap &lt;- subset(penguins, species == ___)\nmany_chinstrap_samp10 &lt;- rep_slice_sample(___, n = 10, reps = 1000)\n\njust_gentoo &lt;- subset(penguins, species == ___)\nmany_gentoo_samp10 &lt;- rep_slice_sample(___, n = 10, reps = 1000)\n\nIf you used the same seed (123) in the above code then you should get these same answers:\n\nmany_adelie_samp10\n\n# A tibble: 10,000 × 9\n# Groups:   replicate [1,000]\n   replicate species island    bill_length_mm bill_depth_mm flipper_length_mm\n       &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1         1 Adelie  Torgersen           38.6          21.2               191\n 2         1 Adelie  Dream               42.3          21.2               191\n 3         1 Adelie  Torgersen           37.3          20.5               199\n 4         1 Adelie  Dream               36            18.5               186\n 5         1 Adelie  Dream               41.5          18.5               201\n 6         1 Adelie  Dream               37.8          18.1               193\n 7         1 Adelie  Dream               38.9          18.8               190\n 8         1 Adelie  Dream               35.7          18                 202\n 9         1 Adelie  Dream               37.3          16.8               192\n10         1 Adelie  Dream               41.1          18.1               205\n# ℹ 9,990 more rows\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\nmany_chinstrap_samp10\n\n# A tibble: 10,000 × 9\n# Groups:   replicate [1,000]\n   replicate species   island bill_length_mm bill_depth_mm flipper_length_mm\n       &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1         1 Chinstrap Dream            47.6          18.3               195\n 2         1 Chinstrap Dream            45.7          17                 195\n 3         1 Chinstrap Dream            50.9          19.1               196\n 4         1 Chinstrap Dream            50.6          19.4               193\n 5         1 Chinstrap Dream            47.5          16.8               199\n 6         1 Chinstrap Dream            50.5          19.6               201\n 7         1 Chinstrap Dream            42.4          17.3               181\n 8         1 Chinstrap Dream            45.6          19.4               194\n 9         1 Chinstrap Dream            52.7          19.8               197\n10         1 Chinstrap Dream            46.2          17.5               187\n# ℹ 9,990 more rows\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\nmany_gentoo_samp10\n\n# A tibble: 10,000 × 9\n# Groups:   replicate [1,000]\n   replicate species island bill_length_mm bill_depth_mm flipper_length_mm\n       &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1         1 Gentoo  Biscoe           43.5          15.2               213\n 2         1 Gentoo  Biscoe           45.2          14.8               212\n 3         1 Gentoo  Biscoe           50            15.3               220\n 4         1 Gentoo  Biscoe           49.6          16                 225\n 5         1 Gentoo  Biscoe           43.6          13.9               217\n 6         1 Gentoo  Biscoe           47.5          15                 218\n 7         1 Gentoo  Biscoe           49.5          16.2               229\n 8         1 Gentoo  Biscoe           45.3          13.8               208\n 9         1 Gentoo  Biscoe           49.8          16.8               230\n10         1 Gentoo  Biscoe           49.1          14.5               212\n# ℹ 9,990 more rows\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\nRemember, we only need to use set.seed in situations where we’re trying to compare output from random sampling. You can delete set.seed after you compared your output to mine.\nYou can use the rbind function to combine all three sets of sampling distributions into a single tibble.\n\npenguins_sample_dists &lt;- rbind(many_adelie_samp10, \n                               many_chinstrap_samp10,\n                               many_gentoo_samp10)\n\npenguins_sample_dists\n\n# A tibble: 30,000 × 9\n# Groups:   replicate [1,000]\n   replicate species island    bill_length_mm bill_depth_mm flipper_length_mm\n       &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1         1 Adelie  Torgersen           38.6          21.2               191\n 2         1 Adelie  Dream               42.3          21.2               191\n 3         1 Adelie  Torgersen           37.3          20.5               199\n 4         1 Adelie  Dream               36            18.5               186\n 5         1 Adelie  Dream               41.5          18.5               201\n 6         1 Adelie  Dream               37.8          18.1               193\n 7         1 Adelie  Dream               38.9          18.8               190\n 8         1 Adelie  Dream               35.7          18                 202\n 9         1 Adelie  Dream               37.3          16.8               192\n10         1 Adelie  Dream               41.1          18.1               205\n# ℹ 29,990 more rows\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNow we have a very large set of samples to examine.\n\n\n\n3.2.2 Sample mean \\(\\bar{Y}\\)\nTo look at the distribution of the sample mean, we first need to calculate the sample mean for all 1000 replicates per species. We will use two helpful functions from dplyr to first group the penguins_sample_dists data.frame by species and replicate, and then calculate the mean for each species with the summarize function. Let’s first look at the sampling distribution of the mean of bill length:\n\nbill_length_sample_dists &lt;-  group_by(penguins_sample_dists, species, replicate)\nbill_length_sample_dists &lt;- summarize(bill_length_sample_dists, \n                                      Y_bar = mean(bill_length_mm, na.rm = TRUE))\n\nbill_length_sample_dists\n\n# A tibble: 3,000 × 3\n# Groups:   species [3]\n   species replicate Y_bar\n   &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;\n 1 Adelie          1  38.6\n 2 Adelie          2  37.0\n 3 Adelie          3  38  \n 4 Adelie          4  38.4\n 5 Adelie          5  39.2\n 6 Adelie          6  39.5\n 7 Adelie          7  39.8\n 8 Adelie          8  39.4\n 9 Adelie          9  40.2\n10 Adelie         10  38.2\n# ℹ 2,990 more rows\n\n\nModify the code above to calculate sample means for flipper_length_mm.\n\n\n3.2.3 Plot the sampling distribution \\(\\bar{Y}\\)\nWe can apply the ggplot() tools we’ve already learned to plot a multiple histogram to compare the sampling distributions in each species.\n\nlibrary(ggplot2)\n\nggplot(bill_length_sample_dists, aes(Y_bar, fill = species)) +\n    geom_histogram(alpha = 0.5, position = \"identity\", bins = 30) +\n    scale_fill_viridis_d()\n\n\n\n\n\n\n\n\nSee if you remember how to use facet_grid() to put each specie in it’s own panel like this:\n\n\n\n\n\n\n\n\n\n\n\n3.2.4 Standard error of the mean\nThe standard error of the mean helps us quantify our uncertainty about our estimate of the population mean given our sample size. We can calculate a hypothetical standard error for the perfect random sample of size \\(n\\) by dividing the population standard deviation by \\(\\sqrt{n}\\): \\(\\sigma / \\sqrt{n}\\). Let’s pretend that the penguins data set is the entire “population”, the population standard deviation for bill_length_mm is:\n\n\n\n\n\nspecies\n\\(\\sigma\\)\n\n\n\n\nAdelie\n2.65\n\n\nChinstrap\n3.29\n\n\nGentoo\n3.06\n\n\n\n\n\nNow calculate the hypothetical standard error of the mean for a sample size of 10. You should get:\n\n\n\n\n\nspecies\n\\(\\sigma\\)\n\\(\\sigma/\\sqrt{n}\\)\n\n\n\n\nAdelie\n2.65\n0.8380036\n\n\nChinstrap\n3.29\n1.0403894\n\n\nGentoo\n3.06\n0.9676570\n\n\n\n\n\nLet’s compare this hypothetical standard error of the mean to what we obtain from our simulations. Remember that the standard error of the mean is simply the standard deviation of the sampling distribution. That means we can get the answer by using the sd() function on our simulated sampling distribution.\n\nbill_length_se &lt;- group_by(bill_length_sample_dists, species)\nbill_length_se &lt;- summarize(bill_length_se, SE_Ybar = sd(Y_bar))\nbill_length_se\n\n# A tibble: 3 × 2\n  species   SE_Ybar\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Adelie      0.841\n2 Chinstrap   1.01 \n3 Gentoo      0.929\n\n\nNotice that we had to first group by Species, then summarize by taking the standard deviation of all of our sample means.\nAre the population standard errors of the mean close to what you calculated from the simulations? Are the standard errors what you expected given the multiple histogram figure above?\n\n\n3.2.5 Sample standard error of the mean\nThe sample standard error (\\(\\mathrm{SE}_{\\bar{Y}}\\)) quantifies our uncertainty in our estimate of the population mean, \\(\\bar{Y}\\). Specifically, \\(\\mathrm{SE}_{\\bar{Y}}\\) is the standard deviation of sampling distribution for \\(\\bar{Y}\\). The equation for the \\(\\mathrm{SE}_{\\bar{Y}}\\) is the sample standard deviation divided by the square-root of the sample size:\n\\[ \\mathrm{SE}_{\\bar{Y}} = \\frac{s}{\\sqrt{n}} \\]\nThere’s no function in R to calculate \\(\\mathrm{SE}_{\\bar{Y}}\\), but you know the functions for sample standard deviation and square-root. Use R to calculate the sample standard error of the mean for the following numbers:\n\n2.16 -0.79 -0.18 1.62 -0.98 -1.15 -0.15 1.34 1.96 1.74\n\nYou should get \\(\\mathrm{SE}_{\\bar{Y}}\\) = 0.42.\n\n\n3.2.6 95% confidence intervals\nConfidence intervals are a way to show the plausible range of parameter values given the data. 95% confidence intervals will include the true population parameter 95% of the time. We’ll learn ways to calculate confidence intervals for different parameters throughout the class. Today, we’ll use the “2 SE” rule to approximate 95% confidence intervals for the sample mean \\(\\bar{Y}\\). The lower bound and upper bounds of the approximate 95% confidence interval using the 2 SE rule are:\n\\[ \\text{lower CI}: \\bar{Y} - 2 \\times \\mathrm{SE}_{\\bar{Y}} \\] \\[ \\text{upper CI}: \\bar{Y} + 2 \\times \\mathrm{SE}_{\\bar{Y}} \\] Use the mean() function and standard error of the mean to calculate the confidence interval for the data you used in the last section. You should get:\n\n\n\n\n\n\\(\\bar{Y}\\)\nLower CI\nUpper CI\n\n\n\n\n0.557\n-0.2813638\n1.395364",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "lab03.html#questions",
    "href": "lab03.html#questions",
    "title": "3  The Sampling Distribution",
    "section": "3.3 Questions",
    "text": "3.3 Questions\nAll questions are about the sampling distribution of the sample mean, \\(\\bar{Y}\\)\n\nImport data\nWe’ll use a dataset about leaf sizes from Wright et al. (2017). We’ll pretend that this is population of all leaf sizes in the world and look at the properties of random samples from the population.\nUse the read.csv(), $, [, and/or dplyr functions to\n\nread-in the dataset\nmake a data.frame with only the latitude and leafsize_cm2 columns\nremove all rows with missing values from leafsize_cm2\nsubset the data to only tropical latitudes between -23.43655° and 23.43655°\nassign this data.frame to the name leafsize\n\nHints:\n\nto get ONE column you can use ...$column_name, to get multiple columns, you can use ...[, c(\"column_name1\", \"column_name2\")]\nyou can figure out if a value is missing with the is.na function\nlatitudes between -23.43655° and 23.43655° is the same as abs(latitude) &lt; 23.43655\n\nIf you’ve done everything correctly, you should get the same values for the population mean seen below:\nmean(leafsize$leafsize_cm2)\n[1] 65.97642\nCreate 1000 replicates each of sample sizes of 64, 256, and 1024 from the leafize data.frame you generated in a. I’ll show you the code for \\(n = 64\\), then you should copy and modify it to make similar objects called sample_dist256 and sample_dist1024. Then use rbind() to combine them into an object called sample_dists.\n# create replicate samples\nsample_dist64 &lt;- rep_slice_sample(leafsize, n = 64, reps = 1e3)\n\n# add a column recording the sample size\nsample_dist64$sample_size &lt;- 64\n\n# create replicate samples\nsample_dist256 &lt;- ___\n\n# add a column recording the sample size\n___ &lt;- 256\n\n# create replicate samples\nsample_dist1024 &lt;- ___\n\n# add a column recording the sample size\n___ &lt;- 1024\n\n# combine using `rbind`\nsample_dists &lt;- ___\nUse the group_by() and summarize() functions to calculate the sample mean for each level of sample size (64, 256, or 1024) and replicate. Make sure you assign the output a name so you can use it to make a plot in the next part.\nMake a multi-panel histogram with separate panels for each sample size. It should look something like this, but will not be exactly the same because the simulations are random.\n\nHow does the location and width of the sampling distribution for \\(\\bar{Y}\\) change as \\(n\\) increases?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "lab04.html",
    "href": "lab04.html",
    "title": "4  Probability",
    "section": "",
    "text": "4.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "lab04.html#goals",
    "href": "lab04.html#goals",
    "title": "4  Probability",
    "section": "",
    "text": "Be able to estimate probabilities from data\nUse sample to simulate events with different probabilities\nUse rules of probability to evaluate if different events in a dataset are likely to be independent or not",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "lab04.html#learning-the-tools",
    "href": "lab04.html#learning-the-tools",
    "title": "4  Probability",
    "section": "4.2 Learning the Tools",
    "text": "4.2 Learning the Tools\n\n4.2.1 More fun with penguins!\nWe’ll use our old friend the Palmer penguins data set to calculate and think about probabilities. Let’s first look at some simple probabilities:\n\nlibrary(palmerpenguins)\n\n# probability species is \"Adelie\"\nPr_adelie &lt;- sum(penguins$species == \"Adelie\") / nrow(penguins)\nPr_adelie\n\n[1] 0.4418605\n\n\nSo there is a 0.44 probability that if we grabbed a penguin out of this dataset it would belong to the Adelie species. The random trial here is “grabbing” a penguin, and the event of interest is that its species is Adelie.\nNotice how we’re calculating the probability. First we use penguins$species == \"Adelie\" to ask R to tell us TRUE ever time it finds the species is Adelie and FALSE ever time it finds the spcies to not be Adelie.\nThen we wrap those TRUE and FALSE values in sum:\n\nsum(penguins$species == \"Adelie\")\n\n[1] 152\n\n\nThat sum tells us the total number of TRUE values, aka the total number of penguins of species Adelie in the data.\nFinally we divide by nrow(penguins) to turn the count into a probability. That’s the definition of probability: the proportion of times an event is true.\nWe can also look at probabilities of numerical data, like the bill length\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- sum(penguins$bill_length_mm &lt; 45) / nrow(penguins)\nPr_bill_l_less45\n\n[1] NA\n\n\nWe got NA?! The reason is that the bill length column has some missing data. We’ve dealt with that before by using subset to remove rows with missing data or by using the na.rm argument in functions like mean() that tell R to ignore NA values in making calculations. Here we can use na.rm because sum() takes that argument:\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- sum(penguins$bill_length_mm &lt; 45, na.rm = TRUE) /\n    nrow(penguins)\nPr_bill_l_less45\n\n[1] 0.5116279\n\n\nSo there is a 0.51 probability of randomly grabbing a penguin from these data and its bill being less than 45 mm long.\nInterestingly, taking the sum of something and then dividing by sample size is also the definition of the mean, so we can take a shortcut and just use the mean function:\n\n# probability bill length is less than 45\nPr_bill_l_less45 &lt;- mean(penguins$bill_length_mm &lt; 45, na.rm = TRUE)\nPr_bill_l_less45\n\n[1] 0.5146199\n\n\nSame answer.\nNow let’s calculate some joint probabilities. What’s the probability that the bill length is less than 45 AND the species is Adelie? To save us some typing, let’s call this probability pAl45\n\npAl45 &lt;- mean(penguins$bill_length_mm &lt; 45 &\n                  penguins$species == \"Adelie\", \n              na.rm = TRUE)\npAl45\n\n[1] 0.4314869\n\n\nInteresting, \\(Pr(\\text{bill length} &lt; 45)\\) and \\(Pr(\\text{bill length} &lt; 45 \\text{ \\& Adelie})\\) are about the same, could that mean that bill length and species are independent? To figure out, we need to calculate \\(Pr(\\text{bill length} &lt; 45)\\) and \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) and see if the two probabilities are roughly the same.\nThe expression \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) tells us that we already know the species is Adelie. That means to calculate the probability bill length &lt; 45 GIVEN Adelie, we need to ignore all the rest of the data\n\n# subest to just Adelie\njustAdelie &lt;- subset(penguins, penguins$species == \"Adelie\")\n\n# now the conditional probability, is calculated just by counting up\n# cases where bill length &lt; 45\npl45GivenA &lt;- mean(justAdelie$bill_length_mm &lt; 45, na.rm = TRUE)\npl45GivenA\n\n[1] 0.9801325\n\n\nRecall that \\(Pr(\\text{bill length} &lt; 45)\\) = 0.51 which is vastly different from \\(Pr(\\text{bill length} &lt; 45 \\mid \\text{Adelie})\\) = 0.98 so we conclude that no, bill length and species are not independent.\nWe can confirm this by visualizing the histograms: different penguin species have different distributions of bill lengths, again confirming that the two variables are not independent.\n\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm, fill = species)) +\n    geom_histogram() + \n    facet_grid(rows = vars(species)) +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "lab04.html#simulating-random-events-with-sample",
    "href": "lab04.html#simulating-random-events-with-sample",
    "title": "4  Probability",
    "section": "4.3 Simulating random events with sample",
    "text": "4.3 Simulating random events with sample\nThe sample function let’s us take a random sample of just about anything. We can take a random sample of integers:\n\n# randomly sample 3 numbers between 1 and 10\nsample(1:10, 3)\n\n[1] 2 4 7\n\n\nWe can also sample characters:\n\n# sample 5 letters from A, B, C with replacement\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE)\n\n[1] \"B\" \"A\" \"A\" \"C\" \"A\"\n\n\nTry running the above code without setting replace = TRUE\n\nsample(c(\"A\", \"B\", \"C\"), 5)\n\nYou’ll get an error like this:\n\n\nError in sample.int(length(x), size, replace, prob) : \n  cannot take a sample larger than the population when 'replace = FALSE'\n\n\nThat’s because we’re trying to sample 5 random events from only 3 possible outcomes. We have to “replace” each outcome once we sampled it.\nWe can also sample with replacement when our number of random draws is less than the total number of outcomes:\n\n# randomly sample 3 numbers between 1 and 10\nsample(1:10, 3, replace = TRUE)\n\n[1] 1 6 6\n\n\nWe can also change the frequency of different outcomes by changing the probability that R assigns to them!\n\n# randomly sample 3 numbers between 1 and 10\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE, prob = c(0.8, 0.1, 0.1))\n\n[1] \"C\" \"A\" \"A\" \"A\" \"A\"\n\n\nThe above code will yield 80% “A”, and 10% of both “B” and “C” if we run it over and over.\nThe probabilities need to sum to 1 across all the outcomes because that’s one of the rules of probability. R will automatically do that for you, even if you don’t re-scale the probabilities yourself. For example, this code is equivalent:\n\n# randomly sample 3 numbers between 1 and 10\nsample(c(\"A\", \"B\", \"C\"), 5, replace = TRUE, prob = c(8, 1, 1))\n\n[1] \"A\" \"A\" \"A\" \"B\" \"A\"\n\n\nThe exact events will be different because each time you run it is a random trial.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "lab04.html#questions",
    "href": "lab04.html#questions",
    "title": "4  Probability",
    "section": "4.4 Questions",
    "text": "4.4 Questions\nIn questions 1–4 you will simulate data using the sample function and use the resulting data.frame to evaluate probabilities\n\nUse sample to make a data.frame with 40 rows and 2 columns: x and y. Column x should be filled with a random sample of the the integers 1 and 2, each with equal probability; column y should be filled with a random sample of the letters A and B, each with equal probability.\nThe data.frame you made in (1.) should have two independent columns (i.e. you were not instructed to use any code that would cause the events in column y to depend on the events in column x). To confirm their independence calculate \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) and evaluate if they are close to the same value. If at first they are not, try re-running the code a few times (each time will be different, you’re making random data!). In most of the times you re-run the code, the difference between \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) should be between -0.1 and 0.1\nGiven that we simulate y and x as independent, why do we find that \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) and not exactly equal? If we simulated a data.frame with 80 rows, would you expect \\(Pr(y = A | x = 1)\\) and \\(Pr(y = A)\\) to be more closely equal or more different? Why?\nDiscuss in 2 to 3 sentence how you could change the code in (2.) to simulate non-independence between the events in column x and column y. You do not need to implement this code, just discuss\n\nQuestions 5–7 will revisit the visualization of probabilities as overlapping circles that we used in class. NOTE: this is the same concept as what we covered in class, but the actual probabilities will be different\n\nFill in the probability figure. Use the made up data below to calculate the probabilities of all the events in the figure (e.g. what is \\(Pr(\\text{length} &gt; 40)\\)? What about \\(Pr(\\text{length} \\leq 40 \\text{ \\& species} \\neq \\text{uhu})\\)?)\n\n\n\n\n\n\n\n\n\nBelow, we have highlighted just two parts of the probability figure, use the above made up data to calculate the probabilities in this figure.\n\n\n\nUsing conditional probabilities (e.g. \\(Pr(A | B)\\)), would you say that the event of \\(length &gt; 40\\) and \\(species = uhu\\) are independent or not?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "lab05.html",
    "href": "lab05.html",
    "title": "5  Statistical Null Hypothesis Testing",
    "section": "",
    "text": "5.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Null Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lab05.html#goals",
    "href": "lab05.html#goals",
    "title": "5  Statistical Null Hypothesis Testing",
    "section": "",
    "text": "Review steps of null hypothesis statistical testing\nGenerate null distributions through repeated random sampling\nTest hypotheses with null distributions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Null Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lab05.html#learning-the-tools",
    "href": "lab05.html#learning-the-tools",
    "title": "5  Statistical Null Hypothesis Testing",
    "section": "5.2 Learning the Tools",
    "text": "5.2 Learning the Tools\n\n5.2.1 Hypothesis testing\nThe remainder of this course works within the null hypothesis statistical testing (NHST) framework that we will introduce this week. As we will discover, there are six core steps in NHST:\n\nState \\(H_0\\) and \\(H_A\\)\nCalculate the test statistic\nGenerate the null distribution\nCalculate the \\(p\\)-value\nDecide:\n\nReject \\(H_0\\) if \\(p \\le \\alpha\\)\nFail to reject \\(H_0\\) if \\(p &gt; \\alpha\\)\n\n\nFor the statistical tests that we encounter during this course, the equations to calculate appropriate test statistics, null distribution of the test statistic, and \\(p\\)-value are well understood. However, the concepts of null hypothesis testing can often be made more obvious by making a computer simulate the null distribution for us. Such “computationally intensive” null distributions also have important applications beyond learning the concepts—there are many real-world situations where no mathematical equation has ever been derived to calculate the necessary test statistic or null distribution for specific data or null hypotheses. In these cases, the only workable solution is to use a computer to simulate a null distribution.\nLet’s have a look at some data and a hypothesis where a “computational null distribution” will both be informative for learning, and also necessary because no mathematical equation exists to define an appropriate null distribution.\n\n\n5.2.2 Coral reef fish across Polynesia\nThe Hawaiian Islands and Rapa Nui are unique in the world for their geographic isolation. We might hypothesize that this isolation makes it difficult for organisms to disperse to these islands. Even fish (turns out they swim) might have a difficult time getting there. As such, we might hypothesize that compared to the rest of Polynesia, Hawaiʻi and Rapa Nui might have fewer species of coral reef-associated fishes. Let’s test that out. We have a dataset from Barneche et. al (2019) that compiles surveys of reef fish from across the globe. The Barneche et. al data report total numbers of species found at different sites. We’ll look at just a subset of those sites to test our scientific hypothesis about species richness of reef fish in Polynesia.\nLet’s go through the steps of null hypothesis testing:\n\n\n1. State \\(H_0\\) and \\(H_A\\)\n\n\\(H_0\\): Between the two groups “Distant Polynesia” (i.e. Hawaiʻi and Rapa Nui) and “Core Polynesia”, there will be no difference in mean species richness\n\\(H_A\\): There will be a difference in mean species richness between “Distant Polynesia” and “Core Polynesia”\n\n\n\n2. Calculate the test statistic\nWhat is the test statistic? The null hypothesis says there will be no difference in the mean richness. So our test statistic will be (mean species richness of Core Polynesia) - (mean species richness of Distant Polynesia).\nTo calculate that, we need to read-in the data,\n\n# remember to insert your actual username\nreef_fish &lt;- read.csv(\"biol220_class/students/username/data/reef_fish.csv\")\n\n\n# have a look at the data\nView(reef_fish)\n\nWe can see there are columns for region, polynesia_isolation, site, lon (longitude), lat (latitude), and richness (species richness). The polynesia_isolation column was added special for this lab. It has values core_polynisia, distant_polynisia, and NA. NA is for all sites outside of Polynesia. Before we calculate our test statistic, we need to subset our data to just Polynesia. We can do that like this:\n\n# \"pol_tri\" for polynesian triangle \npol_tri_fish &lt;- subset(reef_fish, !is.na(reef_fish$polynesia_isolation))\n\n\n# have a look\nView(pol_tri_fish)\n\nNow we will use our friends group_by and summarize from dplyr to help us calculate the test statistic.\n\nlibrary(dplyr)\ngroups &lt;- group_by(pol_tri_fish, polynesia_isolation)\ngroup_means &lt;- summarize(groups, ybar = mean(richness))\n\n# have a look (this is a small data frame, so we donʻt need\n# to use the `View` function)\ngroup_means\n\n# A tibble: 2 × 2\n  polynesia_isolation  ybar\n  &lt;chr&gt;               &lt;dbl&gt;\n1 core_polynesia      105. \n2 distant_polynesia    82.5\n\n\nNow from group_means we can calculate the test statistic of the difference in the means\n\ntest_stat &lt;- diff(group_means$ybar)\ntest_stat\n\n[1] -22.5\n\n\nNote: test_stat is the mean of core_polynesia minus the mean of distant_polynesia. The fact that test_stat is negative means that on average there is higher fish species richness in core_polynesia, which aligns with out scientific hypothesis, but will we reject the statistical null or not? For that we need to…\n\n\n3. Generate the null distribution\nRemember a null distribution seeks to capture what our test statistic would look like if the null hypothesis were true. If the null hypothesis were true, then it shouldn’t matter for reef fish richness if we did a survey in Core Polynesia or in Distant Polynesia. That means we can simulate the null distribution by repeatedly shuffling the values of the column polynesia_isolation and then following the same steps for calculating calculating the test statistic. Let’s look at how we would shuffle polynesia_isolation and calculate the test statistic.\n\n# make a new copy of the data for purposes of making\n# the null distribution\npol_fish_null &lt;- pol_tri_fish\n\nNow we can re-make the polynesia_isolation column as a random shuffle of itself. First, have a look at the behavior of the sample function\n\n# make a simple vector of letters\nx &lt;- c(\"A\", \"A\", \"B\", \"B\")\nx\n\n[1] \"A\" \"A\" \"B\" \"B\"\n\n\n\nx &lt;- sample(x)\nx\n\n[1] \"A\" \"B\" \"B\" \"A\"\n\n\nWe started with x being a orderly sequence A A B B and then used sample to generate a random sequence that happens to be A B B A. If we ran sample(x) again, we’d likely get a different random reshuffling of the letters.\nNow let’s use that approach to make our null distribution. For the null distribution we need to make many many many random re-shufflings. But first we’ll look at how we do it just once:\n\n# remember we already made `pol_fish_null` as a copy\n# of the real data\n\n# reshuffle the group identities\npol_fish_null$polynesia_isolation &lt;- sample(pol_fish_null$polynesia_isolation)\n\n# follow the same steps as before for calculating the test statistic\ngroups_null &lt;- group_by(pol_fish_null, polynesia_isolation)\ngroup_means_null &lt;- summarize(groups_null, ybar = mean(richness))\n\ntest_stat_null &lt;- diff(group_means_null$ybar)\ntest_stat_null\n\n[1] -9.088235\n\n\nGreat! Now we just need to do that over and over again! Luckily we can ask R to do the work for us. We can use the replicate function to do the same task over and over. Let’s look at a simple example first.\nSuppose we want to simulate the distribution of outcomes of rolling two dice (and adding their values). We can do one roll like this:\n\ndie1 &lt;- sample(6, 1)\ndie2 &lt;- sample(6, 1)\ndie1 + die2\n\n[1] 9\n\n\nDo do that 20 times, we just copy and paste the above code into replicate:\n\ndice_rolls &lt;- replicate(20, {\n    die1 &lt;- sample(6, 1)\n    die2 &lt;- sample(6, 1)\n    die1 + die2\n})\n\ndice_rolls\n\n [1]  2 10 10 10  9  5 10  7  4  8  5  8  7  7  3  6  4  7  9  9\n\n\nCool, we got 20 random outcomes! Notice that we had to paste the three lines of code into “squiggly brackets” {} within the replicate function. That’s just our way of letting R know that those three lines of code all need to be run together, that the whole set of three lines is what we want R to compute 20 times. With this structure, we could ask R to make 20 replicates, or 20,000!\nNow we can use the same approach to make the null distribution. Let’s do 1000 re-shufelings to make our null distribution.\n\nnull_dist &lt;- replicate(1000, {\n    # reshuffle the group identities\n    pol_fish_null$polynesia_isolation &lt;- \n        sample(pol_fish_null$polynesia_isolation)\n    \n    # follow the same steps as before for calculating the test statistic\n    groups_null &lt;- group_by(pol_fish_null, polynesia_isolation)\n    group_means_null &lt;- summarize(groups_null, ybar = mean(richness))\n    \n    test_stat_null &lt;- diff(group_means_null$ybar)\n    test_stat_null\n})\n\n# the output of `replicate` will be a vector with 1000 null test statistics\n# let's just look at the first little bit\nhead(null_dist)\n\n[1]   1.058824  30.352941 -17.647059   4.588235 -18.264706  -1.235294\n\n\nLet’s visualize the null distribution as a histogram\n\nlibrary(ggplot2)\n\n# notice we need to make a data.frame in order for ggplot to work\nnull_dist_df &lt;- data.frame(null_test_stat = null_dist)\n\nggplot(null_dist_df, aes(x = null_test_stat)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\n\n\n4. Calculate the \\(p\\)-value\nNow we calculate the \\(p\\)-value. Remember, our default assumption is that we are considering a two-tailed alternative hypothesis. So we need to calculate \\(Pr(\\text{lower tail}) + Pr(\\text{upper tail})\\). Our test statistic is -22.5 which is negative, so our lower tail probability will be the proportion of times that null_dist is less than or equal to test_stat, and the upper tail probability will be the proportion of times that null_dist is greater than or equal to -1 * test_stat. Let’s put that in code:\n\nlower_tail &lt;- mean(null_dist &lt;= test_stat)\nupper_tail &lt;- mean(null_dist &gt;= -1 * test_stat)\npval &lt;- lower_tail + upper_tail\npval\n\n[1] 0.043\n\n\nWow cool! our \\(p\\)-value is 0.043. What does that mean?\n\n\n5. Decide: reject \\(H_0\\) or fail to reject \\(H_0\\)\nThat is the question. Since we’re in biostatistics and the convention in our field is to set \\(\\alpha = 0.05\\), then we will reject \\(H_0\\) because \\(p \\le \\alpha\\), i.e., it is true that 0 \\(\\le 0.05\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Null Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lab05.html#questions",
    "href": "lab05.html#questions",
    "title": "5  Statistical Null Hypothesis Testing",
    "section": "5.3 Questions",
    "text": "5.3 Questions\nA common hypothesis in the study of biodiversity is that species richness is higher in the tropics. The tropics are defined as any place between the latitudes of -23.43615° and 23.43615°, i.e. 23.43615° south of the equator and 23.43615° north of the equator. Reef ecosystems largely exist only within the tropics, but there are a few interesting and unique types of reef ecosystems outside the tropical latitudes as well. That means we can use our reef fish dataset to test whether reef species richness is higher in the tropics or not.\nIn questions 1–7 you will conduct the steps of testing this scientific hypothesis with statistical null hypothesis testing.\n\nFirst we need to do some data manipulation to add a column to our reef fish data that tells us if the sites are tropical or temperate (temperate refers to areas outside the tropics). To get you started, recall this is how we can add a column:\nreef_fish$trop_or_temp &lt;- \"tropical\"\nView(reef_fish)\nNow we’ve added one column, but all the values in that column say “tropical”. We need to manipulate that new trop_or_temp column to say “temperate” for all latitudes outside the tropics. Here’s the code to do that for the Northern Hemisphere:\nreef_fish$trop_or_temp[reef_fish$lat &gt;= 23.43615] &lt;- \"temperate\"\nNow your job is to do the same for the Southern Hemisphere.\nNow that we have our column trop_or_temp telling us which group each data point belongs to, we need to state our null and alternative hypotheses\nNow calculate our test statistic\nNow we generate the null distribution. Let’s do that in two steps:\n\nRefer to the code where we shuffled the polynesia_isolation of pol_fish_null. Use that example to make one random reshuffling of the trop_or_temp column and caculate one null test statistic\nNow use the code from (4a.) and refer to the use of the replicate function to simulate a null distribution with 1000 replicates\n\nPlot a histogram of the null distribution\nCalculate the \\(p\\)-value for the two-tailed alternative hypothesis\nDecide whether we reject or fail to reject the null hypothesis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Null Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lab06.html",
    "href": "lab06.html",
    "title": "6  The Binomial Distribution and Analysis of Proportion Data",
    "section": "",
    "text": "6.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Binomial Distribution and Analysis of Proportion Data</span>"
    ]
  },
  {
    "objectID": "lab06.html#goals",
    "href": "lab06.html#goals",
    "title": "6  The Binomial Distribution and Analysis of Proportion Data",
    "section": "",
    "text": "Gain familiarity with the Binomial Distribution\nTest null hypotheses with the Binomial Test\nGenerate and interpret 95% confidence intervals of proportions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Binomial Distribution and Analysis of Proportion Data</span>"
    ]
  },
  {
    "objectID": "lab06.html#learning-the-tools",
    "href": "lab06.html#learning-the-tools",
    "title": "6  The Binomial Distribution and Analysis of Proportion Data",
    "section": "6.2 Learning the Tools",
    "text": "6.2 Learning the Tools\n\n6.2.1 Binomial distribution\nLet’s get familiar with the binomial distribution using a classic example: flipping a coin. We will simulate the coin flipping process using sample:\n\nsample(c(\"H\", \"T\"), size = 1, replace = TRUE, prob = c(0.5, 0.5))\n\n[1] \"T\"\n\n\nThe above code simulates one coin toss that will come up either heads (\"H\") or tails (\"T\"). We are simulating a fair coin, meaning equal probabilities of heads and tails, thus we specify prob = c(0.5, 0.5). We could simulate the outcome of tossing a coin 4 times like this:\n\nsample(c(\"H\", \"T\"), size = 4, replace = TRUE, prob = c(0.5, 0.5))\n\n[1] \"H\" \"T\" \"H\" \"T\"\n\n\nNow let’s modify the code for 4 coin tosses to represent a biased coin, say one that comes up tails 70% of the time\n\nsample(c(\"H\", \"T\"), size = 4, replace = TRUE, prob = c(0.3, 0.7))\n\n[1] \"T\" \"T\" \"T\" \"H\"\n\n\nLet’s arbitrarily say that a “success” is if the coin lands tails up. We can count the number of successes like this:\n\n# get some coin tosses (we're again using a fair coin)\ntoss &lt;- sample(c(\"H\", \"T\"), size = 4, replace = TRUE, prob = c(0.5, 0.5))\ntoss\n\n[1] \"H\" \"T\" \"T\" \"H\"\n\n# count successes\nnsuccess &lt;- sum(toss == \"T\")\nnsuccess\n\n[1] 2\n\n\nThe number of successes in 4 tosses is exactly the kind of situation where a binomial distribution is relevant. In the example of a fair coin tossed 4 times, the probability of success is \\(p = 0.5\\) and the number of trials is \\(n = 4\\).\nLet’s run the above code 20 times to estimate the frequencies of each potential outcome that will approximate the binomial distribution with \\(p = 0.5\\) and \\(n = 4\\).\nTo do that, simply run the above code 20 times and record the number of successes from each iteration. You can write down the success on paper, or the computer, or team up with a partner. Once you’re done, enter those data into a simple vector using the c function. You’re aiming to get something like this (you’re numbers will be different though):\n\n# vector of the number of tails across 20 trials\nntails &lt;- c(1, 2, 2, 1, 0, \n            1, 0, 2, 2, 4, \n            1, 3, 2, 2, 2, \n            1, 3, 3, 2, 2)\n\n# use the `table` function to get frequencies\nfreqs &lt;- table(ntails)\nfreqs\n\nntails\n0 1 2 3 4 \n2 5 9 3 1 \n\n\nLet’s compare our estimated frequencies to the prediction from the binomial distribution. We will use dbinom to get the probability of each outcome directly from the binomial distribution. Remember, that for \\(n = 4\\) trials, all the possible outcomes are 0, 1, 2, 3, 4, which we can write in R code as 0:4:\n\ndbinom(0:4, size = 4, prob = 0.5)\n\n[1] 0.0625 0.2500 0.3750 0.2500 0.0625\n\n\nCompare that to the frequencies divided by 20 (dividing by the number of simulations turns frequencies into probabilities)\n\nfreqs / 20\n\nntails\n   0    1    2    3    4 \n0.10 0.25 0.45 0.15 0.05 \n\n\nRemember your numbers will be different, you might in fact not even have all the outcomes represented in your simulation. What we observe is that there is a general agreement between our simulation and the true probabilities from dbinom: 2 successes is the most probable outcome, 0 and 4 are the least likely.\nLet’s also compare the probability of multiple events as estimated from our simulation and pbinom. Let’s look at the probability of 3 or fewer tails\n\n# calculate the estimated probability\nsum(ntails &lt;= 3) / 20\n\n[1] 0.95\n\n# compare to the probability given by pbinom\npbinom(3, size = 4, prob = 0.5)\n\n[1] 0.9375\n\n\nPretty close!\n\n\n6.2.2 Binomial tests and confidence intervals\nDoing a binomial test in R is much easier than doing one by hand.\nThe function binom.test() will do an exact binomial test. It requires three pieces of information in the input.\n\nx: the number of successes\nn: the number of trials\np: the proportion stated by the null hypothesis.\n\nFor example, let’s consider the hypothetical feeding trial discussed in lecture with pulelehua Kamehameha caterpillars. We imagined we did a feeding trial with 30 replicates between māmaki and Urtica. Out of those 30 trials we imagined the caterpillars choose māmaki 21 times. Let’s test the null hypothesis of no preference using binom.test. We need to tell binom.test the number of successes (x = 21), the number trials (n = 30) and the null hypothesis probability (p = 0.5).\n\nbinom.test(x = 21, n = 30, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  21 and 30\nnumber of successes = 21, number of trials = 30, p-value = 0.04277\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5060410 0.8526548\nsample estimates:\nprobability of success \n                   0.7 \n\n\nIn this case, the output of the function gives quite a bit of information. One key element that we will be looking for is the P-value; in this case R tells us that the P-value is 0.04277. This is the P-value that corresponds to a two-tailed test.\nThe binom.test() function also gives an estimate of the proportion of successes (in this case 0.7). It also gives an approximate 95% confidence interval. The method used to calculate this confidence interval is different from the method we discussed in lecture. When calculating confidence intervals for proportion data, you are welcome to use either the output of binom.test or the method we covered in lecture involving qbinom.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Binomial Distribution and Analysis of Proportion Data</span>"
    ]
  },
  {
    "objectID": "lab06.html#questions",
    "href": "lab06.html#questions",
    "title": "6  The Binomial Distribution and Analysis of Proportion Data",
    "section": "6.3 Questions",
    "text": "6.3 Questions\nLet’s imagine a new feeding trial of pulelehua Kamehameha caterpillars. This time we will imagine a comparison inspired by Bongar et al. (2024). They looked at māmaki versus a close relative that happens to be invasive in Hawaiʻi: Cecropia obtusifolia. If pulelehua Kamehameha caterpillars could thrive on Cecropia obtusifolia that would be great.\nInspired by the results of Bongar et al. (2024), let’s suppose that out of 50 feeding trials where pulelehua Kamehameha caterpillars are given a choice between māmaki and Cecropia obtusifolia, they choose māmaki 38 times.\n\nUse binom.test() to calculate the estimated proportion of choosing māmaki. What is the 95% confidence interval for this proportion? What is the two-tailed \\(P\\)-value of the null hypothesis?\nBased on the results form binom.test you just produced does it seem promising that pulelehua Kamehameha caterpillars will be able to use Cecropia obtusifolia as a food plant?\nBased on the 95% confidence intervals, do you think the proportion of times the caterpillars choose māmaki over Urtica versus the proportion of times they choose māmaki over Cecropia obtusifolia are different from one another? Explain your answer. The hypothetical experiment comparing choice between māmaki and Urtica comes from our lectures, please refer to lecture slides for details on this hypothetical experiment.\nFind your own proportion data on the Internet or at home if you are doing this after lab. Develop a null hypothesis, state it in your report, and use binom.test() to test it. Did you reject your null hypothesis? Explain why or why not.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Binomial Distribution and Analysis of Proportion Data</span>"
    ]
  },
  {
    "objectID": "lab07.html",
    "href": "lab07.html",
    "title": "7  Frequency and Contingency Analysis",
    "section": "",
    "text": "7.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequency and Contingency Analysis</span>"
    ]
  },
  {
    "objectID": "lab07.html#goals",
    "href": "lab07.html#goals",
    "title": "7  Frequency and Contingency Analysis",
    "section": "",
    "text": "Gain familiarity with the \\(\\chi^2\\) Distribution\nCalculate the \\(\\chi^2\\) test statistic and associated degrees of freedom\nTest null hypotheses with the \\(\\chi^2\\) null distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequency and Contingency Analysis</span>"
    ]
  },
  {
    "objectID": "lab07.html#learning-the-tools",
    "href": "lab07.html#learning-the-tools",
    "title": "7  Frequency and Contingency Analysis",
    "section": "7.2 Learning the Tools",
    "text": "7.2 Learning the Tools\n\n7.2.1 The \\(\\chi^2\\) distribution\nLet’s get familiar with the \\(\\chi^2\\) distribution. We’ll also learn about a new kind of R function, one for generating random samples from any probability distribution. Because we’re focused on the \\(\\chi^2\\) distribution, we’ll use the random sampling function for that distribution: rchisq. We can use rchisq to generate a random sample of any size from a \\(\\chi^2\\) distribution with any degrees of freedom:\n\nrchisq(10, df = 3)\n\n [1] 10.029979  4.979061  4.161660  1.515366  1.038495  2.110834 11.342411\n [8]  2.589811  4.038975  4.724242\n\n\nThe above is a random sample of 10 numbers from the \\(\\chi^2\\) distribution with 3 degrees of freedom.\nWe can take a larger sample, put it in a data.frame, and make a histogram to see approximately what the \\(\\chi^2\\) distribution with 3 degrees of freedom looks like:\n\n# generate the sample\nchi2_df3 &lt;- rchisq(1000, df = 3)\n\n# put it in a data.frame\nsamp &lt;- data.frame(chi2 = chi2_df3)\n\n# plot it\nlibrary(ggplot2)\n\nggplot(samp, aes(x = chi2)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nCool! Let’s keep building on this and visualize the histograms of both df = 3 and df = 6. We can do this by adding to our samp data.frame, both extra rows for a random sample with df = 6 and also a column to keep track of the degrees of freedom.\n\n# first add a column for degrees of freedom\nsamp$df &lt;- 3\n\n# now make a new data.frame for df = 6 (next we'll combine the data.frames)\nchi2_df6 &lt;- rchisq(1000, df = 6)\nsamp_df6 &lt;- data.frame(chi2 = chi2_df6, df = 6)\n\n# now add that new data.frame onto `samp`\nsamp &lt;- rbind(samp, samp_df6)\n\n\n# visualize the results \nView(samp)\n\nNow we can make a faceted histogram to see how degrees of freedom impacts the shape of the \\(\\chi^2\\) distribution\n\nggplot(samp, aes(x = chi2)) +\n    geom_histogram() +\n    facet_grid(rows = vars(df))\n\n\n\n\n\n\n\n\nAs we expected, the bigger the degrees of freedom, the more the distribution shifts to the right.\nLet’s also calculate some probabilities from these random samples. Let’s calculate the probability of observing a \\(\\chi^2\\) test statistic of 7.8 or greater given 3 degrees of freedom\n\n# recall probability is just the proportion of times an event happens\nn_event &lt;- sum(chi2_df3 &gt;= 7.8)\nn_event / length(chi2_df3)\n\n[1] 0.05\n\n\nSo the probability \\(Pr(\\chi^2 \\ge 7.8 | df = 3) \\approx\\) 0.05. We are working with random samples here, so your numbers will likely be a little different. If \\(Pr(\\chi^2 \\ge 7.8 | df = 3) \\approx\\) 0.05 seems suspiciously close to 0.05 that’s because I choose 7.8 very deliberately. 7.8 is approximately the critical value for a \\(\\chi^2\\) distribution with 3 degrees of freedom.\nRecall that a critical value is the value that cuts a probability distribution at the desired \\(\\alpha\\), AKA significance level. To calculate the critical value we use qchisq:\n\n# this returns the value that divides a chi-sqrd distribution\n# into 95% to the left, 5% to the right\nqchisq(0.95, df = 3)\n\n[1] 7.814728\n\n# this does the same\nqchisq(0.05, df = 3, lower.tail = FALSE)\n\n[1] 7.814728\n\n\n\n\n7.2.2 \\(\\chi^2\\) goodness of fit test\nHere’s a question: does the representation of different ethnicities in the faculty at the University of Hawaiʻi look like the ethnic composition of Hawaiʻi as a whole? If not, then something funny might be going on like racist hiring practices, or systemic racism leading to discrepancies in higher education leading to different proportions of qualifications (e.g. holding a graduate degree) across ethnicities.\nThere can also be reasonable causes for differential ethnic representation, such as if UH wants to have thriving Asian Studies or Black/African Studies or Hawaiian Studies program, we will need to hire qualified faculty for those positions who will likely come from those ethnic backgrounds more often than not.\nIt should be noted that discrepancies in ethnic group representation in skilled labor jobs is sometimes used to claim there are “biological” differences between ethnic groups in terms of intelligence. This is false. A great read on this topic is Superior by Angela Saini. I wish I could say that “historically there were claims about ‘biological’ differences…” but these claims persist all too readily into our modern times.\nSo let’s get to it!\nHere are the data on ethnicities in Hawaiʻi and counts of different ethnic groups who sit on our faculty at UH:\n\n\n\n\n\n\n\n\n\n\nethnicity\nstate_pop_proportion\nnumber_uh_faculty\n\n\n\n\nAmerican Indian and Alaska Native\n0.016\n41\n\n\nBlack or African American\n0.025\n63\n\n\nChinese\n0.100\n349\n\n\nFilipino\n0.178\n247\n\n\nJapanese\n0.162\n544\n\n\nKorean\n0.024\n135\n\n\nNative Hawaiian\n0.152\n453\n\n\nOther Pacific Islander\n0.030\n42\n\n\nVietnamese\n0.007\n18\n\n\nWhite\n0.306\n1670\n\n\n\n\n\nDemographic data for Hawaiʻi come from the state government report “Demographic, Social, Economic, and Housing Characteristics for Selected Race Groups in Hawaii” and the data for faculty by ethnic group come from the UH Mānoa Institutional Research Office. Because those two reporting bodies divide ethnicities slightly differently or do not report on others I had to leave some groups out entirely (e.g. Indian, not reported by the state government).\nThese data are in your data folder in Koa and are called uh_faculty_ethnicity.csv\nHow would we use a \\(\\chi^2\\) goodness of fit test for these data? We’re going to leave that to you to answer in the Questions section. Here, we’ll look at a simpler example. First let’s make-up that example. Suppose we have data on 4 groups A, B, C, D, and their expected proportions. We can make such a data set like this:\n\nfake_data &lt;- data.frame(group = c(\"A\", \"B\", \"C\", \"D\"), \n                        prop = c(0.15, 0.5, 0.05, 0.3), \n                        count = c(15, 17, 10, 8))\n\n# have a look\nfake_data\n\n  group prop count\n1     A 0.15    15\n2     B 0.50    17\n3     C 0.05    10\n4     D 0.30     8\n\n\nNow we can walk through the steps of calculating a \\(\\chi^2\\) statistic and comparing it to the null distribution.\n\n7.2.2.1 Calculate the \\(\\chi^2\\) statistic\nFirst we need the expected counts\n\n# just the expected proportion times total number of observations\nfake_data$expected &lt;- fake_data$prop * sum(fake_data$count)\n\nfake_data\n\n  group prop count expected\n1     A 0.15    15      7.5\n2     B 0.50    17     25.0\n3     C 0.05    10      2.5\n4     D 0.30     8     15.0\n\n\nWe can see that we’re not violating the assumptions of the \\(\\chi^2\\) distribution so let’s calculate the \\(\\chi^2\\) test statistic:\n\nchi2stat &lt;- sum((fake_data$count - fake_data$expected)^2 / \n                    fake_data$expected)\n\n\n\n7.2.2.2 Identify the null distribution\nWe’re dealing with frequency data, and doing a lab about \\(\\chi^2\\) distributions, so our null distribution is a \\(\\chi^2\\) distribution, but with what degrees of freedom? There are 4 groups, so df = 3.\n\n\n7.2.2.3 Calculate the \\(P\\)-value and decide if we reject the null\n\npchisq(chi2stat, df = 3, lower.tail = FALSE)\n\n[1] 8.147653e-08\n\n\nThat’s a tiny \\(P\\)-value, so yes, we reject the null at an \\(\\alpha = 0.05\\) level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequency and Contingency Analysis</span>"
    ]
  },
  {
    "objectID": "lab07.html#contingency-analysis",
    "href": "lab07.html#contingency-analysis",
    "title": "7  Frequency and Contingency Analysis",
    "section": "7.3 Contingency analysis",
    "text": "7.3 Contingency analysis\nContingency analysis is very similar to a \\(\\chi^2\\) goodness of fit test, but in contingency analysis we have 2 categorical variables. We still compare the observed frequencies with the expected using a \\(\\chi^2\\) test statistic, and we still use a \\(\\chi^2\\) distribution for the null distribution.\nOur question is often a little different: with contingency analysis we are often asking, is there an association between these two categorical variables?\nFor example, do manu o Kū prefer nesting in certain types of trees? Manu o Kū are the friendly (yes I am anthropomorphizing) little white terns that fly around campus. A few years back, students went out and looked in random trees to see if the manu o Kū were nesting in the trees. Do the manu prefer a certain species? Here the two categorical variables are tree species and yes/no is there a nest. Here is a peek at the data\n\n\n\n\n\ntree\nnest\n\n\n\n\nAlbizia\nno\n\n\nKukui\nno\n\n\nCassia\nno\n\n\nKukui\nno\n\n\nAlbizia\nno\n\n\nCassia\nyes\n\n\n\n\n\nThat is just the first few rows, there are 183 rows in total. How do we do a \\(\\chi^2\\) contingency analysis with these kind of data? Again, let’s get there using a simpler fake dataset\n\nfake_2var &lt;- data.frame(group1 = sample(c(\"A\", \"B\"), 25, replace = TRUE), \n                        yes_no = sample(c(\"yes\", \"no\"), 25, replace = TRUE))\n\nfake_2var\n\n   group1 yes_no\n1       A     no\n2       B     no\n3       A    yes\n4       A     no\n5       B     no\n6       B     no\n7       A     no\n8       A     no\n9       A     no\n10      A     no\n11      B    yes\n12      A    yes\n13      A     no\n14      B     no\n15      B    yes\n16      B     no\n17      B     no\n18      A    yes\n19      B     no\n20      A     no\n21      A    yes\n22      B    yes\n23      B    yes\n24      B    yes\n25      B    yes\n\n\nWe will use the chisq.test function for the contingency analysis rather than calculate everything by hand. The function chisq.test can be used in one of two ways.\nFirst, we can calculate the contingency table from the data and pass that table to chisq.test:\n\n# calculate the contingency table\nfake_tab &lt;- table(fake_2var)\n\n# have a look, remember this is random so yours may be different\nfake_tab\n\n      yes_no\ngroup1 no yes\n     A  8   4\n     B  7   6\n\n\nNow we can pass this to chisq.test\n\nchisq.test(fake_tab)\n\nWarning in chisq.test(fake_tab): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  fake_tab\nX-squared = 0.060096, df = 1, p-value = 0.8063\n\n\nNotice, we are again getting a warning that the assumptions of the \\(\\chi^2\\) test might not be met. That’s ok, this is just an example.\nThe other way we can use the chisq.test function is by directly passing it the categorical variables\n\nchisq.test(fake_2var$group1, fake_2var$yes_no)\n\nWarning in chisq.test(fake_2var$group1, fake_2var$yes_no): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  fake_2var$group1 and fake_2var$yes_no\nX-squared = 0.060096, df = 1, p-value = 0.8063\n\n\nThat really couldn’t be simpler! Under the hood, chisq.test is calculating the contingency table for us.\nWe might as well touch on what to do in real life if you get the warning about the \\(\\chi^2\\) test assumptions not being met. You can instead use a test called “Fisher’s exact test.” This approach is more computationally intensive than the \\(\\chi^2\\) test, so back in the day people didn’t like doing it, but now there’s really no harm. The code is the same too, just swap out the function name:\n\nfisher.test(fake_2var$group1, fake_2var$yes_no)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  fake_2var$group1 and fake_2var$yes_no\np-value = 0.6882\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.2610602 11.8938828\nsample estimates:\nodds ratio \n  1.677387 \n\n\nYou can see it tells us some extra stuff, but our primary concern for now is the \\(P\\)-value, which is still very non-significant.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequency and Contingency Analysis</span>"
    ]
  },
  {
    "objectID": "lab07.html#questions",
    "href": "lab07.html#questions",
    "title": "7  Frequency and Contingency Analysis",
    "section": "7.4 Questions",
    "text": "7.4 Questions\n\nKeep building on the code you ran to make samp. Add another random sample of 1000 numbers, but this time from a \\(\\chi^2\\) distribution with 9 degrees of freedom.\nMake a faceted histogram with all three distributions captured in samp.\nCalculate the critical value for \\(\\alpha = 0.05\\) for the \\(\\chi^2\\) distribution with 9 degrees of freedom\nUse R code to estimate the probability from your random sample from the \\(\\chi^2\\) distribution with 9 degrees of freedom of values greater than or equal to the critical value you just calculated\nFollowing the steps we used to do a \\(\\chi^2\\) goodness of fit test for fake_data, preform a \\(\\chi^2\\) goodness of fit test to answer the question of whether the ethnic composition of UH faculty resembles the ethnic composition of Hawaiʻi. What do you conclude?\nFollowing the steps we used to do a contingency analysis for fake_2var, preform a contingency analysis for the manu o Kū nesting data. Do the manu have a preference for which type of tree to nest in, or do they seem to choose at random? The manu o Kū data are in your data folder and called manuoku.csv.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequency and Contingency Analysis</span>"
    ]
  },
  {
    "objectID": "lab08.html",
    "href": "lab08.html",
    "title": "8  The normal distribution and sample means",
    "section": "",
    "text": "8.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The normal distribution and sample means</span>"
    ]
  },
  {
    "objectID": "lab08.html#goals",
    "href": "lab08.html#goals",
    "title": "8  The normal distribution and sample means",
    "section": "",
    "text": "Visualize properties of the normal distribution.\nUnderstand the Central Limit Theorem.\nCalculate sampling properties of sample means.\nDecide whether a data set likely comes from a normal distribution",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The normal distribution and sample means</span>"
    ]
  },
  {
    "objectID": "lab08.html#learning-the-tools",
    "href": "lab08.html#learning-the-tools",
    "title": "8  The normal distribution and sample means",
    "section": "8.2 Learning the Tools",
    "text": "8.2 Learning the Tools\nThis week we will mainly focus on some exercises to better understand the nature of the normal distribution. We will also learn a couple of tools that help us decide whether a particular data set is likely to have come from population with an approximately normal distribution.\nMany statistical tests assume that the variable being analyzed has a normal distribution. Fortunately, many of these tests are fairly robust to this assumption—that is, they work reasonably well even when this assumption is not quite true, especially when sample size is large. Therefore it is often sufficient to be able to assess whether the data come from a distribution whose shape is even approximately normal (the bell curve).\nA good way to start is to simply visualize the frequency distribution of the variable in the data set by drawing a histogram. Let’s use the age of passengers on the Titanic for our example.\n\n# remember to insert your actual user name\ntitanic_data &lt;- read.csv(\"biol220_class/students/username/data/titanic.csv\" )\n\nRemember we can use ggplot() to draw histograms.\n\nggplot(titanic_data, aes(x = age)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\nLooking at this histogram, we see that the frequency distribution of the variable is not exactly normal; it is slightly asymmetric and there seems to be a second mode near 0. On the other hand, like the normal distribution, the frequency distribution has a large mode near the center of the distribution, frequencies mainly fall off to either side, and there are no outliers. This is close enough to normal that most methods would work fine.\n\n8.2.1 QQ Plot\nAnother graphical technique that can help us visualize whether a variable is approximately normal is called a quantile plot (or a QQ plot). The QQ plot shows the data on the vertical axis ranked in order from smallest to largest (“sample” in the figure below). On the horizontal axis, it shows the expected value of an individual with the same quantile if the distribution were normal (“theoretical” in the same figure). The QQ plot should follow more or less along a straight line if the data come from a normal distribution (with some tolerance for sampling variation).\nQQ plots can be made in R using a function called geom_qq(). Add geom_qq_line() to draw a line through that QQ plot to make the linear relationship easier to see. The only weird thing about these geom’s is that you need to specify sample in the aes() function.\n\nggplot(titanic_data, aes(sample = age)) + \n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nThis is what the resulting graph looks like for the Titanic age data. The dots do not land along a perfectly straight line. In particular the graph curves at the upper and lower end. However, this distribution definitely would be close enough to normal to use most standard methods, such as the t-test.\nIt is difficult to interpret QQ plots without experience. One of the goals of today’s exercises will be to develop some visual experience about what these graphs look like when the data is truly normal. To do that, we will take advantage of a function built into R to generate random numbers drawn from a normal distribution. This function is called rnorm().\nThe function rnorm() will return a vector of numbers, all drawn randomly from a normal distribution. It takes three arguments:\n\nn: how many random numbers to generate (the length of the output vector)\nmean: the mean of the normal distribution to sample from\nsd: the standard deviation of the normal distribution\n\nFor example, the following command will give a vector of 20 random numbers drawn from a normal distribution with mean 13 and standard deviation 4:\n\nrnorm(n = 20, mean = 13, sd = 4)\n\n [1]  9.685629  4.800011 20.336418 11.520671 12.384563 14.584179  8.735137\n [8]  7.874422 14.468579 18.113172 13.373571 15.023412 12.691000 16.354063\n[15] 15.838021 10.305846 19.691060 11.730532 16.770999 11.503842\n\n\nLet’s look at a QQ plot generated from 100 numbers randomly drawn from a normal distribution:\n\nnormal_data &lt;- data.frame(Y = rnorm(n = 100, mean = 13, sd = 4))\nggplot(normal_data, aes(sample = Y)) + \n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nThese points fall mainly along a straight line, but there is some wobble around that line even though these points were in fact randomly sampled from a known normal distribution. With a QQ plot, we are looking for an overall pattern that is approximately a straight line, but we do not expect a perfect line. In the exercises, we’ll simulate several samples from a normal distribution to try to build intuition about the kinds of results you might get.\nWhen data are not normally distributed, the dots in the quantile plot will not follow a straight line, even approximately. For example, here is a histogram and a QQ plot for the population size of various counties, from the data in wright_etal_2017.csv. These data are very skewed to the right, and do not follow a normal distribution at all.\n\n# remember to insert your actual user name\ntitanic_data &lt;- read.csv(\"biol220_class/students/username/data/wright_etal_2017.csv\" )\n\n\nggplot(leafsize, aes(x = leafsize_cm2)) + \n  geom_histogram()\n\n\n\n\n\n\n\nggplot(leafsize, aes(sample = leafsize_cm2)) + \n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\n\n\n8.2.2 Transformations\nWhen data are not normally distributed, we can try to use a simple mathematical transformation on each data point to create a list of numbers that still convey the information about the original question but that may be better matched to the assumptions of our statistical tests. We’ll see more about such transformations in chapter 13 of Whitlock and Schluter, but for now let’s learn how to do one of the most common data transformations, the log-transformation.\nWith a transformation, we apply the same mathematical function to each value of a given numerical variable for individual in the data set. With a log-transformation, we take the logarithm of each individual’s value for a numerical variable.\nWe can only use the log-transformation if all values are greater than zero. Also, it will only improve the fit of the normal distribution to the data in cases when the frequency distribution of the data is right-skewed.\nTo take the log transformation for a variable in R is very simple. We simply use the function log(), and apply it to the vector of the numerical variable in question. For example, to calculate the log of age for all passengers on the Titanic, we use the command:\n\nlog(titanic_data$age)\n\nThis will return a vector of values, each of which is the log of age of a passenger.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The normal distribution and sample means</span>"
    ]
  },
  {
    "objectID": "lab08.html#questions",
    "href": "lab08.html#questions",
    "title": "8  The normal distribution and sample means",
    "section": "8.3 Questions",
    "text": "8.3 Questions\n\nLet’s use R’s random number generator for the normal distribution to build intuition for how to view and interpret histograms and QQ plots. Remember, the lists of values generated by rnorm() come from a population that truly have a normal distribution.\n\nGenerate a list of 10 random numbers from a normal distribution with mean 15 and standard deviation 3, using the following command:\nnormal_data &lt;- data.frame(Y = rnorm(n = 10, mean = 15, sd = 3))\nUse geom_histogram() to plot a histogram of these numbers from part a. You will want to change binwidth for visual clarity.\nPlot a QQ plot from the numbers in part a.\nRepeat a through c several times (at least a dozen times). For each, look at the histograms and QQ plots. Think about the ways in which these look different from the expectation of a normal distribution and remember that each of these samples comes from a truly normal population.\n\nRepeat the procedures of Question 1, except this time have R sample 250 individuals for each sample. You can use the same command as in Question 1, but now set n = 250. Do the graphs and QQ plots from these larger samples look more like the normal expectations than the smaller sample you already did? Why do you think that this is?\nThe file “mammals.csv” (in the data folder) contains information on the body mass of various mammal species.\n\nUse ggplot() to plot the distribution of body mass, and describe its shape. Does this look like it has a normal distribution?\nUse geom_qq() and geom_qq_line() to plot a QQ plot for body mass. Does the data fall approximately along a straight line in the QQ plot? If so, what does this imply about the fit of these data to a normal distribution?\nTransform the body mass data with a log-transformation. Repeat steps (a) and (b) on the transformed data, does the log-transformation bring the data closer to normal or further from it?\nCalculate the mean of log body mass and a 95% confidence interval for this mean. (You may want to refer back to Lab 4 for the R commands to do this.)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The normal distribution and sample means</span>"
    ]
  },
  {
    "objectID": "lab09.html",
    "href": "lab09.html",
    "title": "9  Correlation",
    "section": "",
    "text": "9.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "lab09.html#goals",
    "href": "lab09.html#goals",
    "title": "9  Correlation",
    "section": "",
    "text": "Calculate a correlation coefficient and the coefficient of determination\nTest hypotheses about correlation\nUse the rank-based correlation when normality assumptions are not met",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "lab09.html#learning-the-tools",
    "href": "lab09.html#learning-the-tools",
    "title": "9  Correlation",
    "section": "9.2 Learning the Tools",
    "text": "9.2 Learning the Tools\nThis week and next week we will look at methods to understand the relationship between two numerical variables, using correlation and regression.\nTo demonstrate the new R commands this week, we will use the penguin data set from the palmerpenguins package in R. These data record the measurements of different body dimensions of three species of penguins. For simplicity, we will focus just on the chinstrap penguin. Let’s first load the package and subset the data to just chinstraps.\n\nlibrary(palmerpenguins)\nchinstrap &lt;- subset(penguins, species == \"Chinstrap\")\n\nLet’s look at the correlation between body mass and flipper length. We might expect the heavier the penguin, the bigger the flippers it needs. But before we go further, it is wise to plot a scatterplot to view the relationship of the two variables.\n\nggplot(chinstrap, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nThese data seem to have a moderately strong, positive relationship. Note: this is just an exploratory plot, we haven’t made it look high quality, we’re just getting a sense of what the data look like.\nCalculating a correlation coefficient in R is straightforward. The function cor() calculates the correlation between the two variables given as input:\n\ncor(chinstrap$body_mass_g, chinstrap$flipper_length_mm)\n\n[1] 0.6415594\n\n\nAs we predicted from the graph, the correlation coefficient of these data is positive and fairly strong.\nTo test a hypothesis about the correlation coefficient or to calculate its confidence interval, use cor.test(). It takes as input the names of vectors containing the variables of interest.\n\ncor.test(chinstrap$body_mass_g, chinstrap$flipper_length_mm)\n\n\n    Pearson's product-moment correlation\n\ndata:  chinstrap$body_mass_g and chinstrap$flipper_length_mm\nt = 6.7947, df = 66, p-value = 3.748e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4759352 0.7632368\nsample estimates:\n      cor \n0.6415594 \n\n\nThe output gives many bits of information we might want. After, re-stating the names of the variables being used, the output gives us the test statistic t, degrees of freedom, and P-value of a test of the null hypothesis that the population correlation coefficient is zero. In this case the P-value is quite small, \\(P = 3.748 \\times 10^{-9}\\). After that we have the 95% confidence interval for the correlation coefficient and finally the estimate of the correlation coefficient itself.\nThe name R gives this correlation test is unfortunate. Pearson of course was a terrible eugenicist, and not the only person in the world to ever come up with the idea of correlation. At the very least we know the Indian statistician Anil Kumar Gain developed the same idea. For these reasons, despite the naming convention in R, we will call this type of correlation Product-moment correlation. This is a more descriptive name because the mathematical form of this correlation coefficient comes from multiplying the moments of the data—in statistics, moments are any expression like this \\(\\sum (Y_i - \\bar{Y})^b\\) where \\(b\\) can be any integer. For correlation \\(b = 2\\).\n\n9.2.1 Rank-based correlation\nThe function cor.test() can also calculate a rank-based correlation, if we add the option method = \"spearman\" to the command. Rank-based correlation is helpful when the data are not normally distributed and/or when the relationship between variables is not linear. This is because it looks for a correlation between the ranks (first biggest, second biggest, etc.) of the data values rather than the values themselves.\n\ncor.test(chinstrap$body_mass_g, chinstrap$flipper_length_mm, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  chinstrap$body_mass_g and chinstrap$flipper_length_mm\nS = 17264, p-value = 3.983e-10\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6704871 \n\n\nThe output here is similar to what we described above with product-moment correlation. We call this kind of correlation rank-based because that is more descriptive, and again removes the name of a eugenicist (Spearman) who just happened to receive historical credit for this type of analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "lab09.html#activities",
    "href": "lab09.html#activities",
    "title": "9  Correlation",
    "section": "9.3 Activities",
    "text": "9.3 Activities\n\n9.3.1 Developing an intuition for correlation coefficients.\nIn a web browser, open the app at http://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/\nThis app is simple, it will plot some data in a scatterplot, and you guess the correct correlation coefficient for those data. Select one of the three choices and click the little circle next to your choice. Most people find this pretty challenging at first, but that is the point—to let you develop a better intuition about what a given value of a correlation coefficient means for how strong a relationship is between two numerical variables.\nKeep trying new data sets (by clicking the “Simulate new data” button) until you feel like you can get it right most of the time.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "lab09.html#questions",
    "href": "lab09.html#questions",
    "title": "9  Correlation",
    "section": "9.4 Questions",
    "text": "9.4 Questions\n1. Telomeres length and aging\nThe ends of chromosomes are called telomeres. These telomeres are shortened a bit during each cell cycle as DNA is replicated. One of their purposes is to protect more valuable DNA in the chromosome from degradation during replication. As people get older and their cells have replicated more often, their telomeres shorten. There is evidence that these shortened telomeres may play a role in aging. Telomeres can be lengthened in germ cells and stem cells by an enzyme called telomerase, but this enzyme is not active in most healthy somatic cells. (Cancer cells, on the other hand, usually express telomerase.)\nGiven that the length of telomeres is biologically important, it becomes interesting to know whether telomere length varies between individuals and whether this variation is inherited. A set of data was collected by Nordfjäll et al. (2005) on the telomere length of fathers and their children; these data are in the file “telomere-inheritance.csv”.\n\nCreate a scatter plot showing the relationship between father and offspring telomere length.\nDo the data require any transformation to be bivariate normal before correlation?\nWhat’s the product-moment correlation between father and offspring telomere length? What’s the null hypothesis? Do you reject or fail to reject the null hypothesis?\n\n2. Brain-body mass allometry in mammals\nLarger animals tend to have larger brains. But is the increase in brain size proportional to the increase in body size? A set of data on body and brain size of 62 mammal species was collated by Allison and Cicchetti (1976), and these data are in the data set “mammals.csv”. The file contains columns giving the species name, the average body mass (in kg) and average brain size (in g) for each species.\n\nPlot brain size against body size. Is the relationship linear?\nFind a transformation (for either or both variables) that makes the relationship between these two variables linear.\nIs there statistical evidence that brain size is correlated with body size? Assume that the species data are independent.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "lab10.html",
    "href": "lab10.html",
    "title": "10  Regerssion",
    "section": "",
    "text": "10.1 Goals",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regerssion</span>"
    ]
  },
  {
    "objectID": "lab10.html#goals",
    "href": "lab10.html#goals",
    "title": "10  Regerssion",
    "section": "",
    "text": "Estimate slopes of regressions\nTest regression models\nPlot regression lines\nExamine residual plots for deviations from the assumptions of linear regression",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regerssion</span>"
    ]
  },
  {
    "objectID": "lab10.html#learning-the-tools",
    "href": "lab10.html#learning-the-tools",
    "title": "10  Regerssion",
    "section": "10.2 Learning the Tools",
    "text": "10.2 Learning the Tools\nWe will revisit the data set from Example 2.3B in Whitlock and Schluter that we used last week to introduce correlation. These data investigate the relationship between how ornamented a father guppy is (fatherOrnamentation) and how attractive to females are his sons (sonAttractiveness). Load the data from the .csv file:\n\nguppy_data &lt;- read.csv(\"biol220_class/students/username/data/chap02e3bGuppyFatherSonAttractiveness.csv\")\n\nLet’s plot the data again before we start.\n\nggplot(guppy_data, aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n  geom_point() +\n  xlab(\"Father's ornamentation\") + \n  ylab(\"Son's attractiveness\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNote that these data seem to have a moderately strong, positive relationship.\n\n10.2.1 Linear regression\nRegression in R is a two-step process similar to the steps used in ANOVA last week. In fact, we again start by using lm() to fit a linear model to the data. (Both ANOVA and regression are special cases of linear models, which also can be used to generate much more complicated analyses than these.) We then give the output of lm() to the function summary() to see many useful results of the analysis.\nUsing the lm() function to calculate regression is similar to the steps used for ANOVA. The first argument is a formula, in the form response_variable ~ explanatory_variable. In this case we want to predict son’s attractiveness from father’s ornamentation, so our formula will be sonAttractiveness ~ fatherOrnamentation. The second input argument is the name of the data frame with the data. We will want to assign the results to a new object with a name (we chose “guppy_regression”), so that we can use the results in later calculations with summary().\n\nguppy_regression &lt;- lm(sonAttractiveness ~ fatherOrnamentation, data = guppy_data)\n\n# Let’s look at the output of lm() in this case.\nguppy_regression\n\n\nCall:\nlm(formula = sonAttractiveness ~ fatherOrnamentation, data = guppy_data)\n\nCoefficients:\n        (Intercept)  fatherOrnamentation  \n           0.005084             0.982285  \n\n\nThis tells us that the estimate of the slope of the regression line is 0.982285, and the y-intercept is estimated to be 0.005084. Therefore the line that is estimated to be the best fit to these data is\n\\[\\text{sonAttractiveness} = (0.982285 \\times \\text{fatherOrnamentation} + 0.005084.\\] We can find other useful information by looking at the summary() of the lm() result:\n\nsummary(guppy_regression)\n\n\nCall:\nlm(formula = sonAttractiveness ~ fatherOrnamentation, data = guppy_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66888 -0.14647 -0.02119  0.27727  0.51324 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.005084   0.118988   0.043    0.966    \nfatherOrnamentation 0.982285   0.216499   4.537 6.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3212 on 34 degrees of freedom\nMultiple R-squared:  0.3771,    Adjusted R-squared:  0.3588 \nF-statistic: 20.59 on 1 and 34 DF,  p-value: 6.784e-05\n\n\nWe see the estimates of the slope and intercept repeated here, in the Coefficients table under Estimate. Now, we also are given the standard error and P-value for each of these numbers in that same table. For these data, the P-value for the null hypothesis that the true slope is zero is \\(6.78 \\times 10 ^ {–5}\\).\nPlotting this line on the scatterplot is fairly straightforward in ggplot(). We can use the same plot function as above, with a new layer added with “+ geom_smooth(method = lm)”:\n\nggplot(guppy_data, aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n  geom_point() +\n  xlab(\"Father's ornamentation\") + \n  ylab(\"Son's attractiveness\") + \n  geom_smooth(method = lm) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis layer adds both the best-fitting regression line and also the 95% confidence interval for the line shown in gray shading. The outer edges of the shaded area represent the confidence bands, indicating the 95% confidence intervals for the mean of the Y-variable (son’s attractiveness) at each value of the X-variable (father’s ornamentation). If you want a plot without this confidence interval, add the argument se = FALSE to the geom_smooth() function, as in geom_smooth(method = lm, se = FALSE).\n\n\n10.2.2 Residual plots\nTo check that the assumptions of regression apply for your data set, it is can be really helpful to look at a residual plot. A residual is the difference between the actual value of the y variable and the predicted value based on the regression line.\nR can calculate the residuals from a model with the residuals() function. Simply give this function the results from the lm() function, such as the guppy_regression that we calculated above. A vector of all the residuals for this regression line would be calculated by\n\nresiduals(guppy_regression)\n\nTo make a residual plot, we’ll make a new column called guppy_data$resid. As you long as you’re careful, the order of points in residuals(guppy_regression) matches the order of the original data, but it’s to mess this up if you don’t know what you’re doing. With a residual plot, we plot the residuals of each data point as a function of the explanatory variable.\n\nguppy_data$resid &lt;- residuals(guppy_regression)\n\nggplot(guppy_data, aes(x = fatherOrnamentation, y = resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe command + geom_hline(yintercept = 0) adds the horizontal line at 0 to the plot so that it is easier to see the baseline.\nThis residual plot shows no major deviation from the assumptions of linear regression. There is no strong tendency for the variance of the residuals (indicated by the amount of scatter in the vertical dimension) to increase or decrease with increasing \\(x\\). The residuals show no outliers or other evidence of not being normally distributed for each value of \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regerssion</span>"
    ]
  },
  {
    "objectID": "lab10.html#questions",
    "href": "lab10.html#questions",
    "title": "10  Regerssion",
    "section": "10.3 Questions",
    "text": "10.3 Questions\n\n10.3.1 1. Numerical estimation in grade schoolers\nOpfer and Siegler (2007) asked second- and fourth-grade school children to mark on a number line where a given number would fall. Each child was given a drawing of a number line with two ends marked at 0 and 1000, and was then asked to make an X on that line where a number, for example 150, should be placed. They asked each child to place several different numbers on the number lines, each on a fresh new piece of paper.\n\nThe researchers then measured the placement of each mark on a linear scale. The results, averaged over all 93 kids for each group, are given in the file “numberline.csv”.\n\nPlot the fourth graders’ guesses against the true value. Is this relationship linear? If not, find a transformation of \\(X\\) or \\(Y\\) that converts the relationship into an approximately linear one.\nPlot the second-graders’ guesses against the true value. Is this relationship linear? If not, find a transformation of \\(X\\) or \\(Y\\) that converts the relationship into an approximately linear one. Fit a linear regression to both the transformed and untransformed data. Examine the residual plots for both the transformed and untransformed data.\n\nHint: if you get stuck on which transformation to use, unhide the plot below to reveal curves fit to both relationships without any transformation.\n\n\nShow figure\n\n\n\n\n\n\n\n\n\n\n\n\nAssume that the difference between the shapes of these curves is real. What would you conclude about the difference in the way 2nd graders and 4th graders perceive numbers?\n\n\n\n10.3.2 2. Brain-body mass allometry in mammals\nLarger animals tend to have larger brains. But is the increase in brain size proportional to the increase in body size? A set of data on body and brain size of 62 mammal species was collated by Allison and Cicchetti (1976), and these data are in the data set “mammals.csv”. The file contains columns giving the species name, the average body mass (in kg) and average brain size (in g) for each species. These are the same data used in the second half of the app about residuals that you used in the activities earlier in this lab.\n\nPlot transformed brain size against body size using code from last week\nWhat line best predicts (transformed) brain size from (transformed) body size?\nMake a residual plot using the regression fitted to the transformed variables. Do the data look like they match the assumptions of linear regression?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regerssion</span>"
    ]
  },
  {
    "objectID": "lab11.html",
    "href": "lab11.html",
    "title": "11  Analysis of Variance",
    "section": "",
    "text": "11.1 Learning the Tools\nFor the examples in this tutorial, we will again return to the Titanic data set. We’ll group passengers by the passenger class they traveled under (a categorical variable) and ask whether different passenger classes differed in their mean age (a numerical variable).\ntitanic_data &lt;- read.csv(\"biol220_class/students/username/data/titanic.csv\")\nLet’s first look at the data to get a sense of how well it fits the assumptions of ANOVA. Multiple histogram are useful for this purpose. As we saw in previous weeks, we can use ggplot() and facets to make this plot:\nggplot(titanic_data, aes(x = age)) +\n  geom_histogram(fill = \"tomato\", color = \"black\") +\n  facet_wrap(~ passenger_class, ncol = 1) +\n  theme_bw()\nThese data look sufficiently normal and with similar spreads that ANOVA would be appropriate.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "lab11.html#learning-the-tools",
    "href": "lab11.html#learning-the-tools",
    "title": "11  Analysis of Variance",
    "section": "",
    "text": "11.1.1 ANOVA\nAnalysis of variance (or ANOVA) for categorical explanatory data works very similarly to ANOVA applied to regression analysis. In both cases, we use the lm function to make a model. For ANOVA with categorical data we then supply the output of the lm function to the anova function which completes the analysis of variance.\nAs we know, the function lm() needs a formula and a data frame as arguments. The formula always takes the form of a response variable, followed by a tilde (~), and then at least one explanatory variable. In the case of a one-way ANOVA, this model statement will take the form\n\nnumerical_variable ~ categorical_variable\n\nFor example, to compare differences in mean age among passenger classes on the Titanic, this formula is:\n\nage ~ passenger_class\n\nThis formula tells R to “fit” a model in which the ages of passengers are grouped by the variable passenger_class.\nThe name of the data frame containing the variables stated in the formula is the second argument of lm(). Finally, to complete the lm() command, it is necessary to save the intermediate results by assigning them to a new object, which anova() can then use to make the ANOVA table. For example, here we assign the results of lm() to a new object named “titanicANOVA”:\n\ntitanicANOVA &lt;- lm(age ~ passenger_class, data = titanic_data)\n\nThe function anova() takes the results of lm() as input and returns an ANOVA table as output:\n\nanova(titanicANOVA)\n\nAnalysis of Variance Table\n\nResponse: age\n                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npassenger_class   2  26690 13344.8  75.903 &lt; 2.2e-16 ***\nResiduals       630 110764   175.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis table shows the results of a test of the null hypothesis that the mean ages are the same among the three groups. The P-value is very small, and so we reject the null hypothesis of no differences in mean age among the passenger classes.\n\n\n11.1.2 Tukey-Kramer test\nA single-factor ANOVA can tell us that at least one group has a different mean from another group, but it does not inform us which group means are different from which other group means. A Tukey-Kramer test lets us test the null hypothesis of no difference between the population means for all pairs of groups. The Tukey-Kramer test (also known as a Tukey Honest Significance Test, or Tukey HSD), is implemented in R in the function TukeyHSD().\nWe will use the results of an ANOVA done with lm() as above, that we stored in the variable titanicANOVA. To do a Tukey-Kramer test on these data, we need to first apply the function aov() to titanicANOVA, and then we need to apply the function TukeyHSD to the result. We can do this in a single command:\n\nTukeyHSD(aov(titanicANOVA))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = titanicANOVA)\n\n$passenger_class\n              diff        lwr        upr     p adj\n2nd-1st -11.367459 -14.345803  -8.389115 0.0000000\n3rd-1st -15.148115 -18.192710 -12.103521 0.0000000\n3rd-2nd  -3.780656  -6.871463  -0.689849 0.0116695\n\n\nThe key part of this output is the table at the bottom. It estimates the difference between the means of groups (for example, the 2nd passenger class compared to the 1st passenger class) and calculates a 95% confidence interval for the difference between the corresponding population means. (“lwr” and “upr” correspond to the lower and upper bounds of that confidence interval for the difference in means.) Finally, it give the P-value from a test of the null hypothesis of no difference between the means (the column headed with “p adj”). In the case of the Titanic data, P is less than 0.05 in all pairs, and we therefore reject every null hypothesis. We conclude that the population mean ages of all passenger classes are significantly different from each other.\n\n\n11.1.3 Kruskal-Wallis\nA Kruskal-Wallis test is a non-parametric analog of a one-way ANOVA. It does not assume that the variable has a normal distribution. (Instead, it tests whether the variable has the same distribution with the same mean in each group.)\nTo run a Kruskal-Wallis test, use the R function kruskal.test(). The input for this function is the same as we used for lm() above. It includes a model formula statement and the name of the data frame to be used.\n\nkruskal.test(age ~ passenger_class, data = titanic_data)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  age by passenger_class\nKruskal-Wallis chi-squared = 116.08, df = 2, p-value &lt; 2.2e-16\n\n\nYou can see for the output that a Kruskal-Wallis test also strongly rejects the null hypothesis of equality of age for all passenger class groups with the Titanic data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "lab11.html#questions",
    "href": "lab11.html#questions",
    "title": "11  Analysis of Variance",
    "section": "11.2 Questions",
    "text": "11.2 Questions\n\nThe European cuckoo does not look after its own eggs, but instead lays them in the nests of birds of other species. Previous studies showed that cuckoos sometimes have evolved to lay eggs that are colored similarly to the host bird species’ eggs. Is the same true of egg size—do cuckoos lay eggs similar in size to the size of the eggs of their hosts? The data file “cuckooeggs.csv” in the data folder contains data on the lengths of cuckoo eggs laid in the nests of a variety of host species. Here we compare the mean size of cuckoo eggs found in the nests of different host species.\n\nPlot a multiple histogram showing cuckoo egg lengths by host species.\nLook at the graph. For these data, would ANOVA be a valid method to test for differences between host species in the lengths of cuckoo eggs in their nests?\nUse ANOVA to test for a difference between host species in the mean size of the cuckoo eggs in their nests. What is your conclusion?\nAssuming that ANOVA rejected the null hypotheses of no mean differences, use a Tukey-Kramer test to decide which pairs of host species are significantly different from each other in cuckoo egg mean length. What is your conclusion?\n\nAnimals that are infected with a pathogen often have disturbed circadian rhythms. (A circadian rhythm is an endogenous daily cycle in a behavior or physiological trait that persists in the absence of time cues.) Shirasu-Hiza et al. (2007) wanted to know whether it was possible that the circadian timing mechanism itself could have an effect on disease. To test this idea they sampled from three groups of fruit flies: one “normal”, one with a mutation in the timing gene tim01, and one group that had the tim01 mutant in a heterozygous state. They exposed these flies to a dangerous bacteria, Streptococcus pneumoniae, and measured how long the flies lived afterwards, in days. The date file “circadian mutant health.csv” shows some of their data.\n\nPlot a histogram of each of the three groups. Do these data match the assumptions of an ANOVA?\nUse a Kruskal-Wallis test to ask whether lifespan differs between the three groups of flies.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  }
]