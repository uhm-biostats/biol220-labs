1
00:00:00,000 --> 00:00:06,480
So our last section is going to deal with the process and high level conceptual architecture

2
00:00:06,480 --> 00:00:13,120
 of inferring the parameters that we use in a process model given the results of that model

3
00:00:13,120 --> 00:00:15,120
using neutral theory as an example. 

4
00:00:15,120 --> 00:00:23,080
And specifically we're going to be dealing with the question of what do we mean by inferring parameters from outcomes and why would we want to do this.

5
00:00:23,080 --> 00:00:28,000
We're going to walk through an example of using this approach using neutral theory. 

6
00:00:28,000 --> 00:00:33,480
And then we're going to use that as an opportunity to talk about some of the challenges that we run into using an approach like this.

7
00:00:33,480 --> 00:00:42,920
This is largely set up for doing a much more involved machine learning procedure on these types of data, 

8
00:00:42,920 --> 00:00:45,221
which we approach in the second day of this workshop.

9
00:00:45,280 --> 00:00:50,320
And again, the materials for that part of the workshop are available on the website. 

10
00:00:50,320 --> 00:00:59,640
And if you're having trouble understanding the high level motivation for those materials, this may be a really useful lecture for you to listen to.

11
00:01:01,440 --> 00:01:05,360
So what do we mean by inferring parameters given the outcomes? 

12
00:01:05,360 --> 00:01:10,880
This is important to have a clear sense of what exactly we're accomplishing here.

13
00:01:10,880 --> 00:01:16,640
We are going to use the outcome of a model to guess at the parameters that produced it. 

14
00:01:16,640 --> 00:01:23,720
To do this, we do need to make the assumption that the processes that are baked into a model -

15
00:01:23,720 --> 00:01:30,680
- so in the example of neutral theory, the birth, the death, the probability of speciation, the probability of immigration -

16
00:01:30,680 --> 00:01:36,320
are a reasonably accurate description of the processes that generated the data that we are working with,

17
00:01:36,320 --> 00:01:43,320
 whether those data come from some other theoretical model, from simulation modeling or from the real world.

18
00:01:43,320 --> 00:01:45,400
This could be a really big assumption.

19
00:01:45,400 --> 00:01:57,960
 It comes back to what we talked about in the first lecture about the importance of understanding the rules and structure of a process model so that we can be responsible about how we interpret the results.

20
00:01:57,960 --> 00:02:06,160
However, assuming that the proceses we've got in the model are a reasonable description of the processes that are relevant to our data, 

21
00:02:06,160 --> 00:02:12,680
we can use our knowledge of how the model behaves over large ranges of parameter space 

22
00:02:12,680 --> 00:02:19,360
to try and guess at the parameter settings that might be responsible for a specific outcome that we observe.

23
00:02:19,360 --> 00:02:27,040
This is the whole heart of what we call likelihood free inference, 

24
00:02:27,040 --> 00:02:32,280
which is the topic of the tutorials in the next day of the workshop.

25
00:02:32,280 --> 00:02:36,800
So let's get a little bit clearer sense of what we're talking about here. 

26
00:02:36,800 --> 00:02:45,720
The overall procedure is: we're going to get a clear sense of how the model behaves over a huge range of parameter settings. 

27
00:02:45,720 --> 00:02:55,440
And so we're gonna run many, many, many simulations over a huge chunk of the range of possible combinations of parameters. 

28
00:02:55,440 --> 00:03:01,160
So we're gonna use a whole bunch of different combinations of M and Nu and J for neutral theory.

29
00:03:01,160 --> 00:03:06,680
And we're going to use those simulations to produce a database of how the outcome variables,

30
00:03:06,680 --> 00:03:09,960
 so the Hill numbers calculated with different values of q,

31
00:03:09,960 --> 00:03:14,360
relate to the parameters that we used to generate them. 

32
00:03:14,360 --> 00:03:19,440
Then we're gonna formalize that knowledge by fitting models. 

33
00:03:19,440 --> 00:03:22,080
We're going to be using machine learning models, random forests.

34
00:03:22,080 --> 00:03:28,920
Of the form "parameters ~ the outcome variables".

35
00:03:28,920 --> 00:03:37,760
So put that another way, we're going to train a machine learning model using the hill numbers

36
00:03:37,760 --> 00:03:45,880
to predict the parameter combinations that produce different combinations of hill numbers for the results. 

37
00:03:45,880 --> 00:03:57,280
And then we can use this trained machine learning model to guess at the parameter values that were used to produce observed outcomes.

38
00:03:57,280 --> 00:04:03,080
So then if we have a set of hill numbers that we got from another model 

39
00:04:03,080 --> 00:04:05,080
or from the real world, or anywhere,

40
00:04:05,080 --> 00:04:08,560
we can use those, combined with this trained machine learning model, 

41
00:04:08,560 --> 00:04:17,560
to estimate the parameter values that may have produced this combination of Hill numbers.

42
00:04:17,560 --> 00:04:22,560
So let's go over this workflow using neutral theory. 

43
00:04:22,560 --> 00:04:33,640
We're going to be predicting m, the immigration rate, and Nu, the speciation rate, from the general structure of a neutral theory model.

44
00:04:33,640 --> 00:04:40,080
The first step is going to be to run a whole bunch of simulations over a range of parameters.

45
00:04:40,080 --> 00:04:48,400
For this, we're going to keep the meta community and the local community size equal, and we'll keep the number of time steps equal.

46
00:04:48,400 --> 00:04:58,720
But we'll sample over a huge range of combinations of parameter values for the immigration rate, M and the speciation rate, Nu.

47
00:04:58,720 --> 00:05:06,120
So here we're varying M from 0.6 to 0 and we'll vary Nu also from 0 to 0.6.

48
00:05:06,120 --> 00:05:13,080
Every one of these points in this plot represents a combination of migration rate and speciation rate

49
00:05:13,080 --> 00:05:15,800
that we are using to run this model. 

50
00:05:15,800 --> 00:05:23,760
So this is 10,000 pairs, 10,000 combinations of immigration and speciation rates.

51
00:05:23,760 --> 00:05:29,800
And if we run these, we'll end up with a data set that looks like this.

52
00:05:29,800 --> 00:05:42,120
We have the parameter values that we use in a simulation as a set of inputs and then the results  that were produced by those inputs as the Hill numbers.

53
00:05:42,120 --> 00:05:49,920
And it's worth taking a second to look at how these different outcome variables

54
00:05:49,920 --> 00:05:53,960
 relate to variation in the parameters that are used to generate them.

55
00:05:53,960 --> 00:06:02,120
Here we're looking at heat maps of how the Hill numbers vary over combinations of the immigration rate and the speciation rate.

56
00:06:02,120 --> 00:06:10,760
What we see are these really interesting gradients where it's clear that as we increase either immigration or speciation 

57
00:06:10,760 --> 00:06:13,520
we end up with, for example, greater species richness.

58
00:06:13,520 --> 00:06:19,680
You can see how these are not totally deterministic models, right? 

59
00:06:19,680 --> 00:06:22,560
They're sort of fuzzy gradients, but they are consistent over large scales.

60
00:06:22,560 --> 00:06:24,920
They're going to show this general pattern emerging.

61
00:06:24,920 --> 00:06:28,240
You can also see, and we'll get back to this later,

62
00:06:28,240 --> 00:06:33,480
 what almost look like isoclines within these gradients 

63
00:06:33,480 --> 00:06:39,480
where we can actually get similar values for the hill numbers

64
00:06:39,480 --> 00:06:42,000
with different combinations of parameters.

65
00:06:42,000 --> 00:06:50,920
So for example, we can get this dark blue color is with, like, no speciation and very high immigration (.2) 

66
00:06:50,920 --> 00:06:58,400
or we can still get those values in a scenario where we have no immigration but relatively high speciation.

67
00:06:58,400 --> 00:07:04,640
And this is foreshadowing a little bit of that model identifiability problem that we talked about before.

68
00:07:04,640 --> 00:07:09,960
We break this down to what we're going to be putting into the machine learning model. 

69
00:07:09,960 --> 00:07:14,600
We're going to be trying to predict, for example, the migration rate M.

70
00:07:14,600 --> 00:07:21,480
from the Hill number. When we do the actual model we'll be using all of the hill numbers, but for visualization we're going to use just one.

71
00:07:21,480 --> 00:07:33,600
And again, we can see that there's clearly correlation between the hill number and the immigration and speciation parameters.

72
00:07:33,600 --> 00:07:36,640
There's also a fair amount of noise in this relationship.

73
00:07:36,640 --> 00:07:40,960
Just knowing one of the hill numbers

74
00:07:40,960 --> 00:07:46,560
is not enough information for us to exactly pinpoint the migration rate or the speciation rate.

75
00:07:46,560 --> 00:07:49,720
But we can get a general trend.

76
00:07:49,720 --> 00:07:56,440
So I'm not going to in this lecture go into the details of how random forest models work. 

77
00:07:56,440 --> 00:08:06,240
For now, I will say that we are fitting these using a random forest because you can think of it as a very flexible form of regression

78
00:08:06,240 --> 00:08:15,040
in that we don't need to know, a priori, the shape of the relationship between the predictor variables and the outcome variable, 

79
00:08:15,040 --> 00:08:23,280
but we can leverage whatever relationship there does exist to fit essentially a very flexible regression. 

80
00:08:23,280 --> 00:08:29,920
And this is a gross oversimplification, but is a high-level description of what we're trying to accomplish here.

81
00:08:29,920 --> 00:08:38,040
So we can fit this random forest model in which we are predicting the migration rate from the 3 hill numbers

82
00:08:38,040 --> 00:08:41,840
 using the data that we generated by running a whole bunch of simulations.

83
00:08:41,840 --> 00:08:48,680
And if we examine this regression, we can see a decent approximate R squared.

84
00:08:48,680 --> 00:08:51,760
 And this is telling us that it's a random forest model.

85
00:08:51,760 --> 00:08:56,760
And then if we examine the model accuracy, again, 

86
00:08:56,760 --> 00:08:59,000
we see something that we might have been able to guess at 

87
00:08:59,000 --> 00:09:07,960
based both on heat map plots that we showed and looking at that one dimensional or single predictor regression earlier.

88
00:09:07,960 --> 00:09:13,320
Which is that there is a relationship between the migration rate and the Hill numbers, 

89
00:09:13,320 --> 00:09:18,400
and that we can do a pretty good job of recovering migration rate given the parameters.

90
00:09:18,400 --> 00:09:24,760
That is, the predicted values on the y-axis are a reasonable approximation of the actual values, which are on the x-axis.

91
00:09:24,760 --> 00:09:28,040
If it were perfect they would all fall along this yellow 1:1 line.

92
00:09:28,040 --> 00:09:31,840
And we see that there's some pretty good predictive capacity, 

93
00:09:31,840 --> 00:09:34,680
but also there's a lot of misses in this data 

94
00:09:34,680 --> 00:09:41,160
because again perhaps there's not enough information given just the Hill numbers to uniquely identify the migration rate.

95
00:09:41,160 --> 00:09:45,480
We can run through the same process predicting Nu. 

96
00:09:45,480 --> 00:09:52,000
And we see a better but similarly sort of fuzzy relationship.

97
00:09:52,000 --> 00:10:00,800
So given these models that we have predicting the parameters given the outcomes, 

98
00:10:00,800 --> 00:10:06,640
we can go ahead and run an experiment or play out a game 

99
00:10:06,640 --> 00:10:12,840
where we're going to generate some new data using the same model simulation framework

100
00:10:12,840 --> 00:10:18,320
 and see how well we can recover. the parameters that we used to generate these data.

101
00:10:18,320 --> 00:10:25,160
So this is some code that you can run in R using the functions that are included in the online materials

102
00:10:25,160 --> 00:10:32,840
to run a new set of simulations using a never before seen combination of migration and speciation rate.

103
00:10:32,840 --> 00:10:39,200
And then to compare the predictions from the random forest models to the actual data. 

104
00:10:39,200 --> 00:10:49,960
So to be more specific, to test how well we can guess our new combination of parameter values based on the results of those models.

105
00:10:49,960 --> 00:11:02,760
by using the random forest that we trained on our big data set to estimate the parameters that generated the outcome variables that we provided.

106
00:11:02,760 --> 00:11:10,000
And so for example, we can see that, one time when I ran this, we tried

107
00:11:10,000 --> 00:11:14,680
 a migration rate of .32 and a Nu of .43.

108
00:11:14,680 --> 00:11:19,760
And then we ask the random forest model to predict, based on the hill numbers that we got, 

109
00:11:19,760 --> 00:11:25,120
what was the value for M and what was the value for Nu that were used to produce these data?

110
00:11:25,120 --> 00:11:32,080
And it guessed .28 for M and .37 for Nu which is ballpark but not perfect.

111
00:11:32,080 --> 00:11:42,040
And yeah, the estimation is good, but it's not perfect. And this cycles us back to one of the challenges of estimation.

112
00:11:42,040 --> 00:11:44,280
It's not necessarily the only thing that's going on here 

113
00:11:44,280 --> 00:11:46,920
but a really important one when we're dealing with these process models

114
00:11:46,920 --> 00:12:00,640
is this question of model identifiability, or the lack of statistical leverage for identifying the generative parameters

115
00:12:00,640 --> 00:12:07,840
when multiple combinations of parameters can give us the same results. 

116
00:12:07,840 --> 00:12:17,120
So as we've seen you can have different combinations of Nu and M and you can arrive at the same or similar values for the different Hill numbers.

117
00:12:17,120 --> 00:12:25,120
Which makes it difficult to use the Hill numbers to be precise about what the generative values for M and Nu were. 

118
00:12:27,120 --> 00:12:33,080
So how could we get better traction on a problem like this? 

119
00:12:33,080 --> 00:12:39,880
Well, there's a few options and this is really foreshadowing the next day of the workshop.

120
00:12:39,880 --> 00:12:44,320
But one of the first ones is to have different sets of parameters

121
00:12:44,320 --> 00:12:46,080
 or different sets of model processes 

122
00:12:46,080 --> 00:12:49,720
that better capture the data that we're interested in modeling.

123
00:12:49,720 --> 00:12:55,240
We can also build in more complex models that allow us to take advantage of 

124
00:12:55,240 --> 00:13:00,360
more types or more dimensions of data about a system.

125
00:13:00,360 --> 00:13:06,880
This is in many ways - the procedure that we've outlined here is in many ways similar to trying to

126
00:13:06,880 --> 00:13:09,880
guess what city you're in based only on the temperature reading. 

127
00:13:09,880 --> 00:13:16,240
A city in Florida and a city in Texas may have exactly the same temperature reading 

128
00:13:16,240 --> 00:13:20,760
and so it's really difficult to tell which city you're in based just on the temperature.

129
00:13:20,760 --> 00:13:23,280
But if we could add more data dimensions, 

130
00:13:23,280 --> 00:13:25,280
for example, maybe something about humidity, 

131
00:13:25,280 --> 00:13:30,720
we might have a much better chance at guessing where we are in the world

132
00:13:30,720 --> 00:13:33,440
 based on these 2 data points

133
00:13:33,440 --> 00:13:35,440
 than just on the one data point or one data dimension.

134
00:13:35,440 --> 00:13:40,320
So if we're able to build a model, a process model is able to take into account and take advantage of 

135
00:13:40,320 --> 00:13:44,920
more different dimensions of data or more ways of looking at a system, 

136
00:13:44,920 --> 00:13:50,200
we can often get much better traction on it than we can by looking at just a single dimension.

137
00:13:50,200 --> 00:13:55,640
And this is the motivation for what we call the MESS modeling or RoLE modeling framework 

138
00:13:55,640 --> 00:14:01,000
and it's going to be the main focus of the next sections of the workshop.

139
00:14:02,453 --> 00:14:08,480
So to summarize what we've covered today, in principle, we can use process models 

140
00:14:08,480 --> 00:14:13,040
to infer the parameters that generated observed data 

141
00:14:13,040 --> 00:14:17,720
provided the observed data are generated by similar processes to the model.

142
00:14:17,720 --> 00:14:21,360
And that there are a few things few things that can really complicate this process

143
00:14:21,360 --> 00:14:24,760
 including efforts at what's called out of sample prediction -

144
00:14:24,760 --> 00:14:30,440
So if we were trying to guess it parameters when we've never seen those before - 

145
00:14:30,440 --> 00:14:32,440
model identifiability - 

146
00:14:32,440 --> 00:14:36,520
And, again, these wider concerns or considerations with process models 

147
00:14:36,520 --> 00:14:40,120
including the computational effort required to run the model

148
00:14:40,120 --> 00:14:46,240
 and the underlying validity of the modeling rulebook given what it is that we're trying to model.

149
00:14:48,000 --> 00:14:51,600
And staying tuned for the next sections of the workshop, 

150
00:14:51,600 --> 00:14:57,440
we are going to be talking about considerably more flexible and multi-dimensional biodiversity models 

151
00:14:57,440 --> 00:15:03,520
and ways to operate those at scale using the iBioGen and MESS frameworks.

152
00:15:03,520 --> 00:15:14,560
Those are the next day's worth of materials, which are available on the course website and are a great resource for learning more about this.

